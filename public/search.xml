<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TreadMarks: 基于工作站网络的共享内存计算]]></title>
    <url>%2F2020%2F04%2F01%2FTreadMarks%2F</url>
    <content type="text"><![CDATA[TreadMarks: 基于工作站网络的共享内存计算 论文: TreadMarks: Shared Memory Computing on Networks of Workstations Christiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Ra jamony, Weimin Yu and Willy Zwaenepoel Department of Computer Science Rice University 摘要（Abstract）TreadMarks通过为应用程序提供共享的内存抽象来支持基于工作站网络上的并行计算。共享内存有助于从顺序程序转换成并行程序，因为大多数数据结构都可无需做更改。 只需要添加同步操作。 我们会讨论TreadMarks中用于提供有效的共享内存技术，并讨论了两个主要应用程序的经验，即混合整数编程和遗传连锁分析。 1 介绍（ Introduction）高速网络和快速提高的微处理器性能使工作站的网络成为越来越有吸引力的并行计算工具。通过仅仅依靠廉价硬件和软件，工作站网络可以以相对较低的成本提供并行处理。可以将工作站网络的多处理器实现成处理器组[17]，这是许多专用于提供计算周期的处理器。或者，它也可以由一组动态变化的机器组成，在这些机器上利用空闲周期执行长时间运行的计算[14]。在后一种情况下，（硬件）成本基本上为零，因为许多组织机构已经拥有广泛的工作站网络。在性能方面，处理器速度，网络带宽和延迟的改进使联网的工作站能够为越来越多的应用程序提供接近或超过超级计算机的性能。我们的立场不是让这种松耦合的多处理器设计使更紧密耦合的设计过时。尤其是，更低延迟和更高带宽的这些紧耦合的设计能有效执行对同步和通信要求更加严格的应用程序。但是，我们认为网络技术和处理器性能的进步将极大地扩展可以在工作站网络上有效执行的应用程序的种类。 在论文中，我们会讨论使用TreadMarks分布式共享内存（DSM）系统在工作站网络上进行并行计算的经验。DSM允许进程假定全局共享的虚拟内存，即使它们运行的节点并没有在物理上共享内存[13]。图1举例说明了一个DSM系统，该系统由N个联网的工作站组成，每个工作站都有自己的内存，并通过网络连接起来。DSM软件提供了全局共享内存的抽象，每个处理器都可以访问任何数据项，而程序员不必担心数据在哪里或如何获取。相反，在基于工作站网络的“本地”编程模型中，程序员必须确定处理器何时需要进行通信，与谁进行通信以及要发送什么数据。对于具有复杂数据结构和复杂并行化策略的程序，这可能成为艰巨的任务。在DSM系统上，程序员可以专注于算法开发，而不是管理分区的数据集和数据传输。除了易于编程之外，DSM还提供了与（硬件）共享内存多处理器相同的编程环境，从而允许在两种环境之间进行移植。最后，DSM考虑到在网络环境中无缝集成共享内存的多处理器工作站。本文描述的实际系统TreadMarks [7]，在Unix工作站上以用户级别运行。不需要进行内核修改或使用特权，并且使用标准Unix编译器和链接器。实现DSM系统的挑战在于确保共享内存抽象不会导致大量通信。TreadMarks中使用了多种技术来应对这一挑战，包括lazy release consistency(延迟释放一致性)[6]和多写协议[3]。本文首先会描述TreadMarks提供的应用程序编程接口（第2节），接下来，我们会讨论实现方面的挑战。（第3节）以及用于应对这些挑战的技术（第4和第5节）。我们在第6节中会简要的描述TreadMarks的实现，并通过讨论我们在混合整数编程和遗传连锁分析（第7节）这两个大型应用程序中的经验来证明其效率。最后，我们在第8节中讨论相关工作，并在第9节中提供一些结论和指导，以及进一步的工作。 图１ 分布式共享内存 2 共享内存编程（Shared Memory Programming）2.1 应用程序编程接口（Application Programming Interface）TreadMarks API简洁但功能强大（有关C语言接口，见图2）。它提供了用于进程创建和销毁，同步以及共享内存分配的功能。我们专注于同步原语。 共享内存会引起数据争用。当不同进程以程序员不希望的方式交错对共享变量访问时，就会发生数据竞争。例如，假设一个进程写入共享记录的多个字段，而另一个进程尝试读取这些相同的字段。第二个进程很有可能读取新写入的值，但也有可能在第一个进程写入新值前读取到旧值。通常不希望发生这种情况。 为了避免这种情况，共享内存并行程序包含同步操作。 在这种特殊情况下，将使用锁来允许每个进程对记录的独占访问。 TreadMarks提供了两种同步原语：barrier(屏障)和exclusive locks(排他锁)。进程通过调用 Tmk barrier() 来等待屏障。屏障是全局的：调用进程会被暂停，直到系统中的所有进程到达同一屏障为止。进程调用Tmk lock来获取锁，然后调用Tmk release来释放锁。当另一个进程持有该锁时，任何进程都无法获取该锁。同步原语的这种特定选择跟TreadMarks的设计没有任何关系；以后可能会增加其他原语。 我们通过两个简单的应用程序演示了这些以及其他TreadMarks原语的用法。 2.2 两个简单的示例（Two Simple Illustrations）图3和图4说明了TreadMarks API在Jacobi迭代和解决旅行商问题（TSP）中的使用。 我们很清楚这些示例代码的过于简单。 但在这里仅用于演示目的。 实际应用将在第7节中讨论。 Jacobi是一种求解偏微分方程的方法。我们的示例遍历二维数组。在每次迭代期间，每个矩阵元素都会更新为其最近元素的平均值（上方，下方，左侧和右侧）。Jacobi使用临时数组存储每次迭代期间计算的新值，以避免元素的旧值在其被临近元素使用之前覆盖。在并行版本中，为所有处理器分配的数量大致相等的行。 边界上的行由两个相邻进程共享。 图3中的TreadMarks版本使用两个数组：一个网格和一个临时数组。网格的保存在共享内存中，而临时的是每个进程专有的。网格由进程0分配和初始化。Jacobi中的同步是通过屏障实现的。 Tmk barrier(0) 确保在开始计算之前，进程0的初始化对所有进程可见。Tmk barrier(1) 确保所有进程在上一次迭代中完成读取值之前，不会有进程会覆盖网格中的任何值。Tmk barrier(2) 可防止当前迭代计算中的所有网格的值被设置前，没有任何进程可以开始下一次迭代。 旅行商问题（TSP）是查找从指定城市开始，经过地图上所有其他城市恰好一次并返回原始城市的最短路径。使用了一种简单的分支定界算法。每次局部旅行一次就扩展一个城市。如果局部旅行的长度加上路径剩余部分的下限大于当前最短旅行路径的话，则不会在进一步进行局部旅行了，因为他不会导致比当前最短路径更短的路径了。 该程序维护一个局部旅行的列队，保持最短的局部旅行在列队最前面。它不断往列队中添加可能最优的局部旅行，直到从列队的开头开始找到一个比阈值长的路径为止。然后程序将它从列队中移除，并尝试剩余程序的所有排列。最终，它将比较当前路径和包含此路径的最短路径，且如果有必要的话就更新当前最短路径。列队和当前最短路径是共享的，通过锁来保护对他们的访问。 123456789101112131415161718192021222324/* the maximum number of parallel processes supported by TreadMarks */#define TMK_NPROCS/* the actual number of parallel processes after Tmk_startup */extern unsigned Tmk_nprocs;/* the process id, an integer in the range 0 ... Tmk_nprocs - 1 */extern unsigned Tmk_proc_id;/* the number of lock synchronization objects provided by TreadMarks */#define TMK_NLOCKS/* the number of barrier synchronization objects provided by TreadMarks */#define TMK_NBARRIERS/* Initialize TreadMarks and start the remote processes */void Tmk_startup(argc, argv) int argc; char **argv;/* Terminate the calling process. Other processes are unaffected. */void Tmk_exit(status) int status;/* Block the calling process until every other process arrives at the barrier. */void Tmk_barrier(id) unsigned id;/* Block the calling process until it acquires the specified lock. */void Tmk_lock_acquire(id) unsigned id;/* Release the specified lock. */void Tmk_lock_release(id) unsigned id;/* Allocate the specified number of bytes of shared memory */char *Tmk_malloc(size) unsigned size;/* Free shared memory allocated by Tmk_malloc. */void Tmk_free(ptr) char *ptr; 图 2 TreadMarks C语言接口 1234567891011121314151617181920212223242526272829#define M 1024#define N 1024float **grid;float scratch[M][N];Tmk_startup();if (Tmk_proc_id == 0)&#123; grid = Tmk_malloc(M * N * sizeof(float)); initialize grid;&#125;Tmk_barrier(0);length = M / Tmk_nprocs;begin = length * Tmk_proc_id;end = length * (Tmk_proc_id + 1);for (number of iterations)&#123; for (i = begin; i &lt; end; i++) for (j = 0; j &lt; N; j++) scratch[i][j] = (grid[i - 1][j] + grid[i + 1][j] + grid[i][j - 1] + grid[i][j + 1]) / 4; Tmk_barrier(1); for (i = begin; i &lt; end; i++) for (j = 0; j &lt; N; j++) grid[i][j] = scratch[i][j]; Tmk_barrier(2);&#125; 图 3 The TreadMarks Jacobi 程序 实现的挑战（Implementation Challenges）提供内存一致性是DSM系统的核心：DSM软件必须以一种提供全局共享内存的方式在处理器之间移动数据。在李的原始IVY系统中[13]，虚拟内存硬件用于维护内存一致性。每个处理器的本地（物理）存储器形成全局虚拟地址空间的高速缓存（请参见图5）。如果一个页面不在本地处理器的内存中，则会产生缺页错误。DSM软件将该页面的最新副本从其远程位置导入本地内存，然后重新启动该过程。例如，图5显示了处理器1发生缺页错误，然后从处理器3的本地内存中复制需要页面的副本。IVY进一步将读取错误与写入错误区分开。发生读取错误时，页面将所有副本以只读方式进行复制，而发生写入错误时，所有现有副本都将无效，并且写的处理器保留唯一副本。 1234567891011121314151617181920212223242526272829queue_type *Queue;int *Shortest_length;int queue_lock, tour_lock;main()&#123; Tmk_startup(); queue_lock = 0; tour_lock = 1; if (Tmk_proc_id == 0) Queue = Tmk_malloc(sizeof(queue_type)); Shortest_length = Tmk_malloc(sizeof(int)); initialize Heap and Shortest_length Tmk_barrier(0); Tmk_lock_acquire(queue_lock); Exit if queue is empty; Keep adding to queue until a long, promising tour appears at the head; Path = Delete the tour from the head; Tmk_lock_release(queue_lock); length = recursively try all cities not on Path, find the shortest tour length Tmk_lock_acquire(tour_lock); if (length &lt; *Shortest_length) *Shortest_length = length; Tmk_lock_release(tour_lock);&#125; 图4 The TreadMarks TSP 程序 尽管很简单，但DSM的IVY实现可能产生大量通信。而在工作站网络上通信的成本很高。发送消息可能涉及到进入操作系统内核陷阱，中断，上下文切换以及可能经过多层网络协议的处理。因此，必须保持较少的消息的数量。 我们使用图3和图4的示例来说明IVY中的一些通信问题。 第一个问题与IVY的一致性模型有关，通常称为顺序一致性（sequential consistency） [9]。粗略地说，顺序一致性要求对共享内存的写入对于其他处理器“立即”变为可见。这就是为什么IVY在写入共享内存之前发送无效消息的原因。在许多情况下，顺序一致性提供了过强的保证。 例如，思考下图4中TSP中最佳巡回的更新及其长度。在IVY中，这两个共享内存更新将导致无效信息发送到所有其他的缓存包含这些变量的页面的处理器。 但是，只会在有相应锁保护的临界区内访问这些变量，因此仅将无效发送给获取锁的下一个处理器就足够了，并且仅在获取锁时才发送。 第二个问题涉及到潜在的伪共享的问题。当位于同一个页面的两个或两个以上不相关的数据对象由不同的处理器并发写入时，就会导致伪共享，从而导致该页在处理器之间像乒乓球一样来回移动。由于一致性单元很大（虚拟内存页），所以伪共享是一个潜在的严重问题。图6展示了Jacobi中网格数组可能的页面布局。 当两个处理器都更新其网格数组的一部分时，它们正在同时写入同一页面，并导致该页面在处理器间来回移动多次。 图５ IVH DSM系统操作 4 懒惰更新释放一致性（Lazy Release Consistency）4.1 释放一致性（Release Consistency）释放一致性[5]是一个宽松的内存一致性模型。它不能始终保证一致性。 对共享内存更新的传播可能会延迟一段时间。这样，释放一致性实现可以将多个更新消息合并为单个消息，从而减少消息的数量。但是，通过在某些同步操作中强制执行一致性，至少对于正确同步的程序而言，对程序员屏蔽了潜在的不一致性。我们都在直观层面上讨论这些问题。关于更详细的说明，读者可以参考Gharachorloo 等人的论文 释放一致性的直觉如下。并行程序不应出现数据竞争，因为这会导致不确定的结果。因此，必须充分的使用同步来防止数据竞争。更具体地说，在对共享内存的两个冲突性访问之间必须存在同步（如果两个访问相同的内存位置，并且其中至少一个是写操作，则这两个访问是冲突性的）。由于存在这种同步，因此DSM系统可以延迟发送从一个进程到另一个进程的更新，直到发生这样的同步事件为止，因为只有执行了同步操作，第二个进程才会访问数据。对于没有数据竞争的程序，顺序一致性和释放一致性的结果是相同的。 我们将通过第2节中的Jacobi和TSP示例来说明这个原理。在Jacobi中，执行barrier 1 后才进行写共享内存操作，此时将新计算的值从临时数组复制到网格数组中。执行barrier 2时计算阶段才结束。执行barrier 2是为了避免在所有新值被写入网格数组前进程就开始下一次迭代。不管是什么存储模型，都必须需要barrier来确保正确性。但是，它的存在使我们可以推迟网格数组元素新值的传播，直到遇到更低的barrier。 在TSP中，任务队列是主要的共享数据结构。处理器从队列中获取任务并对其进行处理，也会创建新的任务。 这些新创建的任务将插入队列。任务队列结构的更新需要对共享内存进行一系列的写入，例如其大小，位于队列开头的任务等。原子地对任务队列数据结构进行访问是为了保证程序的正常运行。一次仅允许一个处理器访问任务队列数据结构。这些保证通过在操作前加锁，操作后解锁来实现。因此只有获得锁的进程才能访问数据结构，也无须立即将更新传播给其他进程。这些处理器需要首先获得锁。因此在获得锁的时候传播数据结构的更新就可以了。 这两个示例说明了释放一致性的基本原理。在共享内存的并行程序中引入了同步，以防止进程在同步操作完成之前查看某些内存。由此得出结论，在同步操作完成之前，不必传播那些内存操作的值。相反，顺序一致性会立即更新每个远程内存。 释放一致性需要少得多的消息，并导致更少的消息到达延迟。 程序员不必对此太在意。 如果程序没有数据竞争，则它的行为就好像在普通的共享内存系统上执行一样，其条件是：所有同步都必须使用TreadMarks提供的原语来完成。 否则，TreadMarks无法知道何时应该让共享内存保持一致性。 图６ Jacobi的伪共享例子 4.2 懒惰更新释放一致性（Lazy Release Consistency）TreadMarks使用懒惰更新释放一致性算法[6]来实现释放一致性。粗略地说，懒惰更新释放一致性在获取锁时确保一致性，相反的，Munin早期实现的释放一致性在释放锁时实施一致性。懒惰更新释放一致性发送的消息更少。在锁释放时，Munin的释放锁的处理器发送缓存了修改数据的所有其他处理器。比较起来，在懒惰更新释放一致性中，一致性消息仅在锁的最后一个释放者和新获取者之间传播。 懒惰更新释放一致性（lazy release consistency）比急迫更新释放一致性（eager release consistency）更复杂一些。在释放后，Munin可以忘记释放的处理器在释放之前所做的所有修改。懒惰更新释放一致性不是这种情况，因为第三个处理器以后可能会获取锁并需要查看修改。在实践中，我们的经验表明，对于工作站网络而言，发送消息的成本很高，减少消息数量所带来的收益超过了更复杂的实现的成本。 5 多写协议（Multiple-Writer Protocols）大多数硬件缓存和DSM系统使用单写程序协议。这些协议允许多个读同时访问同一页面，但是在执行任何修改之前，只允许一个写对页面进行修改。单写程序协议易于实现，因为给定页面的所有副本始终都是相同的，并且始终可以通过从当前具有有效副本的任何其他处理器中获取页面的副本来解决缺页错误。不幸的是，这种简单性通常以牺牲消息流量为代价的。 在写页面之前，所有其他副本都必须无效。如果其页面已失效的处理器仍在访问该页面的数据，这些失效则可能导致随后的访问丢失。 伪共享会由于无关访问之间的干扰而导致单写程序协议的性能更差。DSM系统通常比硬件系统遭遇更多的伪共享，因为它们以虚拟内存页面而不是高速缓存行的粒度跟踪数据访问。 顾名思义，多写协议允许多个写同时修改同一页面，并延后发送保证一致性的消息，尤其是直到同步发生为止 TreadMarks使用虚拟内存硬件来检测对共享内存页面的访问和修改。图7显示了如何使用保护故障（protection faults）来创建差异（diffs）。有效页面最初被写保护。 发生写操作时，TreadMarks会创建虚拟内存页的一个副本，或一个twin（双胞胎），并将其保存在系统空间中。当需要将修改发送到另一个处理器时，则将页面的当前副本与twin逐字比较，并将变化的字节保存到diff数据结构中。一旦创建diff后，twin将被丢弃。 除了处理器第一次访问页面外，其页面副本仅通过应用diff进行更新。 不再需要页面的新完整副本。 使用diff的主要好处是它们可用于实现多写协议，由于diff通常比页面小得多，因此它们还可显着降低总体带宽需求。 ６ TreadMarks系统（The TreadMarks System）TreadMarks完全以Unix上用户库的方式实现的。不需要修改Unix内核，因为现代Unix提供了在用户级别实现TreadMarks所需的所有通信和内存管理功能。用C，C++或者FORTRAN编写的程序可以用这些语言标准的编译器来编译和连接TreadMarks库。该系统是相对便携式的。 当前，它可以在SPARC，DECStation，DEC / Alpha，IBM RS-6000，IBM SP-1和SGI平台以及以太网和ATM网络上运行。 在本节中，我们简要描述如何实现TreadMarks的通信和内存管理。 TreadMarks使用Berkeley套接字接口实现了机器间通信。取决于底层的网络硬件，例如以太网或ATM，TreadMarks使用UDP / IP或AAL3 / 4作为消息传输协议。默认情况下，除非计算机通过ATM LAN连接，否则TreadMarks使用UDP / IP。AAL3 / 4是ATM标准规定的面向连接的最大努力交付协议（best-efforts delivery protocol）。 由于UDP / IP和AAL3 / 4都不保证可靠的传递，因此TreadMarks使用轻量，特定操作的用户级协议来确保消息到达。 TreadMarks发送的每个消息要么是请求消息要么是响应消息。TreadMarks发送的消息可能是显式调用TreadMarks库产生的或者是缺页错误产生的。一旦机器发送了请求消息，它将阻塞直到有一个请求消息或者预期的响应消息到达。为了请求处理的延迟最小化，TreadMarks使用SIGIO信号的处理程序来处理请求。消息到达用于接收请求消息的任何套接字都会产生SIGIO信号。由于AAL3 / 4是面向连接的协议，因此与其他每台机器都有一个相对应的套接字。为了确定哪个套接字处理请求，处理程序将执行select系统调用。处理程序收到请求消息后，将执行指定的操作，发送响应消息，然后返回到中断的进程。 为了实现一致性协议，TreadMarks使用mprotect系统调用来控制对共享页面的访问。尝试在共享页面上执行受限访问都会产生SIGSEGV信号。SIGSEGV信号处理程序检查本地数据结构以确定页面的状态。如果本地副本是只读的，则该处理程序从空闲页面池中分配一个页面，并执行bcopy来创建一个twin。最后，处理程序将访问权限升级到原来的页面并返回。如果本地页面无效，则处理程序执行一个过程（procedure）通过从最少的远程计算机集中获取对共享内存的必要更新。 图７ DiffCreation 7 应用（Applications）使用TreadMarks已实现了许多应用程序，并且一些基准测试的性能已在之前进行了报道[7]。在这里，我们描述最近使用TreadMarks实现的两个大型应用程序的经验。在本文作者的帮助下，顺序执行代码的作者将混合整数编程（mixed integer programming）和遗传连锁分析（genetic linkage analysis）这些应用程序从现有的顺序代码进行了并行处理。虽然很难量化所涉及的工作量，但已经证明为获得有效的并行代码而进行的修改的工作量相对很小，这将在本节的其余部分中演示。 7.1 混合整数编程（Mixed Integer Programming）混合整数编程（MIP）是线性编程（LP）的一个版本。在LP中，在由一组线性不等式描述的区域中优化了目标函数。在MIP中，部分或全部变量被约束为只能用整数值（有时仅是0或1）。 图8显示了一个精确的数学公式，图9显示了一个简单的二维实例。 图 8 混合整数编程问题(MIP) ​ 图 9 MIP二维实例 图10 解决MIP问题的分支界定法 Lee等人实现了用于解决MIP问题的TreadMarks代码 [11]。该代码使用分支剪切法。首先将MIP问题简化为相应的LP问题。通常，这个LP问题的解决方案将为某些约束为整数的变量产生非整数值。下一步是选择这些变量中的一个，并分支出两个新的LP问题，一个问题具有 $x_{i} = \left \lfloor x_{i} \right \rfloor$ 的附加约束，另一个具有 $x_{i} = \left \lceil x_{i} \right \rceil$ 的附加约束（见图10）。慢慢地，该算法会生成此类分支的树。找到解决方案后，此解决方案便会在解决方案上建立界限。分支中的LP问题的解决方案产生的结果低于此边界则无需再进入下一步了。为了加快此过程，该算法使用一种称为plunging的技术，本质上是对树进行深度优先搜索，以找到整数解并尽快建立边界。最后一种感兴趣的改进的算法是cutting planes的使用。 这些是添加到LP问题的附加约束，以加强对整数问题的描述。 该代码用于解决MIPLIB库中的所有51个问题。该库包括航空人员调度，网络流量，工厂位置，机队调度等方面的代表性示例。图11显示了在8台运行Ultrix 4.3且通过100Mbps Fore ATM交换机连接的DecStation-5000 / 240机器的网络上获得的加速，这些问题的顺序运行时间超过2,000秒。对于大多数问题，加速几乎是线性的。 一个问题表现为超线性加速，这是因为并行代码命中了前期运行的解决方案，从而裁剪掉了大多数分枝定界树。对于另一个问题，几乎没有加速，因为在预处理步骤之后不久就找到了解决方案，此时还没有进行并行化。除了MIPLIB库中的问题外，该代码还可用于解决以前无法解决的多商品流问题[2]。 该问题在具有8个处理器的IBM SP-上花费的CPU时间为30天，并且还表现出接近线性的加速。 图11 MIPLIB库的结果 7.2 遗传连锁（Genetic Linkage）遗传连锁分析是一种统计技术，使用家族谱系信息来绘制人类基因图谱并在人类基因组中定位疾病基因。生物学和遗传学的最新进展已提供了大量可用的遗传材料，使计算成为进一步研究的瓶颈。 在Mendelian经典的继承理论中，孩子的染色体接收父母一方每个染色体的一条链。考虑重组时，情况会有所不同。如果发生重组，则孩子的染色体链将包含父母染色体的每一条链中的每一条（见图12）。连锁分析的目的是推导我们寻找的基因与已知位置的基因之间发生重组的概率。 从这些概率可以计算出基因在染色体上的大概位置。 ILINK [4]是广泛使用的遗传连锁分析程序的一个并行版本，它是LINKAGE [10]程序包的一部分。ILINK将称为谱系的家谱作为输入，并增加了有关该家族成员的一些遗传信息。它计算重组概率θ的最大似然估计。从最上层来看，ILINK由一个优化θ的循环组成。在优化循环的每次迭代中，程序遍历整个谱系，一次遍历一个核心家庭，在已知家庭成员的遗传信息的情况下，计算当前θ的可能性。对于核心家庭的每个成员，该算法都会更新一系列条件概率，这些条件概率表示个体具有特定遗传特征的概率，其条件取决于θ和已经遍历的部分家谱。 通过以均衡负载的方式在可用处理器之间分配每个核心家庭的迭代空间，可以并行化上述算法。负载均衡是必不可少的，且依赖于阵列元素中表示的遗传信息的知识。另一个方式是拆分树，它未能产生良好的加速效果，因为大多数计算都发生在树的一小部分上（通常，这些节点靠近树的根部，他们表示已知遗传信息很少的已故者）。另外，无法并行评估不同的θ值，因为优化程序会从一个θ值顺序移至下一个θ值。 图13显示了使用ILINK为各种数据集获得的加速。数据集来自对真实的疾病基因定位的研究。对于运行时间较长的数据集，可以实现良好的加速。 对于最小的数据集BAD，由于通信与计算的比率变大，因此加速要少得多。 图12 DNA重组 8 相关工作（Related Work）本节的目标不是进行广泛的并行编程研究，而是以一个独特的示例说明替代性方案，并将最终的系统与TreadMarks进行比较。 消息传递（PVM）。 当前，消息传递是分布式存储系统的主要编程范例。便携式虚拟机（PVM）[15]是一种流行的消息传递软件。它允许将异构的计算机网络视为单个并发计算引擎。尽管PVM中的编程比底层机器的本机消息传递范例中的编程容易得多，并且可移植性强，但是应用程序程序员仍然需要编写代码来显式交换消息。TreadMarks的目标是减轻程序员的负担。 对于具有复杂数据结构和复杂并行化策略的程序，我们认为这是一个主要优势。 隐式并行（HPF）。 如HPF [8]中所示，隐式并行性依赖于用户提供的数据分布，编译器随后使用这些数据分布来生成消息传递代码。这种方法适用于数据并行程序，例如Jacobi。 动态并行性的程序，例如TSP，ILINK或MIP，很难在HPF框架中表达。 面向对象的并行计算（Orca）。Orca [16]和其他面向对象的并行计算系统一样没有为程序员提供共享的存储空间，而是支持对象的共享空间，每个对象都可以通过适当同步的方法进行访问。除了从编程的角度来看的优点之外，这种方法还允许编译器推断出某些优化方法，这些优化方法可用于减少通信量。缺点是，顺序程序中“自然”的对象通常不是正确的并行化对象，需要更多的更改才能获得有效的并行程序 硬件共享内存实现（DASH）。 另一种方法是在硬件中实现共享内存，对少量处理器使用侦听总线协议，或对大量处理器使用基于目录的协议（例如[12]）。我们使用这种方法来共享编程模型，但是我们的实现避免了昂贵的缓存控制器硬件。 另一方面，硬件实现可以有效地支持具有更细粒度并行性的应用程序。 条目一致性（Entry Consistency）（中途）。 条目一致性是另一种宽松的内存模型[1]。它要求所有共享数据都与同步对象相关联。当获取同步对象时，只传输与该同步对象关联的修改后的数据，从而进一步减少通信数据量。但是，条目一致性内存模型比释放一致性弱，这可能会使编程更加困难。 图13 ILINK结果 9 总结和下一步工作（Conclusions and Further Work）我们的经验表明，使用适当的实现技术，分布式共享内存可以为工作站网络上的并行计算提供有效的平台。多数应用程序移植到TreadMarks分布式共享内存系统上，几乎没有困难，并且性能良好。在下一步的工作中，我们打算尝试其他实际应用，包括地震建模代码。我们还在开发各种工具，以进一步减轻编程负担并提高性能。特别是，我们正在研究使用诸如支持预取的编译器，以及使用性能监视工具来消除不必要的同步。 引用（References）[1] B.N. Bershad, M.J. Zekauskas, and W.A. Sawdon. The Midway distributed shared memory system. In Proceedings of the ‘93 CompCon Conference, pages 528-537, February 1993.[2] D. Bienstock and O. Gumluk. Computational experience with a difficult mixed-integer multi-commodity flow problem. To appear in Mathematical Programming, 1994.[3] J.B. Carter, J.K. Bennett, and W. Zwaenepoel. Implementation and performance of Munin. In Proceedings of the 13th ACM Symposium on Operating Systems Principles, pages 152-164, October 1991.[4] S. Dwarkadas, A.A. Schäffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. Parallelization of general linkage analysis problems. Human Heredity, 44:127-141, 1994.[5] K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. Memory consistency and event ordering in scalable shared-memory multiprocessors. In Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 15-26, May 1990.[6] P. Keleher, A. L. Cox, and W. Zwaenepoel. Lazy release consistency for software distributed shared memory. In Proceedings of the 19th Annual International Symposium on Computer Architecture, pages 13-21, May 1992.[7] P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: Distributed shared memory on standard workstations and operating systems. In Proceedings of the 1994 Winter Usenix Conference, pages 115-131, January 1994.[8] C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. The High Performance Fortran Handbook. The MIT Press, 1994.[9] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess programs. IEEE Transactions on Computers, C-28(9):690{691, September 1979.[10] G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. Strategies for multilocus linkage analysis in humans. Proc. Natl. Acad. Sci. USA, 81:3443-3446, June 1984.[11] E. Lee, R. Bixby, W. Cook, and A.L. Cox. Parallelism in mixed integer programming. Submitted for publication, 1994.[12] D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. The directory-based cache coherence protocol for the DASH multiprocessor. In Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 148-159, May 1990.[13] K. Li and P. Hudak. Memory coherence in shared virtual memory systems. ACM Transactions on Computer Systems, 7(4):321-359, November 1989.[14] M. Litzkow, M. Livny, and M. Mutka. Condor - a hunter of idle workstations. In Proceedings of the 8th International Conference on Distributed Computing Systems, pages 104-111, June 1988.[15] V. Sunderam. PVM: A framework for parallel distributed computing. Concurrency:Practice and Experience, 2(4):315-339, December 1990.[16] A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal. Parallel programming using shared ob jects and broadcasting. IEEE Computer, 25(8):10-20, August 1992.[17] A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, J. Jansen, and G. van Rossum. Experiences with the Amoeba distributed operating system. Communications of the ACM, 33(12):46-63, December 1990.]]></content>
      <categories>
        <category>2020</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grpc]]></title>
    <url>%2F2018%2F06%2F02%2Fgrpc%2F</url>
    <content type="text"><![CDATA[grpc 特性、原理、实践、生态 gRPC概述gRPC是一个由google设计开发基于HTTP/2协议和Protobuf序列化协议的的高性能、多语言、通用的开源 RPC 框架。 跨语言、跨平台插件化 ： 负载均衡，tracing，健康检查，认证等等编码压缩 ： 节省带宽多路复用 ： 降低的 TCP 链接次数 使用场景 低延迟、高扩展的分布式系统 与云服务通信 设计一个需要准确，高效且与语言无关的新协议 分层设计，以实现扩展，例如：身份验证，负载平衡，日志记录和监控等 特性基于HTTP/2 HTTP/2 提供了 链接多路复用、双向流、服务器推送、请求优先级、首部压缩等机制。gRPC 协议使用了HTTP2 现有的语义，请求和响应的数据使用HTTP Body 发送，其他的控制信息则用Header 表示。 IDL使用ProtoBuffer gRPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议（类似于XML、JSON）。ProtoBuf能够将数据进行序列化，并广泛应用在数据存储、通信协议等方面。压缩和传输效率高，向后兼容，语法简单，表达力强。 多语言支持 gRPC支持多种语言，并能够基于语言自动生成客户端和服务端。 目前支持： C#, C++, Dart, Go, Java, Node, Objective-C, PHP, Python, Ruby 等。 详见官网 HTTP/2HTTP/2HTTP/1.x 是超文本传输协议第1版，可读性好，但效率不高。而HTTP/2 是超文本传输协议第2版，是一个二进制协议。 HTTP/1 和 HTTP/2 的基本语义并没有改变，如方法语义（GET/PUST/PUT/DELETE），状态码（200/404/500等），Range Request，Cacheing，Authentication、URL路径。 HTTP/2通用术语： Stream： 流，一个双向流，一条连接可以有多个 streams。 Message： 逻辑上面的 request，response。 Frame：帧，HTTP/2 数据传输的最小单位。每个 Frame 都属于一个特定的 stream。一个 message 可能由多个 frame 组成。 HTTP/2 流、帧 HTTP/2连接上传输的每个帧(frame)都关联到一个流，一个连接上可以同时有多个流，同一个流的帧按序传输，不同流的帧交错混合传输，客户端、服务端双方都可以建立流，流也可以被任意一方关闭。客户端发起的流使用奇数流ID，服务端发起的使用偶数。 Frame结构 : 123456789+-----------------------------------------------+| Length (24) |+---------------+---------------+---------------+| Type (8) | Flags (8) |+-+-------------+---------------+-------------------------------+|R| Stream Identifier (31) |+=+=============================================================+| Frame Payload (0...) ...+---------------------------------------------------------------+ Length ： 也就是 Frame 的长度 Type ：Frame 的类型，有 DATA，HEADERS，SETTINGS 等 Flags ：帧标志位，8个比特表示可以容纳8个不同的标志：stream是否结束(END_STREAM)，header是否结束(END_HEADERS)，priority等等 R：保留位 Stream Identifier：标识frame所属的 stream，如果为 0，则表示这个 frame 属于整条连接(如SETTINGS帧) Frame Payload：帧内容 帧类型 HEADERS 类似于HTTP/1的 Headers DATA 类似于HTTP/1的 Body CONTINUATION 头部太大，分多个帧传输（一个HEADERS+若干CONTINUATION） SETTINGS 连接设置 WINDOW_UPDATE 流量控制 PUSH_PROMISE 服务端推送 PRIORITY 流优先级更改 PING 心跳或计算RTT RST_STREAM 马上中止一个流 GOAWAY 关闭连接并且发送错误信息 HTTP/2 特性新的二进制格式（Binary Format） HTTP/1 的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同。基于这种考虑HTTP/2的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing） HTTP/1 的request是阻塞的，如果想并发发送多个request，必须使用多个 TCP connection。这样会消耗更多资源，且浏览器为了控制资源，会对单个域名有TCP connection请求限制。 HTTP/2 一个TCP connection可以有多个streams(最大数量由参数SETTINGS_MAX_CONCURRENT_STREAMS控制)， 多个streams 并行发送不同的请求的frames。 可以在SETTINGS帧中设置SETTINGS_MAX_CONCURRENT_STREAMS。而此值是针对一端而言的，客户端可以告知服务器最大的streams并发数，服务端也可以告知客户端。 如果一条链接上 ID 分配完了， server 则会给 client 发送一个 GOAWAY frame 强制让 client 新建一条连接。 header压缩 HTTP/1 是使用文本协议，而且header每次都要重复发送，浪费了带宽也导致资源加载过慢。 HTTP/2 采取了压缩和缓存来避免重复发送和带宽问题： 对消息头采用HPACK 进行压缩传输来节省消息头占用的网络的流量。 对这些headers采取了压缩策略来减少重复headers的请求数 HTTP/2在客户端和服务器端使用 headlist 来存储之前发送过的 header，对于相同的header，不再通过每次请求和响应发送； HPACK: Header Compression for HTTP/2 服务端推送 server push功能 : 在无需客户端请求资源的情况下，服务端会直接推送客户端可能需要的资源到客户端。 当服务器想用Server Push推送资源时，会先向客户端发送PUSH_PROMISE帧。推送的响应必须与客户端的某个请求相关联，因此服务器会在客户端请求的流上发送PUSH_PROMISE帧。 优先级排序 设置优先级的目的是为了告诉对端在并发的多个流之间如何分配资源的行为，同时当发送容量有限时，可以使用优先级来选择用于发送帧的流。 客户端可以通过 HEADERS 帧的 PRIORITY 信息指定一个新建立流的优先级，也可以发送 PRIORITY 帧调整流优先级。 参考官网 Flow Control HTTP/2 支持流控，receiver 端可以对某些stream进行流控也可以针对整个connection流控。而TCP层只能针对整个connection进行流控。 特性 ： Flow control 是由方向的 : Receiver 可以选择给 stream 或者整个连接设置接收端的 window size。 Flow control 是基于信任的 : Receiver 只是会给 sender 建议 连接和 stream 的 flow control window size。 Flow control 无法禁止 Flow control 是基于WINDOW_UPDATE帧的 Flow control 是 hop-by-hop的，而不是 end-to-end 的。例如，用nginx做proxy，则flow control作用于nginx到server和client到nginx这两个connection。 Connection 和 stream 的初始 flow-control window 大小都是 65535。Connection 的初始窗口大小不能改变，但 stream 的可以(所有stream)，通过发送 SETTINGS 帧，携带 SETTINGS_INITIAL_WINDOW_SIZE，这个值即为新的 stream flow-control window 初始大小。 增加flow control window size能加快数据传输，但同时会消耗更多资源。 主动重置链接 HTTP/1 的body的length的被送给客户端后，服务端就无法中断请求了，只能断开整个TCP connection，但这样导致的代价就是需要重新通过三次握手建立一个新的TCP连接。 HTTP/2 引入了一个 RST_STREAM frame 来让客户端在已有的连接中发送重置请求，从而中断或者放弃响应。当浏览器进行页面跳转或者用户取消下载时，它可以防止建立新连接，避免浪费所有带宽。 HTTP/2 站点demoHTTP/1 和 HTTP/2 加载速度比较：https://http2.akamai.com/demo 访问http2站点 ：https://http2.golang.org/ ProtoBufProtoBufGoogle Protocol Buffer 是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化。适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。 描述简单，对开发人员友好 跨平台、跨语言，不依赖于具体运行平台和编程语言 高效自动化解析和生成 压缩比例高 可扩展、兼容性好 gRPC与protobuf gRPC使用 protobuf 作为IDL来定义数据结构和服务。 可以定义数据结构，也可以定义rpc 接口。然后用proto编译器生成对应语言的框架代码。 定义数据结构 ： 生成对象的 序列化 代码 定义rpc接口 ： 生成 gRPC服务端、客户端响应的代码 protobuf 基本数据类型https://developers.google.com/protocol-buffers/docs/proto#scalar 数据结构定义user.proto 1234567891011121314151617181920212223242526272829syntax = &quot;proto3&quot;;import &quot;google/protobuf/any.proto&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; google.protobuf.Any details = 11;&#125; package package声明符，用来防止不同的消息类型有命名冲突。生成的代码将会包含再package(go等语言)或者命名空间(c++, java等)中。 option go_package = &quot;protos_golang/user&quot;;$LANGUAGE_package 是指定生成的代码的import path和package。 import 要导入其他.proto文件的定义，在文件中添加一个导入声明。使用导入proto的类型 package名字.结构名 来使用导入proto的类型。如上面common.Flag 分配字段编号 每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的。为了保证向后兼容，一旦开始使用就不要再改变。 文件版本申明 syntax = &quot;proto2&quot;; 指定使用proto2语法syntax = &quot;proto3&quot;; 指定为proto3语法 标识符修饰符 required 和 optional 是proto2的语法，proto3已经不支持。proto3中所有的字段都是optional的。具体原因见 required : 必须字段。 optional ：可选字段。 repeated ：数组类型字段。 reserved ：保留字段。指出这些字段编号已经删除，不要再重用这些编号了。因为如果这些编号被重新定义成其他类型，那么对于旧版本的protobuf数据，会导致解码错误。 枚举 与数据结构中 enum 类似。字段编号从0开始。 oneof oneof与数据结构联合体(UNION)类似，一次最多只有一个字段有效。 map map 类型则可以用来表示键值对。key_type 可以是任何 int 或者 string 类型，float、double 和 bytes除外 any Any类型包括: bytes : 被序列化为bytes类型的任意消息 URL : 全局标识符 使用import google/protobuf/any.proto来导入any类型 any可以用来替换proto2中的extension 嵌套类型 可以在消息类型中定义其他消息类型 服务定义12345678910111213141516syntax = &quot;proto2&quot;;import &quot;user.proto&quot;;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; user.User user = 1;&#125; 如果在 .proto 文件中定义了 RPC 服务接口， 编译器将使用生成服务接口代码和 stubs。 import &quot;user.proto&quot;; 导入user结构定义的proto文件。 插件protoc编译器通过插件机制实现对不同语言的支持。protoc会先查找是否有内置的语言插件，如果没有，则会去查找系统中是否存在protoc-gen-$LANGUAGE 的插件。例如：如果指定--go_out参数，那么protoc会查询是否有内置的go插件，如果没有则继续查询系统中是否存在protoc-gen-go的可执行程序，再通过插件来生成相关的语言代码。 插件运行流程： protoc 启动 protoc-gen-xx 将CodeGeneratorRequest的protobuf二进制传入到 protoc-gen-xx的标准输入 protoc-gen-xx读取标准输入再反序列化成CodeGeneratorRequest 遍历CodeGeneratorRequest的FileDescriptorProto数组，其描述了proto文件的语法树 将FileDescriptorProto编译成语言源码 生成CodeGeneratorResponse对象输出到标准输出 protoc根据protoc-gen-xx的标准输出再生成源码文件 plugin.proto定义了CodeGeneratorRequest 和 CodeGeneratorResponse，是protoc与插件交互的对象。 descriptor.proto描述的是一个.proto文件的语法树 插件的plugins 插件本身的也是支持以内部plugins形式进行扩展的。 例如：生成go grpc的命令中： 1protoc --go_out=plugins=grpc:. pb/user.proto grpc就是 proto-gen-go的plugin。 代码 Name()返回grpc命名就是plugin的名字，就是上面plugins=grpc 1234// Name returns the name of this plugin, &quot;grpc&quot;.func (g *grpc) Name() string &#123; return &quot;grpc&quot;&#125; gRPC 原理概念 gRPC 定义服务，服务包含远程调用的方法。在服务器端，服务器实现rpc接口并运行一个gRPC服务器来处理客户端请求。在客户端，客户端有一个”存根stub”，提供与服务器相同签名的方法，来处理客户端请求的编码、解码等，再将请求转发到服务器端，这样客户端调用rpc方法就像调用本地函数一样。 实现gRPC把HTTP2的steam identifier当作请求ID，每一次请求都发起一个新的stream。 请求的方法、响应的状态码等都放在HEADER frame中。而请求内容和响应内容由protobuf序列化后使用DATA frame中。 请求Request主要由 Request-Headers 和 Data 以及 EOS (END_STREAM)组成。 如下图： Request-Headers Request-Headers 由 HEADERS 和 CONTINUATION frames 组成。如果Flags有设置标志位END_HEADERS则代表Request-Headers结束。 Request-Headers 主要有 Call-Definition 以及 Custom-Metadata : Call-Definition : 包括 Method, Scheme, Path, TE, Authority, Timeout, Content-Type ,Message-Type, Message-Encoding, Message-Accept-Encoding, User-Agent Custom-Metadata : 应用层自定义的任意 key-value，key 不要使用gRPC保留的key前缀字符 grpc- 。 Data 请求体，由一个或多个 Data frame组成。如果Flags有设置标志位END_STREAM则代表Data结束，请求结束。 request格式大致如下 1234567891011121314151617# request-headers HEADERS (flags = END_HEADERS):method = POST:scheme = http:path = /user.UserService/GetUserInfo:authority = localhost:50000grpc-timeout = 999127ucontent-type = grpc-go/1.20.0-dev## 自定义metadataservice : test_clienttraceid : xxxx# dataDATA (flags = END_STREAM)&lt;Length-Prefixed Message&gt; 响应Response 主要由 Response-Headers 和 Data 以及 Trailers 组成。如果遇到了错误，也可以直接返回 Trailers-Only。 如下图： Response-Headers Response-Headers 包含 : HTTP-Status, Message-Encoding, Message-Accept-Encoding, Content-Type, Custom-Metadata等。 Data 响应体，由一个或多个 Data frame组成。如果Flags有设置标志位END_STREAM则代表Data结束。 Trailers Trailers-Only 包含 HTTP-Status, Content-Type, Trailers等。 Trailers 包含 Status, Status-Message, Custom-Metadata等。 Trailers作用主要是给响应包含一些额外的动态生成的信息。如：消息body发送后，再发送一些信息 如数字签名，后处理状态等 格式大致如下： 123456789101112131415161718192021# response-headersHEADERS (flags = END_HEADERS):status = status: 200 content-type = application/grpc## 自定义metadataservice: server_testspanid: xxxx# dataDATA&lt;Length-Prefixed Message&gt;# headersHEADERS (flags = END_STREAM, END_HEADERS)grpc-status: 0## trailers 自定义metadatatimestamp: 1560656283730441829 Status code HTTP状态码对应的gRPC状态码 gRPC通信方式gRPC有四种通信方式: 1、 unary RPC 一般的rpc调用，客户端发送一个请求对象，然后等待服务端返回一个响应对象 123# 获取用户信息# protorpc GetUserInfo (UserRequest) returns (UserResponse) &#123;&#125; 2、 Server-side streaming RPC 服务端流式rpc 客户端发起一个请求到服务端，服务端返回一段连续的数据流响应。 123# 获取一个用户的所有地理位置历史记录# protorpc UserLocationsStream(UserRequest) returns (stream LocationsResponse) &#123;&#125; 3、 Client-side streaming RPC 客户端流式rpc 客户端将一段连续的数据流发送到服务端，服务端返回一个响应。 123# 客户端将所有数据备份到服务端# protorpc BackupStream(stream BackupRequest) returns (BackupResponse) &#123;&#125; 4、 Bidirectional streaming RPC 双向流式rpc 客户端将连续的数据流发送到服务端，服务端返回交互的数据流。 123# 在线聊天# protorpc LiveChat(stream Message) returns (stream Message) &#123;&#125; 配置waitForReady 发送请求时，如果connection没有ready，则会一直等待connection ready 或直到超时(达到deadline)。也常称为fail fast。 timeout 请求超时时间。如果超时，则会中止请求且返回DEADLINE_EXCEEDED 错误。 maxRequestMessageBytes 请求体的最大payload size(没有压缩的)。如果客户端请求大于此值的请求会返回RESOURCE_EXHAUSTED错误。 maxResponseMessageBytes 响应体的最大payload size(没有压缩的)。如果服务端响应大于此值，响应将发送失败。且客户端会得到RESOURCE_EXHAUSTED错误。 gRPC 实践实践部分以go语言进行demo 环境安装protoc mac 1brew install protobuf linux 123456PROTOC_ZIP=protoc-3.5.1-linux-x86_64.zipcurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/$PROTOC_ZIPsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protocsudo unzip -o $PROTOC_ZIP -d /usr/local include/*rm -f $PROTOC_ZIP golang的protobuffers插件 1go get -u github.com/golang/protobuf/&#123;protoc-gen-go,proto&#125; Coding定义proto文件123456789101112131415161718192021222324252627282930313233343536373839404142syntax = &quot;proto3&quot;;import &quot;google/protobuf/any.proto&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; google.protobuf.Any details = 11;&#125;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; User user = 1;&#125; 生成代码生成代码的导入路径和包名 123## protos_golang ： 生成代码的路径## user : golang package 名option go_package = &quot;protos_golang/user&quot;; 目标代码 如果包含rpc接口：则需要指定插件plugins=grpc --go_out=. ： 生成的代码在当前目录; 也可以指定其他目录，如:--go_out=/tmp 代码路径 ： 如果.pb中指定了go_package : 代码路径是 ./$go_package/user.pb.go 如果.pb中没有指定go_package : 则代码路径是 ./pb/user.pb.go 1234protoc --go_out=plugins=grpc:. pb/user.proto# 如果没有rpc定义protoc --go_out=. pb/user.proto 服务端1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mainimport ( &quot;context&quot; &quot;log&quot; &quot;net&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( port = &quot;:50000&quot;)// grpc servertype server struct&#123;&#125;// 实现gRPC接口func (s *server) GetUserInfo(ctx context.Context, in *pb.UserRequest) (*pb.UserResponse, error) &#123; return &amp;pb.UserResponse&#123; User: &amp;pb.User&#123; Name: &quot;test_user&quot;, &#125;, &#125;, nil&#125;// 拦截器，简单打印下日志func LogUnaryInterceptorMiddleware() grpc.UnaryServerInterceptor &#123; return func(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (r interface&#123;&#125;, err error) &#123; r, err = handler(ctx, req) fmt.Printf(&quot;fullMethod(%s), errCode(%v)\n&quot;, info.FullMethod, err) return r, err &#125;&#125;func main() &#123; lis, err := net.Listen(&quot;tcp&quot;, port) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; // 拦截器 options := grpc.UnaryInterceptor(LogUnaryInterceptorMiddleware()) s := grpc.NewServer(options) // 注册服务器实现 pb.RegisterUserServiceServer(s, &amp;server&#123;&#125;) // 注册服务端反射 reflection.Register(s) // 启动服务器 if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 客户端123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;google.golang.org/grpc&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( address = &quot;localhost:50000&quot;)func main() &#123; conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := pb.NewUserServiceClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.GetUserInfo(ctx, &amp;pb.UserRequest&#123;&#125;) if err != nil &#123; log.Fatalf(&quot;fatal: %v&quot;, err) &#125; log.Printf(&quot;response: %s&quot;, r)&#125; 调试为了方便调试服务端，所以服务端需要支持reflection功能。 1reflection.Register(grpcServer) 两款比较著名的调试工具： [grpc_cli](https://github.com/grpc/grpc/blob/master/doc/command_line_tool.md : 官方的 grpcurl : go的，安装简单 列出服务端注册的service 如果没有配置好公钥和私钥文件，也没有忽略证书的验证过程，则需要加-plaintext 123$ grpcurl -plaintext localhost:50000 list grpc.reflection.v1alpha.ServerReflectionuser.UserService 列出服务的接口 12$ grpcurl -plaintext localhost:50000 list user.UserServiceuser.UserService.GetUserInfo 获取接口的签名 123$ grpcurl -plaintext localhost:50000 describe user.UserService.GetUserInfouser.UserService.GetUserInfo is a method:rpc GetUserInfo ( .user.UserRequest ) returns ( .user.UserResponse ); 获取类型信息 12345$ grpcurl -plaintext localhost:50000 describe .user.UserRequestuser.UserRequest is a message:message UserRequest &#123; uint32 id = 1;&#125; 调试接口 请求体以json的形式描述类型。 123456$ grpcurl -plaintext -d &apos;&#123;&quot;id&quot;:1&#125;&apos; localhost:50000 user.UserService.GetUserInfo&#123; &quot;user&quot;: &#123; &quot;name&quot;: &quot;test_user&quot; &#125;&#125; go gRPC 生态服务组件上下文信息传递 rpc客户端将上下文信息传递给服务端。链路调用信息，服务信息，认证信息等等。 官方实现 服务器反射 服务端反射协议， 可以用途于: 服务端调试 : grpcurl 工具就是用reflection协议来进行服务端调试的。可以list出服务端的接口定义，以及命令行构造请求进行调试。 运行时构造gRPC请求 ：客户端可以运行时根据反射的接口定义构造请求。 官方实现 负载均衡 客户端负载均衡器 官方实现 认证 gRPC主要的两种认证方式： 基于SSL/TLS认证方式 Token认证方式 两种方式可以同时应用 官方实现 实现了几种认证方式： alts google oauth 自定义认证方式 go-grpc-middleware的实现 健康检查 服务端提供一个Check接口返回其状态信息。客户端调用此接口获取到服务健康状态，是否可以继续提供服务。 官方实现 keepalive 定期发送HTTP/2.0 pings帧来检测 connection 是否存活，如果断开则进行重新连接。与健康检查区别在于keepalive是检查connection而健康检查是检查服务是否可用。 官方实现 naming 命名解析。通过服务命名来获取服务相关的信息来达到服务发现目的。 与balancer结合使用来实现进程内负载均衡与服务发现。 官方实现 限流 限制流量来保护服务端以防止服务过载。 可以在客户端，balancer，服务端 进行限流。 go-grpc-middleware实现服务端限流 recovery 将服务内部的错误转换成gRPC错误码。 go-grpc-middleware实现 ： recover go的panic， 并转换成gRPC错误。 重试 客户端对于返回某些gRPC错误码的请求进行重试。 go-grpc-middleware tracing 在链路上下文携带tracing信息，以及将信息以opentracing的规范发送给分布式链路分析服务。 tracing信息包含traceid,spanid,请求时间,错误信息,日志等等。如：通过设置客户端spanid为服务端spanid的parent_spanid，这样就能知道是客户端调用了服务端rpc请求。 go-grpc-middleware实现opentracing的middleware open-tracing 微服务框架、组件go-kit : 微服务组件micro : 微服务框架go-chassis : 华为开发的go微服务框架go-grpc-middleware : 服务端和客户端的一些中间件，认证、日志、分布式追踪跟重试等grpc-gateway ：一个 protoc 的插件，可以将 gRPC 接口转换为对外暴露 RESTful API 的工具，同时还能生成 swagger 文档 gRPC 与 负载均衡进程内LB(Balancing-aware Client) 需要实现： 服务注册 健康检查 服务发现 负载均衡 缺点： 开发成本：要实现上述功能 维护成本：不同语言栈的sdk维护与升级 官方已经提供接口来实现进程内的负载均衡。同时结合服务发现，健康检查一起使用。 集中式LB(Proxy Model) proxy 实现服务发现，健康检查，负载均衡等等。还方便做限流等控制和其他统一控制策略。 缺点： 单点问题 多一层性能开销 不方便调试 Nginx Nginx(1.13.10已经支持gRPC) 1234567891011121314151617181920212223upstream grpcservers &#123; server localhost:50000; server localhost:50001;&#125;server &#123; listen 9000 http2; # router location /user.UserService &#123; grpc_pass grpc://grpcservers; error_page 502 = /error502grpc; &#125; # 将默认错误页面更改成gRPC状态码 location = /error502grpc &#123; internal; default_type application/grpc; add_header grpc-status 14; add_header grpc-message &quot;unavailable&quot;; return 204; &#125;&#125; nginx gRPC module 独立LB进程(External Load Balancing Service) 在主机上部署独立的LB进程，来实现服务发现，健康检查，负载均衡等功能。不用对于不同语言维护不同sdk版本；常常用于微服务service mesh。 缺点： 单点问题：但是只影响本机 不方便调试 常用的组件： Istio Envoy gRPC 生态环境组件 grpc 只是实现了 RPC 核心功能，缺少很多微服务的特性（服务注册发现、监控、治理、管理等），而基于 HTTP/2 相对来说比较容易进行扩展。 grpc-ecosystem 上有一些比较优秀的外围组件来完善gRPC的生态体系 awesome-grpc 收集了一些优秀的gRPC项目 grpc 文档与交流文档 官网文档 : https://grpc.io/docs/ github 上 grpc 仓库下的 doc ： https://github.com/grpc/grpc/tree/master/doc 博客 : https://grpc.io/blog/ 交流 https://grpc.io/community/ 交流的方式有： 邮件列表 Gitter Reddit Meetup Group 参考grpc.io developers.google.com gRPC github doc http2 specsorgithub http2 spec]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NUMA]]></title>
    <url>%2F2018%2F04%2F12%2Fnuma%2F</url>
    <content type="text"><![CDATA[NUMA 概念、历史、问题 NUMA 概念NUMA的几个概念（Node，socket，core，thread） socket就是主板上的CPU插槽; core就是socket里独立的一组程序执行的硬件单元，比如寄存器，计算单元等; thread：就是超线程hyperthread的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。 NUMA体系结构中多了Node的概念，这个概念其实是用来解决core的分组的问题，具体参见下图来理解（图中的OS CPU可以理解thread，那么core就没有在图中画出），从图中可以看出每个Socket里有两个node，共有4个socket，每个socket 2个node，每个node中有8个thread，总共4（Socket）× 2（Node）× 8 （4core × 2 Thread） = 64个thread。 另外每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量，因为Node内有自己内部总线，所以增加CPU数量可以通过增加Node的数目来实现，如果单纯的增加CPU的数量，会对总线造成很大的压力，所以UMA结构不可能支持很多的核。 《NUMA Best Practices for Dell PowerEdge 12th Generation Servers》 根据上面提到的，由于每个node内部有自己的CPU总线和内存，所以如果一个虚拟机的vCPU跨不同的Node的话，就会导致一个node中的CPU去访问另外一个node中的内存的情况，这就导致内存访问延迟的增加。在有些特殊场景下，比如NFV(Network Function Virtualization)环境中，对性能有比较高的要求，就非常需要同一个虚拟机的vCPU尽量被分配到同一个Node中的pCPU上，所以在OpenStack的Kilo版本中增加了基于NUMA感知的虚拟机调度的特性。 查看机器的NUMA拓扑结构123456789101112131415161718192021222324[root@local ~]$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 48 // 48个逻辑CPU（threads）On-line CPU(s) list: 0-47Thread(s) per core: 2 // 每个core有2个threadsCore(s) per socket: 12 // 每个socket有12个coresSocket(s): 2 // 共总有2个socketsNUMA node(s): 2 // 2个NUMA nodesVendor ID: GenuineIntelCPU family: 6Model: 63Model name: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHzStepping: 2CPU MHz: 2500.089BogoMIPS: 4999.27Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 30720KNUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47 可以看出当前机器有2个sockets，每个sockets包含1个numa node，每个numa node中有12个cores，每个cores包含2个thread，所以总的threads数量=2x1x12x2=48. NUMA 历史在若干年前，对于x86架构的计算机，那时的内存控制器还没有整合进CPU，所有内存的访问都需要通过北桥芯片来完成。此时的内存访问如下图所示，被称为UMA（uniform memory access, 一致性内存访问）。这样的访问对于软件层面来说非常容易实现：总线模型保证了所有的内存访问是一致的，不必考虑由不同内存地址之前的差异。 之后的x86平台经历了一场从“拼频率”到“拼核心数”的转变，越来越多的核心被尽可能地塞进了同一块芯片上，各个核心对于内存带宽的争抢访问成为了瓶颈；此时软件、OS方面对于SMP多核心CPU的支持也愈发成熟；再加上各种商业上的考量，x86平台也搞了NUMA（Non-uniform memory access, 非一致性内存访问）。 NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die(核心)上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）。所以NUMA（Non-Uniform Memory Access）就此得名 在这种架构之下，每个Socket都会有一个独立的内存控制器IMC（integrated memory controllers, 集成内存控制器），分属于不同的socket之内的IMC之间通过QPI link通讯。 然后就是进一步的架构演进，由于每个socket上都会有多个core进行内存访问，这就会在每个core的内部出现一个类似最早SMP架构相似的内存访问总线，这个总线被称为IMC bus。 于是，很明显的，在这种架构之下，两个socket各自管理1/2的内存插槽，如果要访问不属于本socket的内存则必须通过QPI link。也就是说内存的访问出现了本地/远程（local/remote）的概念，内存的延时是会有显著的区别的。 以Xeon 2699 v4系列CPU的标准来看，两个Socket之之间通过各自的一条9.6GT/s的QPI link互访。而每个Socket事实上有2个内存控制器。双通道的缘故，每个控制器又有两个内存通道（channel），每个通道最多支持3根内存条（DIMM）。理论上最大单socket支持76.8GB/s的内存带宽，而两个QPI link，每个QPI link有9.6GT/s的速率（~57.6GB/s）事实上QPI link已经出现瓶颈了。 核心数还是源源不断的增加，Skylake桌面版本的i7 EE已经有了18个core，Skylake Xeon 28个Core(2017)。为了塞进更多的core，原本核心之间类似环网的设计变成了复杂的路由。由于这种架构上的变化，导致内存的访问变得更加复杂。两个IMC也有了local/remote的区别，在保证兼容性的前提和性能导向的纠结中，系统允许用户进行更为灵活的内存访问架构划分。于是就有了“NUMA之上的NUMA”这种妖异的设定（SNC）。 性能提升内核调度和操作方式 在一个启用了NUMA支持的Linux中，Kernel不会将任务内存从一个NUMA node搬迁到另一个NUMA node。 一个进程一旦被启用，它所在的NUMA node就不会被迁移，为了尽可能的优化性能，在正常的调度之中，CPU的core也会尽可能的使用可以local访问的本地core，在进程的整个生命周期之中，NUMA node保持不变。 一旦当某个NUMA node的负载超出了另一个node一个阈值（默认25%），则认为需要在此node上减少负载，不同的NUMA结构和不同的负载状况，系统见给予一个延时任务的迁移——类似于漏杯算法。在这种情况下将会产生内存的remote访问。 NUMA node之间有不同的拓扑结构，各个 node 之间的访问会有一个距离（node distances）的概念，如numactl -H命令的结果有这样的描述： 123456789101112[root@local ~]$ numactl -Havailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73363 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117527 MBnode distances:node 0 1 0: 10 21 1: 21 10 可以看出：0 node 到0 node之间距离为10，是最近的距离。 上图记录了某个Benchmark工具，在开启/关闭NUMA功能时QPI带宽消耗的情况。很明显的是，在开启了NUMA支持以后，QPI的带宽消耗有了两个数量级以上的下降，性能也有了显著的提升！ 通常情况下，用户可以通过numactl来进行NUMA访问策略的手工配置，cgroup中cpuset.mems也可以达到指定NUMA node的作用。 Numa内存分配策略有四种: 缺省default:总是在本地节点分配(当前进程运行的节点上)。 绑定bind:强制分配到指定节点上。 交叉interleavel:在所有节点或者指定节点上交叉分配内存。 优先preferred:在指定节点上分配，失败则在其他节点上分配 以numactl命令为例，它有如下策略： –interleave=nodes //允许进程在多个node之间交替访问 –membind=nodes //将内存固定在某个node上，CPU则选择对应的core。 –cpunodebind=nodes //与membind相反，将CPU固定在某（几）个core上，内存则限制在对应的NUMA node之上。 –physcpubind=cpus //与cpunodebind类似，不同的是物理core。 –localalloc //本地配置 –preferred=node //按照推荐配置 对于某些大内存访问的应用，比如Mongodb，将NUMA的访问策略制定为interleave=all则意味着整个进程的内存是均匀分布在所有的node之上，进程可以以最快的方式访问本地内存。北桥有一个功能就是PCI/PCIe控制器，南桥（PCH）整合了PCIe控制器。在PCIe channel上也是有NUMA亲和性的。 比如：查看网卡em1的NUMA 12345678[root@local ~]$ numactl --prefer netdev:em1 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 PCI address 为00:1f.2的SATA控制器，用到了pci:00:1f.2 SATA controller: Intel Corporation C610/X99 series chipset 6-Port SATA Controller [AHCI mode] (rev 05) 1234567[root@local ~]$ numactl --prefer pci:00:1f.2 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前系统numa策略： 1234567[root@local ~]$ numactl --showpolicy: defaultpreferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前numa的节点情况： 123456789101112[root@local ~]$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73338 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117521 MBnode distances:node 0 1 0: 10 21 1: 21 10 NUMA带来的问题 MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases 这些问题都是：“因为CPU亲和策略导致的内存分配不平均”及“NUMA Zone Claim内存回收”有关，而和数据库种类并没有直接联系。 数据库与NUMAMySQL在NUMA架构上遇到的典型问题 The MySQL “swap insanity” problem and the effects of the NUMA architecture A brief update on NUMA and MySQL 大致分析如下： CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核CPU通过一条总线共享内存成为瓶颈 于是NUMA出现了，CPU平均划分为若干个Chip（不多于4个），每个Chip有自己的内存控制器及内存插槽 CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存（下文称Remote Access）的速度相较慢三倍左右 于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中 由于内存页没有动态调整策略，使得大部分内存页都集中在CPU 0上 又因为Reclaim默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出 当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现了 解决方案： numactl –interleave=all 在MySQL进程启动前，使用sysctl -q -w - vm.drop_caches=3清空文件缓存所占用的空间 Innodb在启动时，就完成整个Innodb_buffer_pool_size的内存分配 不过这种三合一的解决方案只是减少了NUMA内存分配不均，导致的MySQL SWAP问题出现的可能性。如果当系统上其他进程，或者MySQL本身需要大量内存时，Innodb Buffer Pool的那些Page同样还是会被Swap到存储上。于是又在这基础上出现了另外几个进阶方案 配置vm.zone_reclaim_mode = 0使得内存不足时去remote memory分配优先于swap out local page echo -15 &gt; /proc//oom_adj调低MySQL进程被OOM_killer强制Kill的可能 memlock 对MySQL使用Huge Page（黑魔法，巧用了Huge Page不会被swap的特性） 为什么Interleave的策略就解决了问题？借用两张 Carrefour性能测试 的结果图，可以看到几乎所有情况下Interleave模式下的程序性能都要比默认的亲和模式要高，有时甚至能高达30%。究其根本原因是Linux服务器的大多数workload分布都是随机的：即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分布的。而Interleave模式就恰恰是针对这种特性将内存page随机打散到各个CPU Core上，使得每个CPU的负载和Remote Access的出现频率都均匀分布。相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多。 也就是说，像MySQL这种外部请求随机性强，各个线程访问内存在地址上平均分布的这种应用，Interleave的内存分配模式相较默认模式可以带来一定程度的性能提升。此外各种论文 中也都通过实验证实，真正造成程序在NUMA系统上性能瓶颈的并不是Remote Acess带来的响应时间损耗，而是内存的不合理分布导致Remote Access将interconnect这个小水管塞满所造成的结果。而Interleave恰好，把这种不合理分布情况下的Remote Access请求平均分布在了各个小水管中。所以这也是Interleave效果奇佳的一个原因。 那是不是简简单单的配置个Interleave就已经把NUMA的特性和性能发挥到了极致呢？答案是否定的，目前Linux的内存分配机制在NUMA架构的CPU上还有一定的改进空间。例如：Dynamic Memory Loaction, Page Replication。 Dynamic Memory RelocationMySQL的线程分为两种，用户线程（SQL执行线程）和内部线程（内部功能，如：flush，io，master等）。对于用户线程来说随机性相当的强，但对于内部线程来说他们的行为以及所要访问的内存区域其实是相对固定且可以预测的。如果能对于这把这部分内存集中到这些内存线程所在的core上的时候，就能减少大量Remote Access，潜在的提升例如Page Flush，Purge等功能的吞吐量，甚至可以提高MySQL Crash后Recovery的速度（由于recovery是单线程）。那是否能在Interleave模式下，把那些明显应该聚集在一个CPU上的内存集中在一起呢？很可惜，Dynamic Memory Relocation这种技术目前只停留在理论和实验阶段。我们来看下难点：要做到按照线程的行为动态的调整page在memory的分布，就势必需要做线程和内存的实时监控（profile）。对于Memory Access这种非常异常频繁的底层操作来说增加profile入口的性能损耗是极大的。 Page Replication一些动态加载的库，把他们放在任何一个线程所在的CPU都会导致其他CPU上线程的执行效率下降。而这些共享数据往往读写比非常高，如果能把这些数据的副本在每个Memory Zone内都放置一份，理论上会带来较大的性能提升，同时也减少在interconnect上出现的瓶颈。由于缺乏硬件级别（如MESI协议的硬件支持）和操作系统原生级别的支持，Page Replication在数据一致性上维护的成本显得比他带来的提升更多。因此这种尝试也仅仅停留在理论阶段。当然，如果能得到底层的大力支持，相信这个方案还是有极大的实际价值的。 关闭NUMA特性的方法 硬件层，在BIOS中设置关闭 OS内核，启动时设置numa=off 进程，numactl 进程启动时。numactl –interleave=all NUMA取舍指定numa在运行程序的时候使用numactl -m和-physcpubind就能制定将这个程序运行在哪个cpu和哪个memory中:numactl –physcpubind=2,6 ./program 玩转cpu-topology(站点已经无法访问) 的测试中显示当程序只使用一个node资源和使用多个node资源的比较表（差不多是38s与28s的差距）。所以限定程序在numa node中运行是有实际意义的。 指定numa带来的问题SWAP的罪与罚 文章就说到了一个numa的陷阱的问题。现象是当你的服务器还有内存的时候，发现它已经在开始使用swap了，甚至已经导致机器出现停滞的现象。如果一个进程限制它只能使用自己的numa节点的内存，那么当自身numa node内存使用光之后，就不会去使用其他numa node的内存了，会开始使用swap，甚至更糟的情况，机器没有设置swap的时候，可能会直接死机！所以你可以使用numactl --interleave=all来取消numa node的限制。 根据具体业务决定NUMA的使用: 如果你的程序是会占用大规模内存的，你大多应该选择关闭numa node的限制。因为这个时候你的程序很有几率会碰到numa陷阱。 如果你的程序并不占用大内存，而是要求更快的程序运行时间。你大多应该选择限制只访问本numa node的方法来进行处理。 推荐阅读: NUMA-aware scheduler for Go PostgreSQL, NUMA and zone reclaim mode on linux NUMA and Java Databases MySQL Server and NUMA architectures]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What Is the Most Important Thing in Life？]]></title>
    <url>%2F2018%2F01%2F01%2Fwhich-most-important%2F</url>
    <content type="text"><![CDATA[the most important thing in life. the most important thing in life. Keeping healthy. It is health that is real wealth and not pieces of gold and silver. Everybody needs somebody, be that a friend, a partner, or someone you’re related to. Making someone’s day full of sunshine even when yours is not. Money should not be a priority. The beautiful thing about learning is that nobody can take it away from you. Know who you are. Don’t be a victim BUT instead be a hero in your life. Don’t give up. A winner is just a loser who tried on more time. Always make time for gratitude.]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>人生</tag>
      </tags>
  </entry>
</search>
