<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes雪崩了]]></title>
    <url>%2F2021%2F10%2F26%2Fkubernetes_case%2F</url>
    <content type="text"><![CDATA[云原生最重要的组件之一，竟然“雪崩”了…… kubernetes作为云原生不可或缺的重要组件，在编排调度系统中一统天下，成为事实上的标准，如果kubernetes出现问题，那将是灾难性的。而我们在容器化推进的过程中，就遇到了很多有意思的故障，今天我们就来分析一个“雪崩”的案例。 问题描述第一次故障一个黑色星期三，突然收到大量报警：服务大量5xx；某台kubernetes宿主机器CPU飚高到近80%；内存增速过快等等。赶紧打开监控，发现一些服务的超时请求很多，内存急剧增高，gc变慢，CPU、goroutines暴涨等等。 粗略分析看不出到底哪个是因哪些是果，于是启动应急预案，赶紧将此机器上的POD迁移到其他机器上，迁移后服务运行正常，流量恢复。 初步解决问题，开始详细分析原因，从监控看实在分析不出原因，也没有内核日志，只有故障4分钟后的一条OOM kill的日志，没有什么价值。但是从监控上看A服务在此机器上的POD问题尤为严重。 由于当时急于恢复流量，没有保留好现场，以为是某个服务出现什么问题导致的（当时还猜测可能linux某部分隔离性没有做好引发的）。 服务故障部分监控 宿主机 内核OOM日志 第二次故障3个小时后，又一台机器发生故障，先恢复故障的同时使用perf捕获了调用栈，然后又继续骚操作将POD迁移走，这一次就导致其他3台宿主机出现问题了，这3台宿主机上的服务以及依赖这些服务的服务也相继出现问题。 这时，才意识到是什么达到了临界，才会导致其他机器相继出现问题。于是赶紧将一些服务转移到物理机部署，宿主机立即恢复正常）。 宿主机CPU利用率 top查看到很多服务的CPU使用率都很高 perf record 初步分析虽然问题初步解决了，但还得继续分析根本原因。 从perf record来看，大多数进程的调用都blocking在__write_lock_failed函数，大部分都是由于__neigh_create调用造成的，于是先google，搜索到相似问题的文章: https://www.cnblogs.com/tencent-cloud-native/p/14481570.html 但还不能如此草率的下定论，仍然需要进一步分析才能盖棺定论。 内核源码分析线上机器内核较老，3.10.0-1160.36.2.el7.x86_64版本，由于有tcp流量也有udp流量，所以两种协议都要进行分析。 系统调用从系统调用send开始分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len, unsigned int, flags)&#123; return sys_sendto(fd, buff, len, flags, NULL, 0);&#125;SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len, unsigned int, flags, struct sockaddr __user *, addr, int, addr_len)&#123; ...... // 根据fd找到socket sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; // 构造msghdr iov.iov_base = buff; iov.iov_len = len; msg.msg_name = NULL; msg.msg_iov = &amp;iov; msg.msg_iovlen = 1; msg.msg_control = NULL; msg.msg_controllen = 0; msg.msg_namelen = 0; // 发送数据 err = sock_sendmsg(sock, &amp;msg, len); ......&#125;int sock_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)&#123; ...... ret = __sock_sendmsg(&amp;iocb, sock, msg, size); ......&#125;static inline int __sock_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; ..... return err ?: __sock_sendmsg_nosec(iocb, sock, msg, size);&#125;static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; ...... // AF_INET 协议族提供的是inet_sendmsg函数来发送数据 return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);&#125; 1234567int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; ...... return sk-&gt;sk_prot-&gt;sendmsg(iocb, sk, msg, size); &#125;EXPORT_SYMBOL(inet_sendmsg); inet_sendmsg会调用不同协议的发送函数， udp协议：udp_sendmsg tcp协议：tcp_sendmsg 协议层udp1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)&#123; ...... /* Lockless fast path for the non-corking case. */ if (!corkreq) &#123; // 构造skb skb = ip_make_skb(sk, fl4, getfrag, msg-&gt;msg_iov, ulen, sizeof(struct udphdr), &amp;ipc, &amp;rt, msg-&gt;msg_flags); err = PTR_ERR(skb); if (!IS_ERR_OR_NULL(skb)) err = udp_send_skb(skb, fl4); goto out; &#125; ...... return err;&#125;EXPORT_SYMBOL(udp_sendmsg);// static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)&#123; ...... // 填充UDP头，checksum uh = udp_hdr(skb); uh-&gt;source = inet-&gt;inet_sport; uh-&gt;dest = fl4-&gt;fl4_dport; uh-&gt;len = htons(len); ...... uh-&gt;check = csum_tcpudp_magic(fl4-&gt;saddr, fl4-&gt;daddr, len, sk-&gt;sk_protocol, csum); ...... // 发送 err = ip_send_skb(sock_net(sk), skb); if (err) &#123; if (err == -ENOBUFS &amp;&amp; !inet-&gt;recverr) &#123; UDP_INC_STATS_USER(sock_net(sk), UDP_MIB_SNDBUFERRORS, is_udplite); err = 0; &#125; &#125; else ...... return err; &#125;int ip_send_skb(struct net *net, struct sk_buff *skb)&#123; int err; err = ip_local_out(skb); if (err) &#123; if (err &gt; 0) err = net_xmit_errno(err); if (err) IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS); &#125; return err;&#125; 最终调用到ip_local_out函数 tcp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t size)&#123; ...... struct tcp_sock *tp = tcp_sk(sk); lock_sock(sk); ...... /* Ok commence sending. */ // 获取⽤户传递过来的数据和标志 iovlen = msg-&gt;msg_iovlen; iov = msg-&gt;msg_iov; // 数据地址 copied = 0; ...... // 遍历用户层的数据块 while (--iovlen &gt;= 0) &#123; size_t seglen = iov-&gt;iov_len; unsigned char __user *from = iov-&gt;iov_base; // 待发送数据块 iov++; ...... while (seglen &gt; 0) &#123; int copy = 0; int max = size_goal; // 获取发送队列，列队最后一个skb，尝试将现在数据加入skb skb = tcp_write_queue_tail(sk); if (tcp_send_head(sk)) &#123; if (skb-&gt;ip_summed == CHECKSUM_NONE) max = mss_now; copy = max - skb-&gt;len; &#125; if (copy &lt;= 0) &#123; // 需要申请新的skb // 申请 skb，并添加到发送队列的尾部 if (!sk_stream_memory_free(sk)) goto wait_for_sndbuf; skb = sk_stream_alloc_skb(sk, select_size(sk, sg), sk-&gt;sk_allocation); if (!skb) goto wait_for_memory; ...... // 把 skb 挂到socket的发送队列上 skb_entail(sk, skb); copy = size_goal; max = size_goal; &#125; /* Try to append data to the end of skb. */ if (copy &gt; seglen) copy = seglen; /* Where to copy to? */ // skb 中有⾜够的空间 if (skb_availroom(skb) &gt; 0) &#123; // copy⽤户空间的数据到内核空间、计算校验和 // user space data ---- copy -----&gt; skb copy = min_t(int, copy, skb_availroom(skb)); err = skb_add_data_nocache(sk, skb, from, copy); if (err) goto do_fault; &#125; else &#123; ...... &#125; ...... if (forced_push(tp)) &#123; tcp_mark_push(tp, skb); // 发送数据 __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH); &#125; else if (skb == tcp_send_head(sk)) tcp_push_one(sk, mss_now); // 发送 continue; ...... &#125; &#125; ......&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss, int nonagle)&#123; ...... if (tcp_write_xmit(sk, cur_mss, nonagle, 0, sk_gfp_atomic(sk, GFP_ATOMIC))) tcp_check_probe_timer(sk);&#125;static bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle, int push_one, gfp_t gfp)&#123; ...... // 循环获取待发送的skb while ((skb = tcp_send_head(sk))) &#123; ...... // 滑动窗口管理 cwnd_quota = tcp_cwnd_test(tp, skb); ...... // 发送数据 if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp))) break;repair: tcp_event_new_data_sent(sk, skb); tcp_minshall_update(tp, mss_now, skb); sent_pkts += tcp_skb_pcount(skb); if (push_one) break; &#125; if (likely(sent_pkts)) &#123; if (tcp_in_cwnd_reduction(sk)) tp-&gt;prr_out += sent_pkts; /* Send one loss probe per tail loss episode. */ if (push_one != 2) tcp_schedule_loss_probe(sk); tcp_cwnd_validate(sk); return false; &#125; return (push_one == 2) || (!tp-&gt;packets_out &amp;&amp; tcp_send_head(sk));&#125;static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it, gfp_t gfp_mask)&#123; ...... err = icsk-&gt;icsk_af_ops-&gt;queue_xmit(skb, &amp;inet-&gt;cork.fl); if (likely(err &lt;= 0)) return err; return net_xmit_eval(err);&#125; 1234567// 网络层int ip_queue_xmit(struct sk_buff *skb, struct flowi *fl)&#123; ...... res = ip_local_out(skb); return res;&#125; 最终也调用到ip_local_out函数 网络层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859int ip_local_out(struct sk_buff *skb)&#123; int err; err = __ip_local_out(skb); // 构造ip header，netfilter过滤 if (likely(err == 1)) // 允许数据通过 err = dst_output(skb); return err;&#125;int __ip_local_out(struct sk_buff *skb)&#123; // 设置ip头 struct iphdr *iph = ip_hdr(skb); iph-&gt;tot_len = htons(skb-&gt;len); ip_send_check(iph); // checksum // netfilter的hook, iptables的实现 return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL, skb_dst(skb)-&gt;dev, dst_output);&#125;static inline int dst_output(struct sk_buff *skb)&#123; return skb_dst(skb)-&gt;output(skb); // 调用ip_output&#125;int ip_output(struct sk_buff *skb)&#123; ...... // if !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED) 为真 // 将skb发给netfilter，过滤通过，则调用ip_finish_output return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev, ip_finish_output, !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED));&#125;static int ip_finish_output(struct sk_buff *skb)&#123; // 如果内核启用了 netfilter 和数据包转换（XFRM）， // 则更新 skb 的标志并通过 dst_output 来发送#if defined(CONFIG_NETFILTER) &amp;&amp; defined(CONFIG_XFRM) /* Policy lookup after SNAT yielded a new policy */ if (skb_dst(skb)-&gt;xfrm != NULL) &#123; IPCB(skb)-&gt;flags |= IPSKB_REROUTED; return dst_output(skb); &#125;#endif // 如果数据包的长度大于 MTU 并且分片不会 offload 到设备， // 则会调用 ip_fragment 在发送之前对数据包进行分片 if (skb-&gt;len &gt; ip_skb_dst_mtu(skb) &amp;&amp; !skb_is_gso(skb)) return ip_fragment(skb, ip_finish_output2); else return ip_finish_output2(skb);&#125; 邻居子系统重点来了！ 调用ip_finish_output2到达邻居子系统。 12345678910111213141516171819202122232425262728static inline int ip_finish_output2(struct sk_buff *skb)&#123; struct dst_entry *dst = skb_dst(skb); struct rtable *rt = (struct rtable *)dst; struct net_device *dev = dst-&gt;dev; // 出口设备 ...... rcu_read_lock_bh(); // 下一跳 nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)-&gt;daddr); // 根据下一跳和设备名去arp缓存表查询，如果没有，则创建 neigh = __ipv4_neigh_lookup_noref(dev, nexthop); if (unlikely(!neigh)) neigh = __neigh_create(&amp;arp_tbl, &amp;nexthop, dev, false); if (!IS_ERR(neigh)) &#123; int res = dst_neigh_output(dst, neigh, skb); rcu_read_unlock_bh(); return res; &#125; rcu_read_unlock_bh(); kfree_skb(skb); return -EINVAL;&#125; 下一跳为网关，则返回网关地址。 下一跳为POD的IP地址，所以对于macvlan的网络架构来说，同一宿主机的两个POD之间互相访问，都会产生两条ARP表项。123456static inline __be32 rt_nexthop(const struct rtable *rt, __be32 daddr)&#123; if (rt-&gt;rt_gateway) return rt-&gt;rt_gateway; return daddr;&#125; 查找ARP表项 __ipv4_neigh_lookup_noref是根据 出口设备“唯一标示”和目标ip 计算一个hash值，然后在 arp hash table中查找是否存在此arp项所以arp hash table里面存储的条目是全局的 dev：可以到达neighbour的设备，是dev设备指针的值，而此时在内核态，所以可以看作是设备的唯一标示 ip：L3层目标地址(下一跳地址) 12345678910111213141516171819202122232425262728293031323334static inline struct neighbour *__ipv4_neigh_lookup_noref(struct net_device *dev, u32 key)&#123; struct neigh_hash_table *nht = rcu_dereference_bh(arp_tbl.nht); struct neighbour *n; u32 hash_val; // 将dev指针传递给hash函数 hash_val = arp_hashfn(key, dev, nht-&gt;hash_rnd[0]) &gt;&gt; (32 - nht-&gt;hash_shift); for (n = rcu_dereference_bh(nht-&gt;hash_buckets[hash_val]); n != NULL; n = rcu_dereference_bh(n-&gt;next)) &#123; if (n-&gt;dev == dev &amp;&amp; *(u32 *)n-&gt;primary_key == key) return n; &#125; return NULL;&#125;static inline u32 arp_hashfn(u32 key, const struct net_device *dev, u32 hash_rnd)&#123; u32 val = key ^ hash32_ptr(dev); // 计算的是ip + dev指针， return val * hash_rnd;&#125;static inline u32 hash32_ptr(const void *ptr)&#123; unsigned long val = (unsigned long)ptr;#if BITS_PER_LONG == 64 val ^= (val &gt;&gt; 32);#endif return (u32)val;&#125; 创建邻居表项 如果ARP缓存表不存在此项，则创建，创建失败，返回-EINVAL。这就是问题所在了，没有任何内核日志，返回一个有歧义的错误码，也就导致无法快速定位原因。 还好在linux kernel 5.x内核代码中，会打印报错信息：neighbour: arp_cache: neighbor table overflow! 123456789101112131415struct neighbour *__neigh_create(struct neigh_table *tbl, const void *pkey, struct net_device *dev, bool want_ref)&#123; // 分配neighbour // struct neighbour *n1, *rc, *n = neigh_alloc(tbl, dev); struct neigh_hash_table *nht; if (!n) &#123; rc = ERR_PTR(-ENOBUFS); goto out; &#125; ......out: return rc;&#125; 分配neighbour，同时如果ARP缓存表超过gc_thresh3，则强制做一次gc。如果gc没有回收任何过期ARP记录且表项数超过gc_thresh3，则返回NULL，标识分配邻居表项失败 1234567891011121314151617181920212223242526272829// 分配失败，返回NULLstatic struct neighbour *neigh_alloc(struct neigh_table *tbl, struct net_device *dev)&#123; struct neighbour *n = NULL; unsigned long now = jiffies; int entries; entries = atomic_inc_return(&amp;tbl-&gt;entries) - 1; // 如果超过gc_thres3或者 // 超过gc_thresh2且5秒没有刷新(HZ一般为1000) // 则强制gc if (entries &gt;= tbl-&gt;gc_thresh3 || (entries &gt;= tbl-&gt;gc_thresh2 &amp;&amp; time_after(now, tbl-&gt;last_flush + 5 * HZ))) &#123; if (!neigh_forced_gc(tbl) &amp;&amp; entries &gt;= tbl-&gt;gc_thresh3) goto out_entries; // arp表项超过阈值 且回收失败， &#125; n = kzalloc(tbl-&gt;entry_size + dev-&gt;neigh_priv_len, GFP_ATOMIC); ...... out: return n;out_entries: atomic_dec(&amp;tbl-&gt;entries); goto out;&#125; 对ARP表进行垃圾回收，回收期间需要获取写锁。如果成功回收则返回1，否则返回0 12345678910111213141516171819202122232425262728293031323334353637383940static int neigh_forced_gc(struct neigh_table *tbl)&#123; // 获取 arp 缓存hash table 锁 write_lock_bh(&amp;tbl-&gt;lock); nht = rcu_dereference_protected(tbl-&gt;nht, lockdep_is_held(&amp;tbl-&gt;lock)); for (i = 0; i &lt; (1 &lt;&lt; nht-&gt;hash_shift); i++) &#123; struct neighbour *n; struct neighbour __rcu **np; np = &amp;nht-&gt;hash_buckets[i]; while ((n = rcu_dereference_protected(*np, lockdep_is_held(&amp;tbl-&gt;lock))) != NULL) &#123; /* Neighbour record may be discarded if: * - nobody refers to it. * - it is not permanent */ write_lock(&amp;n-&gt;lock); if (atomic_read(&amp;n-&gt;refcnt) == 1 &amp;&amp; !(n-&gt;nud_state &amp; NUD_PERMANENT)) &#123; rcu_assign_pointer(*np, rcu_dereference_protected(n-&gt;next, lockdep_is_held(&amp;tbl-&gt;lock))); n-&gt;dead = 1; shrunk = 1; write_unlock(&amp;n-&gt;lock); neigh_cleanup_and_release(n); continue; &#125; write_unlock(&amp;n-&gt;lock); np = &amp;n-&gt;next; &#125; &#125; tbl-&gt;last_flush = jiffies; write_unlock_bh(&amp;tbl-&gt;lock); return shrunk;&#125; 123456789101112131415161718192021222324static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n, struct sk_buff *skb)&#123; const struct hh_cache *hh; if (dst-&gt;pending_confirm) &#123; unsigned long now = jiffies; dst-&gt;pending_confirm = 0; /* avoid dirtying neighbour */ if (n-&gt;confirmed != now) n-&gt;confirmed = now; &#125; hh = &amp;n-&gt;hh; // 如果下面三种情况之一，不需要ARP请求 // NUD_PERMANENT : 静态路由 // NUD_NOARP：多播地址，广播地址或者本地环回设备lookback // NUD_REACHABLE：邻居表项状态是reachable if ((n-&gt;nud_state &amp; NUD_CONNECTED) &amp;&amp; hh-&gt;hh_len) return neigh_hh_output(hh, skb); else return n-&gt;output(n, skb); // 非连接状态, neigh_resolve_output&#125; ARP 下面的源码分析其实已经不重要了， neigh_resolve_output会间接调用函数neigh_event_send()，实现邻居项状态从 NUD_NONE 到 NUD_INCOMPLETTE 状态的改变 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/* Slow and careful. */int neigh_resolve_output(struct neighbour *neigh, struct sk_buff *skb)&#123; ...... // 发送arp请求 if (!neigh_event_send(neigh, skb)) &#123; // 如果neigh已经连接，则发送skb int err; struct net_device *dev = neigh-&gt;dev; unsigned int seq; if (dev-&gt;header_ops-&gt;cache &amp;&amp; !neigh-&gt;hh.hh_len) neigh_hh_init(neigh, dst); // 初始化mac缓存值 do &#123; __skb_pull(skb, skb_network_offset(skb)); seq = read_seqbegin(&amp;neigh-&gt;ha_lock); err = dev_hard_header(skb, dev, ntohs(skb-&gt;protocol), neigh-&gt;ha, NULL, skb-&gt;len); &#125; while (read_seqretry(&amp;neigh-&gt;ha_lock, seq)); if (err &gt;= 0) rc = dev_queue_xmit(skb); // 真正发送数据 else goto out_kfree_skb; &#125; ......&#125;EXPORT_SYMBOL(neigh_resolve_output);static inline int neigh_event_send(struct neighbour *neigh, struct sk_buff *skb)&#123; ...... if (!(neigh-&gt;nud_state&amp;(NUD_CONNECTED|NUD_DELAY|NUD_PROBE))) return __neigh_event_send(neigh, skb); return 0;&#125;int __neigh_event_send(struct neighbour *neigh, struct sk_buff *skb)&#123; write_lock_bh(&amp;neigh-&gt;lock); ...... // 初始化阶段 if (!(neigh-&gt;nud_state &amp; (NUD_STALE | NUD_INCOMPLETE))) &#123; if (neigh-&gt;parms-&gt;mcast_probes + neigh-&gt;parms-&gt;app_probes) &#123; unsigned long next, now = jiffies; atomic_set(&amp;neigh-&gt;probes, neigh-&gt;parms-&gt;ucast_probes); neigh-&gt;nud_state = NUD_INCOMPLETE; // 设置表项尾incomplete neigh-&gt;updated = now; next = now + max(neigh-&gt;parms-&gt;retrans_time, HZ/2); neigh_add_timer(neigh, next); // 定时器：刷新状态后在neigh_update中发送skb immediate_probe = true; &#125; else &#123; ...... &#125; ...... &#125; if (neigh-&gt;nud_state == NUD_INCOMPLETE) &#123; if (skb) &#123; while (neigh-&gt;arp_queue_len_bytes + skb-&gt;truesize &gt; neigh-&gt;parms-&gt;queue_len_bytes) &#123; struct sk_buff *buff; // 丢弃 buff = __skb_dequeue(&amp;neigh-&gt;arp_queue); if (!buff) break; neigh-&gt;arp_queue_len_bytes -= buff-&gt;truesize; kfree_skb(buff); NEIGH_CACHE_STAT_INC(neigh-&gt;tbl, unres_discards); &#125; skb_dst_force(skb); __skb_queue_tail(&amp;neigh-&gt;arp_queue, skb); //报文放入arp_queue队列中，arp_queue 队列用于缓存等待发送的数据包 neigh-&gt;arp_queue_len_bytes += skb-&gt;truesize; &#125; rc = 1; &#125;out_unlock_bh: if (immediate_probe) neigh_probe(neigh); // 进行探测，发送arp else write_unlock(&amp;neigh-&gt;lock); local_bh_enable(); return rc;&#125;EXPORT_SYMBOL(__neigh_event_send);static void neigh_probe(struct neighbour *neigh) __releases(neigh-&gt;lock)&#123; struct sk_buff *skb = skb_peek(&amp;neigh-&gt;arp_queue); // 取出报文 /* keep skb alive even if arp_queue overflows */ if (skb) skb = skb_copy(skb, GFP_ATOMIC); write_unlock(&amp;neigh-&gt;lock); neigh-&gt;ops-&gt;solicit(neigh, skb); // 发送arp请求，arp_solicit atomic_inc(&amp;neigh-&gt;probes); kfree_skb(skb);&#125; ARP响应的处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071static struct packet_type arp_packet_type __read_mostly = &#123; .type = cpu_to_be16(ETH_P_ARP), .func = arp_rcv,&#125;;void __init arp_init(void)&#123; neigh_table_init(&amp;arp_tbl); dev_add_pack(&amp;arp_packet_type); // add packet handler ......&#125;static int arp_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)&#123; ...... return NF_HOOK(NFPROTO_ARP, NF_ARP_IN, skb, dev, NULL, arp_process);&#125;static int arp_process(struct sk_buff *skb) &#123; ...... n = __neigh_lookup(&amp;arp_tbl, &amp;sip, dev, 0); if (n) &#123; // 更新邻居项 neigh_update(n, sha, state, override ? NEIGH_UPDATE_F_OVERRIDE : 0); &#125; ......&#125;int neigh_update(struct neighbour *neigh, const u8 *lladdr, u8 new, u32 flags)&#123; ...... write_lock_bh(&amp;neigh-&gt;lock); ...... if (new &amp; NUD_CONNECTED) neigh_connect(neigh); else neigh_suspect(neigh); if (!(old &amp; NUD_VALID)) &#123; // 源状态不为valid，则发送缓存的skb struct sk_buff *skb; while (neigh-&gt;nud_state &amp; NUD_VALID &amp;&amp; (skb = __skb_dequeue(&amp;neigh-&gt;arp_queue)) != NULL) &#123; struct dst_entry *dst = skb_dst(skb); struct neighbour *n2, *n1 = neigh; write_unlock_bh(&amp;neigh-&gt;lock); rcu_read_lock(); n2 = NULL; if (dst) &#123; n2 = dst_neigh_lookup_skb(dst, skb); if (n2) n1 = n2; &#125; n1-&gt;output(n1, skb); // 调用neigh的output函数，此时已经改成connect函数 if (n2) neigh_release(n2); rcu_read_unlock(); write_lock_bh(&amp;neigh-&gt;lock); &#125; skb_queue_purge(&amp;neigh-&gt;arp_queue); neigh-&gt;arp_queue_len_bytes = 0; &#125; ...... write_unlock_bh(&amp;neigh-&gt;lock); ......&#125; 自旋锁 对邻居表进行修改需要获取写锁，而这个锁的实现为自旋锁。 12345678910111213141516171819202122232425262728293031323334static inline void __raw_write_lock_bh(rwlock_t *lock)&#123; local_bh_disable(); preempt_disable(); rwlock_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);&#125;# define do_raw_write_trylock(rwlock) arch_write_trylock(&amp;(rwlock)-&gt;raw_lock)static inline void arch_write_lock(arch_rwlock_t *rw)&#123; u32 val = __insn_fetchor4(&amp;rw-&gt;lock, __WRITE_LOCK_BIT); if (unlikely(val != 0)) __write_lock_failed(rw, val);&#125;/* * If we failed because there were readers, clear the &quot;writer&quot; bit * so we don&apos;t block additional readers. Otherwise, there was another * writer anyway, so our &quot;fetchor&quot; made no difference. Then wait, * issuing periodic fetchor instructions, till we get the lock. */void __write_lock_failed(arch_rwlock_t *rw, u32 val)&#123; int iterations = 0; do &#123; if (!arch_write_val_locked(val)) val = __insn_fetchand4(&amp;rw-&gt;lock, ~__WRITE_LOCK_BIT); delay_backoff(iterations++); val = __insn_fetchor4(&amp;rw-&gt;lock, __WRITE_LOCK_BIT); &#125; while (val != 0);&#125;EXPORT_SYMBOL(__write_lock_failed); 根本原因原因已经明确了，由于POD之间通信都会产生2条ARP记录缓存在ARP表中，如果宿主机上POD较多，则ARP表项会很多，而linux对ARP大小是有限制的。ARP表项相关的内核参数 内核参数 含义 默认值 /proc/sys/net/ipv4/neigh/default/base_reachable_time reachable 状态基础过期时间 30秒 /proc/sys/net/ipv4/neigh/default/gc_stale_time stale 状态过期时间 60秒 /proc/sys/net/ipv4/neigh/default/delay_first_probe_time delay 状态过期到 probe 的时间 5秒 /proc/sys/net/ipv4/neigh/default/gc_interval gc 启动的周期时间 30秒 /proc/sys/net/ipv4/neigh/default/gc_thresh1 最小可保留的表项数量 128 /proc/sys/net/ipv4/neigh/default/gc_thresh2 最多纪录的软限制 512 /proc/sys/net/ipv4/neigh/default/gc_thresh3 最多纪录的硬限制 1024 ARP表垃圾回收机制 表的垃圾回收分为同步和异步： 同步：neigh_alloc函数被调用时执行 异步：gc_interval指定的周期性gc 回收策略 arp 表项数量 &lt; gc_thresh1：不做处理 gc_thresh1 =&lt; arp 表项数量 &lt;= gc_thresh2：按照 gc_interval 定期启动ARP表垃圾回收 gc_thresh2 &lt; arp 表项数量 &lt;= gc_thresh3，5 * HZ（一般5秒）后ARP表垃圾回收 arp 表项数量 &gt; gc_thresh3：立即启动ARP表垃圾回收 ARP表项状态转换 图来源于《Understanding Linux Network Internals》 如图所示，只有两种状态的ARP记录才能被回收： stale状态过期后 failed状态 结论 由于我们kubernetes是基于macvlan网络的，所以会为每个POD分配一个mac地址，这样同一台宿主机的POD之间访问，就会产生2条ARP表项。 所以，当ARP表项超过gc_thresh3，则会触发回收ARP表，当没有可回收的表项时，则会导致neigh_create分配失败，从而导致系统调用失败。那么上层应用层很可能会进行不断重试，也会有其他流量调用内核发送数据，最终只要数据接收方的ARP记录不在ARP表中，都会导致进行垃圾回收，而垃圾回收需要加写锁后再对ARP表进行遍历，这个遍历虽然耗时不长，但是持写锁时间 * 回收次数 导致持写锁的总体时间很长，这样就导致大量的锁竞争（自旋），导致CPU暴涨，从而引发故障。 验证 先将gc_thresh改小 1234#!/bin/bashecho 6 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh1echo 7 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh2echo 8 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh3 ping下其他POD，让ARP表大小达到阈值 123456789[root@d /]# ip neigh10.170.254.32 dev eth0 lladdr 02:00:0a:aa:fe:20 REACHABLE10.170.254.1 dev eth0 lladdr 2c:21:31:79:96:c0 REACHABLE10.170.254.27 dev eth0 lladdr 02:00:0a:aa:fe:1b REACHABLE10.170.254.24 dev eth0 lladdr 02:00:0a:aa:fe:18 REACHABLE10.170.254.25 dev eth0 lladdr 02:00:0a:aa:fe:19 REACHABLE10.170.254.30 dev eth0 lladdr 02:00:0a:aa:fe:1e REACHABLE10.170.254.28 dev eth0 lladdr 02:00:0a:aa:fe:1c REACHABLE10.170.254.29 dev eth0 lladdr 02:00:0a:aa:fe:1d REACHABLE ping下不在ARP表中的POD，发现ping命令“卡住了” 12[root@d tmp]# ping 10.170.254.10PING 10.170.254.10 (10.170.254.10) 56(84) bytes of data. 再将gc_thresh改大 1echo 80 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh3 ping命令立即恢复了 12345678[root@dns-8685bbc547-4dmld tmp]# ping 10.170.254.10PING 10.170.254.10 (10.170.254.10) 56(84) bytes of data.64 bytes from 10.170.254.10: icmp_seq=125 ttl=64 time=2004 ms64 bytes from 10.170.254.10: icmp_seq=126 ttl=64 time=1004 ms64 bytes from 10.170.254.10: icmp_seq=127 ttl=64 time=4.04 ms64 bytes from 10.170.254.10: icmp_seq=128 ttl=64 time=0.022 ms64 bytes from 10.170.254.10: icmp_seq=129 ttl=64 time=0.021 ms64 bytes from 10.170.254.10: icmp_seq=130 ttl=64 time=0.020 ms 总结当将线上kubernetes宿主机的ARP参数都改大后，再也没有出现过此类问题了。 123sysctl -w net.ipv4.neigh.default.gc_thresh3=32768sysctl -w net.ipv4.neigh.default.gc_thresh2=16384sysctl -w net.ipv4.neigh.default.gc_thresh1=8192 虽然问题发生在kubernetes上，但是本质还是由于对底层OS不了解导致的。]]></content>
      <categories>
        <category>2021</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从使用者，DBA，内核开发三个不同角度来分析SQL的性能问题]]></title>
    <url>%2F2021%2F09%2F10%2Fpostgresql_sql_case%2F</url>
    <content type="text"><![CDATA[+++author = “彭亮”title = “从使用者，DBA，内核开发三个不同角度来分析SQL的性能问题”date = “2021-09-10”tags = [ “SQL”, “postgreSQL”, “database”,]categories = [ “postgreSQL”, “database”,]draft = false+++ 有时候写的SQL有性能问题时往往束手无策，而求助于DBA。今天，我们从使用者、DBA、内核开发三个不同的角度来分析一个有趣的SQL性能问题的案例，从浅入深了解postgreSQL的优化器。 问题描述同事A来问我这个假DBA一条SQL的性能问题： A：两条SQL语句只有limit不一样，而limit 1的执行比limit 10的慢N倍 我：是不是缓存问题，先执行limit 10再执行limit 1试试 A：……，执行了，limit还是很慢 两条SQL生产环境执行情况 limit 10 1select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 10; Execution Time: 1.307 ms limit 1 1select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 1; Execution Time: 144.098 ms 分析执行计划既然不是缓存问题，那我们先看看执行计划有什么不一样的 limit 1 1234567891011121314# explain analyze verbose select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 1; QUERY PLAN--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.43..416.25 rows=1 width=73) (actual time=135.213..135.214 rows=1 loops=1) Output: xxx -&gt; Index Scan Backward using user_gift_pkey on yay.user_gift (cost=0.43..368000.44 rows=885 width=73) (actual time=135.212..135.212 rows=1 loops=1) Output: xxx Filter: ((user_gift.user_id = 11695667) AND (user_gift.user_type = &apos;default&apos;::user_type)) Rows Removed by Filter: 330192 Planning Time: 0.102 ms Execution Time: 135.235 ms(8 rows)Time: 135.691 ms limit 10 123456789101112131415161718# explain analyze verbose select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 10; QUERY PLAN---------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=868.20..868.22 rows=10 width=73) (actual time=1.543..1.545 rows=10 loops=1) Output: xxx -&gt; Sort (cost=868.20..870.41 rows=885 width=73) (actual time=1.543..1.543 rows=10 loops=1) Output: xxx Sort Key: user_gift.id DESC Sort Method: top-N heapsort Memory: 27kB -&gt; Index Scan using idx_user_type on yay.user_gift (cost=0.56..849.07 rows=885 width=73) (actual time=0.020..1.366 rows=775 loops=1) Output: xxx Index Cond: (user_gift.user_id = 11695667) Filter: (user_gift.user_type = &apos;default&apos;::user_type) Planning Time: 0.079 ms Execution Time: 1.564 ms(12 rows)Time: 1.871 ms 可以看到，两个SQL执行计划不一样： limit 1语句 ：使用主键进行倒序扫描， Index Scan Backward using user_gift_pkey on yay.user_gift limit 10语句 ：使用(user_id, user_type)复合索引直接查找用户数据，Index Scan using idx_user_type on yay.user_gift 为什么执行计划不一样？ total cost 其实postgreSQL的执行计划并没有“问题”，因为limit 1的total costLimit (cost=0.43..416.25 rows=1 width=73) 是416，run cost是416-0.43=415.57。而limit 10的total costLimit (cost=868.20..868.22 rows=10 width=73)是868.22。 如果使用Index Scan Backward using user_gift_pkey的方式估算，那么limit 1成本是415, limit 2是415*2=830, limit 3 是 1245，大于868，所以当limit 3的时候会使用Index Scan using idx_user_type扫索引的计划。 验证 123456789101112131415161718192021# explain select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 2; QUERY PLAN------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.43..831.95 rows=2 width=73) -&gt; Index Scan Backward using user_gift_pkey on user_gift (cost=0.43..367528.67 rows=884 width=73) Filter: ((user_id = 11695667) AND (user_type = &apos;default&apos;::user_type))(3 rows) Time: 0.341 ms# explain select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 3; QUERY PLAN---------------------------------------------------------------------------------------------------------- Limit (cost=866.19..866.20 rows=3 width=73) -&gt; Sort (cost=866.19..868.40 rows=884 width=73) Sort Key: id DESC -&gt; Index Scan using idx_user_type on user_gift (cost=0.56..854.76 rows=884 width=73) Index Cond: (user_id = 11695667) Filter: (user_type = &apos;default&apos;::user_type)(6 rows) Time: 0.352 ms 结果显示： 当limit 2时，执行计划是Index Scan Backward using user_gift_pkey 当limit 3时，就改变计划了，Index Scan using idx_user_type on user_gift 实际执行时间limit 1时成本估算的是416.25，比limit 10的868.22还是要快的。但是实际limit 1执行cost是135.691 ms，而limit 10执行cost是1.871 ms，比limit 10慢了70倍！！！ 我们重新执行下explain，加上buffers选项 12345678910111213141516# explain (analyze, buffers, verbose) select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 1; QUERY PLAN--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.43..416.29 rows=1 width=73) (actual time=451.542..451.544 rows=1 loops=1) Output: xxx Buffers: shared hit=214402 read=5280 dirtied=2302 I/O Timings: read=205.027 -&gt; Index Scan Backward using user_gift_pkey on yay.user_gift (cost=0.43..368032.94 rows=885 width=73) (actual time=451.540..451.540 rows=1 loops=1) Output: xxx Filter: ((user_gift.user_id = 11695667) AND (user_gift.user_type = &apos;default&apos;::user_type)) Rows Removed by Filter: 333462 Buffers: shared hit=214402 read=5280 dirtied=2302 I/O Timings: read=205.027 Planning Time: 1.106 ms Execution Time: 451.594 ms(12 rows) 12345678910111213141516171819202122# explain (analyze, buffers, verbose) select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 3; QUERY PLAN----------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=860.51..860.52 rows=3 width=73) (actual time=14.633..14.634 rows=3 loops=1) Output: xxx Buffers: shared hit=467 read=321 I/O Timings: read=10.112 -&gt; Sort (cost=860.51..862.72 rows=885 width=73) (actual time=14.632..14.632 rows=3 loops=1) Output: xxx Sort Key: user_gift.id DESC Sort Method: top-N heapsort Memory: 25kB Buffers: shared hit=467 read=321 I/O Timings: read=10.112 -&gt; Index Scan using idx_user_type on yay.user_gift (cost=0.56..849.07 rows=885 width=73) (actual time=0.192..14.424 rows=775 loops=1) Output: xxx Index Cond: (user_gift.user_id = 11695667) Filter: (user_gift.user_type = &apos;default&apos;::user_type) Buffers: shared hit=464 read=321 I/O Timings: read=10.112 Planning Time: 0.111 ms Execution Time: 14.658 ms(18 rows) 可以看出： limit 1时的IO成本I/O Timings: read=205.027，Rows Removed by Filter: 333462显示过滤了333462行记录 limit 3时IO成本I/O Timings: read=10.112， 从上面输出Buffers: shared hit=214402 read=5280 dirtied=2302可以看出limit 1的计划从磁盘读取了5280个blocks(pages)才找到符合where条件的记录。 为什么要读取这么多数据呢？我们来看看统计信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344schemaname | yaytablename | user_giftattname | idinherited | fnull_frac | 0avg_width | 8n_distinct | -1most_common_vals | most_common_freqs | histogram_bounds | &#123;93,9817,19893,28177,.......&#125;correlation | 0.788011most_common_elems | most_common_elem_freqs | elem_count_histogram | schemaname | yaytablename | user_giftattname | user_idinherited | fnull_frac | 0avg_width | 4n_distinct | -0.175761most_common_vals | &#123;11576819,10299480,14020501,.......,11695667,......&#125;most_common_freqs | &#123;0.000353333,0.000326667,0.000246667,......,9.33333e-05,......&#125;histogram_bounds | &#123;3,10002181,10005599,10009672,......,11693300,11698290,......&#125;correlation | 0.53375most_common_elems | most_common_elem_freqs | elem_count_histogram | schemaname | yaytablename | user_giftattname | user_typeinherited | fnull_frac | 0avg_width | 4n_distinct | 3most_common_vals | &#123;default, invalid, deleted&#125;most_common_freqs | &#123;0.997923,0.00194,0.000136667&#125;histogram_bounds | correlation | 0.99763most_common_elems | most_common_elem_freqs | elem_count_histogram | 从统计信息里可以看出： user_id字段的most_common_vals中有11695667(user_id)的值，则可以直接通过其对应的most_common_freqs来得到其selectivity是9.33333e-05； user_type字段为default对应的selectivity是0.997923。 所以where user_id=11695667 and user_type=&#39;default&#39;的selectivity是0.0000933333*0.997923 = 0.0000931394467359。 那么可以估算出满足where条件的用户数是0.0000931394467359 * 9499740(总用户数) = 884.8，和执行计划(cost=0.43..367528.67 rows=884 width=73)的884行一样。 而优化器的估算是基于数据分布均匀这个假设的： 从user_gift_pkey(主键id)扫描的话：只要扫描9499740/884=10746行就能找到满足条件的记录，且无须进行排序(order by id desc) 从idx_user_type索引扫描的话：虽然能很快找到此用户的数据，但是需要给884行进行排序，扫描+排序的cost比从主键扫描要高。 那么数据分布真的均匀吗？继续查看数据的实际分布： 表最大的page=128709 12345# select max(ctid) from user_gift; max------------- (128709,29)(1 row) user id=11695667的最大page=124329 12345# select max(ctid), min(ctid) from user_gift where user_id=11695667; max | min-------------+----------- (124329,22) | (3951,64)(1 row) 表本身的pages和tuples数量12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;user_gift&apos;; relpages | reltuples----------+------------- 128875 | 9.49974e+06(1 row) 每个page存储的记录数：9.49974e+06 tuples / 128875 pages = 73.713 tuples/page。 计算：表(main table)的B+tree的最大page是128709，而实际用户11695667的最大page是124329，128709 - 124329 = 4380，需要扫描4380个page才能找到符合where条件的记录所在的page，所以过滤的rows是4380 pages * 73.713 tuples/page ≈ 322862。 实际limit 1时扫描了5280个pages(包含了主键索引的pages)，过滤了333462万行记录，和估算的基本一样： 123Rows Removed by Filter: 333462 Buffers: shared hit=214402 read=5280 dirtied=2302 I/O Timings: read=205.027 所以，此用户数据分布倾斜了： 优化器假设数据分布均匀，只需要扫描10746个记录 而实际需要扫描322862个记录 那么扫描5280个pages要多久？ 需要读取的数据量：5280pages * 8KB/page = 41.2MB的数据。 123456789[root]$ fio -name iops -rw=randread -bs=8k -runtime=10 -iodepth=1 -filename /dev/sdb -ioengine mmap -buffered=1...Run status group 0 (all jobs): READ: bw=965KiB/s (988kB/s), 965KiB/s-965KiB/s (988kB/s-988kB/s), io=9656KiB (9888kB), run=10005-10005msec[root]$ fio -name iops -rw=read -bs=8k -runtime=10 -iodepth=1 -filename /dev/sdb -ioengine mmap -direct=1...Run status group 0 (all jobs): READ: bw=513MiB/s (538MB/s), 513MiB/s-513MiB/s (538MB/s-538MB/s), io=5132MiB (5381MB), run=10001-10001msec 从fio结果可以看出，此数据库机器磁盘的顺序读取速度约为 500MB/s，如果数据都是顺序的，那么扫描40MB数据需要约80ms， 如果数据都是随机的，那么需要40秒。不是所有的数据都是顺序访问的，而且测试的是非线上机器，没有其他IO进程在运行。 到这里问题基本定位清楚了： postgreSQL的优化器认为数据分布是均匀的，只需要倒序扫描很快就找到符合条件的记录，而实际上此用户的数据分布在表的前端，就导致了实际执行start-up time如此慢了。 从内核视角来分析我们从postgreSQL内核的角度来继续分析几个问题： 优化器如何估算cost 优化器如何统计actual time 表的信息 表结构 123456789101112131415# \d user_gift; Table &quot;yay.user_gift&quot; Column | Type | Collation | Nullable | Default--------------+--------------------------+-----------+----------+------------------------------------------------ id | bigint | | not null | nextval(&apos;user_gift_id_seq&apos;::regclass) user_id | integer | | not null | ug_name | character varying(100) | | not null | expired_time | timestamp with time zone | | | now() created_time | timestamp with time zone | | not null | now() updated_time | timestamp with time zone | | not null | now() user_type | user_type | | not null | &apos;default&apos;::user_typeIndexes: &quot;user_gift_pkey&quot; PRIMARY KEY, btree (id) &quot;idx_user_type&quot; btree (user_id, ug_name) &quot;user_gift_ug_name_idx&quot; btree (ug_name) 主键索引 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;user_gift_pkey&apos;; relpages | reltuples----------+------------- 40035 | 9.49974e+06(1 row) user_id 索引 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;idx_user_type&apos;; relpages | reltuples----------+------------- 113572 | 9.49974e+06(1 row) 表本身的pages是128875 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;user_gift&apos;; relpages | reltuples----------+------------- 128875 | 9.49974e+06(1 row) user id=11695667的数据775行 1234567891011=# select count(1) from user_gift where user_id=11695667; count------- 775(1 row)=# select count(1) from user_gift where user_id=11695667 and user_type = &apos;default&apos; ; count------- 775(1 row) 树高度 1234567891011121314# 主键高度# select * from bt_metap(&apos;user_gift_pkey&apos;); magic | version | root | level | fastroot | fastlevel | oldest_xact | last_cleanup_num_tuples--------+---------+------+-------+----------+-----------+-------------+------------------------- 340322 | 3 | 412 | 2 | 412 | 2 | 0 | 9.31928e+06(1 row)// idx_user_type 高度# select * from bt_metap(&apos;idx_user_type&apos;); magic | version | root | level | fastroot | fastlevel | oldest_xact | last_cleanup_num_tuples--------+---------+-------+-------+----------+-----------+-------------+------------------------- 340322 | 3 | 15094 | 3 | 15094 | 3 | 0 | 9.49974e+06(1 row) 估算coststart-up costpostgreSQL对于每种索引的成本估算是不一样的，我们看看B+tree的start-up成本是如何估算的： 12345678910111213141516171819// selfuncs.cvoidbtcostestimate(PlannerInfo *root, IndexPath *path, double loop_count, Cost *indexStartupCost, Cost *indexTotalCost, Selectivity *indexSelectivity, double *indexCorrelation, double *indexPages)&#123; ...... descentCost = ceil(log(index-&gt;tuples) / log(2.0)) * cpu_operator_cost; costs.indexStartupCost += descentCost; ...... // This cost is somewhat arbitrarily set at 50x cpu_operator_cost per page touched descentCost = (index-&gt;tree_height + 1) * 50.0 * cpu_operator_cost; costs.indexStartupCost += descentCost; ......&#125; 其实start-up cost估算很简单，只考虑从B+tree的root page遍历到leaf page，且将这个page读入第一个tuple(记录)的cost。 start-up估算公式如下：$$\left { ceil({\log_2 (N_{index,tuple})}) + (Height_{index} + 1) \times 50 \right }\ \times cpu_operator_cost$$ N(index,tuple) ：索引tuples(记录)数量 Height(index) ： 索引B+tree的高度 cpu_operator_cost : 默认值0.0025 使用user_gift_pkey计划的start-up cost 从上面表信息中可以看出： N(index,tuple) ：9.49974e+06， Height(index) ： 2 所以$$\left { ceil({\log_2 (9499740)}) + (2 + 1) \times 50 \right }\ \times cpu_operator_cost = 173 \times 0.0025 = 0.435$$和postgreSQL估算的start-up cost=0.43 一样。 使用idx_user_type计划的start-up cost N(index,tuple) ：9.49974e+06， Height(index) ： 3$$\left { ceil({\log_2 (9499740)}) + (3 + 1) \times 50 \right }\ \times cpu_operator_cost = 223 \times 0.0025 = 0.5575$$和postgreSQL估算的start-up cost=0.56 一样。 run costrun cost的估算是比较复杂的，判断的条件非常多，无法用一个固定的公式计算出来，所以这里只是简单描述下，有兴趣的可以看postgreSQL源码src/backend/optimizer/path/costsize.c的cost_index函数，针对这个案例，一般情况下可以根据此链接的脚本进行来模拟计算cost。 run cost$$run_cost = 索引成本 + 主表成本$$ 索引成本$$索引成本 = 随机读取索引相关pages的成本 + 操作相关tuples的成本$$ 主表成本$$主表成本 = max_io_cost + index_correlation ^ 2 \times (min_io_cost - max_io_cost)$$ ndex_correlation : 索引相关性。索引的顺序与主表数据排列顺序的关联性，用来描述通过索引扫描数据时，回表的顺序读的概率 max io cost（最坏情况下IO成本） 所有pages都是随机读取$$max_io_cost = pages_fetched \times random_page_cost$$ min_io_cost（最优情况下IO成本） 第一个page是随机读取，后面pages都是顺序读取$$min_io_cost = 1 \times random_page_cost + (pages_fetched - 1) \times seq_page_cost$$ actual start-up time vs estimated start-up cost刚刚的分析中有一个疑问被忽略了：estimated start-up cost是开始执行计划到从表中读到的第一个tuple的cost(cost is an arbitrary unit)；而actual start-up time则是开始执行计划到从表中读取到第一个符合where条件的tuple的时间。这是为什么呢？ SQL处理流程：postgreSQL将SQL转化成AST，然后进行优化，再将AST转成执行器(executor)来实现具体的操作。不进行优化的执行器是这样的： 123456789101112131415161718┌──────────────┐│ projection │└──────┬───────┘ │ │┌──────▼──────┐│ limit │└──────┬──────┘ │ │┌──────▼──────┐│ selection │└──────┬──────┘ │ │┌──────▼──────┐│ index scan │└─────────────┘ 简化的执行流程如下： index scan executor：扫描到一个tuple，就返回给selection executor selection executor：对tuple进行过滤，如果符合条件则返回给limit executor，如果不符合则继续调用index scan executor limit executor：当达到limit限制则将数据返回给projection executor projection executor：过滤掉非select列的数据 那么如果进行优化，一般会将selection executor和projection executor合并到index scan executor中执行，以减少数据在executor之间的传递。 1234567891011┌─────────────┐│ limit │└──────┬──────┘ │ │┌──────▼──────┐│ index scan ││ ││ + selection ││ + projection│└─────────────┘ 优化后的执行流程： index scan executor：扫描到tuple，然后进行selection过滤，如果符合条件就进行projection再返回给limit，如果不符合条件，则继续扫描 limit executor：当达到limit限制则将数据返回 而通过下面代码可以看出，postgreSQL对于执行时间的统计是基于executor的， 123456789101112// src/backend/executor/execProcnode.cstatic TupleTableSlot *ExecProcNodeInstr(PlanState *node)&#123; TupleTableSlot *result; InstrStartNode(node-&gt;instrument); result = node-&gt;ExecProcNodeReal(node); // 统计执行指标 InstrStopNode(node-&gt;instrument, TupIsNull(result) ? 0.0 : 1.0); return result;&#125; 所以actual time的start-up是从启动executor直到扫描到符合where语句的第一条结果为止。 再看看实际的函数调用栈，user_id=xxx的过滤已经下沉到index scan executor里面了。 1234567891011121314151617181920---&gt; int4eq(FunctionCallInfo fcinfo) (/home/ken/cpp/postgres/src/backend/utils/adt/int.c:379) ExecInterpExpr(ExprState * state, ExprContext * econtext, _Bool * isnull) (/home/ken/cpp/postgres/src/backend/executor/execExprInterp.c:704) ExecInterpExprStillValid(ExprState * state, ExprContext * econtext, _Bool * isNull) (/home/ken/cpp/postgres/src/backend/executor/execExprInterp.c:1807) ExecEvalExprSwitchContext(ExprState * state, ExprContext * econtext, _Bool * isNull) (/home/ken/cpp/postgres/src/include/executor/executor.h:322)---&gt; ExecQual(ExprState * state, ExprContext * econtext) (/home/ken/cpp/postgres/src/include/executor/executor.h:391) ExecScan(ScanState * node, ExecScanAccessMtd accessMtd, ExecScanRecheckMtd recheckMtd) (/home/ken/cpp/postgres/src/backend/executor/execScan.c:227)---&gt; ExecIndexScan(PlanState * pstate) (/home/ken/cpp/postgres/src/backend/executor/nodeIndexscan.c:537) ExecProcNodeInstr(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:466) ExecProcNodeFirst(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:450) ExecProcNode(PlanState * node) (/home/ken/cpp/postgres/src/include/executor/executor.h:248)---&gt; ExecLimit(PlanState * pstate) (/home/ken/cpp/postgres/src/backend/executor/nodeLimit.c:96) ExecProcNodeInstr(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:466) ExecProcNodeFirst(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:450) ExecProcNode(PlanState * node) (/home/ken/cpp/postgres/src/include/executor/executor.h:248) ExecutePlan(EState * estate, PlanState * planstate, _Bool use_parallel_mode, CmdType operation, _Bool sendTuples, uint64 numberTuples, ScanDirection direction, DestReceiver * dest, _Bool execute_once) (/home/ken/cpp/postgres/src/backend/executor/execMain.c:1632) standard_ExecutorRun(QueryDesc * queryDesc, ScanDirection direction, uint64 count, _Bool execute_once) (/home/ken/cpp/postgres/src/backend/executor/execMain.c:350) ExecutorRun(QueryDesc * queryDesc, ScanDirection direction, uint64 count, _Bool execute_once) (/home/ken/cpp/postgres/src/backend/executor/execMain.c:294) ExplainOnePlan(PlannedStmt * plannedstmt, IntoClause * into, ExplainState * es, const char * queryString, ParamListInfo params, QueryEnvironment * queryEnv, const instr_time * planduration, const BufferUsage * bufusage) (/home/ken/cpp/postgres/src/backend/commands/explain.c:571) ExplainOneQuery(Query * query, int cursorOptions, IntoClause * into, ExplainState * es, const char * queryString, ParamListInfo params, QueryEnvironment * queryEnv) (/home/ken/cpp/postgres/src/backend/commands/explain.c:404) ExplainQuery(ParseState * pstate, ExplainStmt * stmt, ParamListInfo params, DestReceiver * dest) (/home/ken/cpp/postgres/src/backend/commands/explain.c:275) 下面代码是scan的实现，其中的ExecQual(qual, econtext)是对tuple进行过滤，因为selection已经合并到scan中了。 123456789101112131415161718192021222324252627TupleTableSlot *ExecScan(ScanState *node, ExecScanAccessMtd accessMtd, ExecScanRecheckMtd recheckMtd)&#123; ...... for (;;) &#123; TupleTableSlot *slot; slot = ExecScanFetch(node, accessMtd, recheckMtd); ...... econtext-&gt;ecxt_scantuple = slot; // Note : selection判断 if (qual == NULL || ExecQual(qual, econtext)) &#123; if (projInfo) &#123; return ExecProject(projInfo); &#125; else &#123; return slot; &#125; &#125; else InstrCountFiltered1(node, 1); &#125;&#125; 解决方案禁用走主键扫描既然计划走的是user_gift_pkey倒序扫描，那么我们可以手动避免优化器使用这个索引。 1# explain analyze verbose select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id+0 desc limit 1; 将order by id改成order by id+0，由于id+0是个表达式所以优化器就就不会使用user_gift_pkey这个索引了。 这个方案不适合所有场景，如果数据分布均匀的话则某些情况下使用user_gift_pkey扫描更加合理。 增加(user_id, id)索引1create index idx_user_id on user_gift(user_id, id); 通过增加where条件列和排序键的复合索引，来避免走主键扫描。 写在最后从排除缓存因素，分析查询计划，定位数据分布倾斜，到调试内核源码来进一步确定原因，最终成功解决性能问题。通过这个有趣的SQL优化经历，相信能给大家带来收获。]]></content>
      <categories>
        <category>2021</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>postgreSQL</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[译文]TreadMarks: 基于工作站网络的共享内存计算]]></title>
    <url>%2F2020%2F04%2F01%2FTreadMarks%2F</url>
    <content type="text"><![CDATA[TreadMarks: 基于工作站网络的共享内存计算 以前学MIT6.824时看过TreadMarks相关论文，这篇论文当时只翻译了一半。最近无意中看到这篇未完成的翻译，google了下发现仍然没有人翻译过这篇论文，于是借个周末时间翻译完了。 论文: TreadMarks: Shared Memory Computing on Networks of Workstations Christiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Ra jamony, Weimin Yu and Willy Zwaenepoel Department of Computer Science Rice University 摘要（Abstract）TreadMarks通过为应用程序提供共享的内存抽象来支持基于工作站网络上的并行计算。共享内存有助于从顺序程序转换成并行程序，因为大多数数据结构都无需做更改。 只需要添加同步操作。 我们会讨论在TreadMarks中使用的用于提供有效共享内存的技术，并讨论了两个主要应用程序的经验，即混合整数编程和遗传连锁分析。 1 引言（ Introduction）高速网络和快速提高的微处理器性能使工作站的网络成为越来越有吸引力的并行计算工具。通过仅仅依靠廉价硬件和软件，工作站网络可以以相对较低的成本提供并行处理。可以将工作站网络的多处理器实现成处理器组[17]，这是许多专用于提供计算周期的处理器。或者，它也可以由一组动态变化的机器组成，在这些机器上利用空闲周期执行长时间运行的计算[14]。在后一种情况下，（硬件）成本基本上为零，因为许多组织机构已经拥有广泛的工作站网络。在性能方面，处理器速度，网络带宽和延迟的改进使联网的工作站能够为越来越多的应用程序提供接近或超过超级计算机的性能。我们的立场不是让更紧密耦合的设计在这种松耦合的多处理器设计面前显得过时。特别是，更低延迟和更高带宽的这些紧耦合的设计能有效地执行对同步和通信要求更加严格的应用程序。但是，我们认为网络技术和处理器性能的进步将极大地扩展可以在工作站网络上有效执行的应用程序的种类。 在论文中，我们会讨论使用TreadMarks分布式共享内存（DSM）系统在工作站网络上进行并行计算的经验。DSM允许进程假定全局共享的虚拟内存，即使它们运行的节点并没有在物理上共享内存[13]。图1举例说明了一个DSM系统，该系统由N个联网的工作站组成，每个工作站都有自己的内存，并通过网络连接起来。DSM软件提供了全局共享内存的抽象，每个处理器都可以访问任何数据项，而程序员不必担心数据在哪里或如何获取。相反，在基于工作站网络的“本地”编程模型中，程序员必须确定处理器何时需要进行通信，与谁进行通信以及要发送什么数据。对于具有复杂数据结构和复杂并行化策略的程序，这可能成为艰巨的任务。在DSM系统上，程序员可以专注于算法开发，而不是管理分区的数据集和数据传输。除了易于编程之外，DSM还提供了与（硬件）共享内存多处理器相同的编程环境，从而允许在两种环境之间进行移植。最后，DSM考虑到无缝集成在网络环境中共享内存的多处理器工作站。 本文描述的实际系统TreadMarks [7]，在Unix工作站上以用户级别运行。不需要进行内核修改或使用特权，并且使用标准Unix编译器和链接器。实现DSM系统的挑战在于确保共享内存抽象不会导致大量通信。TreadMarks中使用了多种技术来应对这一挑战，包括lazy release consistency(懒惰更新释放一致性)[6]和multiple writer protocols（多写协议）[3]。本文首先会描述TreadMarks提供的应用程序编程接口（第2节），接下来，我们会讨论实现方面的挑战。（第3节）以及用于应对这些挑战的技术（第4和第5节）。我们在第6节中会简要的描述TreadMarks的实现，并通过讨论我们在混合整数编程和遗传连锁分析（第7节）这两个大型应用程序中的经验来证明其效率。最后，我们在第8节中讨论相关工作，并在第9节中提供一些结论，以及进一步的工作的方向。 图１ 分布式共享内存 2 共享内存编程（Shared Memory Programming）2.1 应用程序编程接口（Application Programming Interface）TreadMarks API简洁但功能强大（有关C语言接口，见图2）。它提供了用于进程创建和销毁，同步以及共享内存分配的功能。我们专注于同步原语。 共享内存会引起数据竞争。当不同进程以程序员不希望的方式交错对共享变量访问时，就会发生数据竞争。例如，假设一个进程写入共享记录的多个字段，而另一个进程尝试读取这些相同的字段。第二个进程很有可能读取到新写入的值，但也有可能在第一个进程写入新值前读取到旧值。通常不希望发生这种情况。 为了避免这种情况，共享内存并行程序包含同步操作。 在这种特殊情况下，将使用锁来允许每个进程对记录的独占访问。 TreadMarks提供了两种同步原语：barrier(屏障)和exclusive locks(排他锁)。进程通过调用 Tmk_barrier() 来等待屏障。屏障是全局的：调用进程会被暂停，直到系统中的所有进程到达同一屏障为止。进程调用Tmk_lock_acquire来获取锁，然后调用Tmk_lock_release来释放锁。当另一个进程持有该锁时，任何进程都无法获取该锁。同步原语的这种特定选择跟TreadMarks的设计没有任何关系；以后可能会增加其他原语。 我们通过两个简单的应用程序演示了这些以及其他TreadMarks原语的用法。 2.2 两个简单的示例（Two Simple Illustrations）图3和图4说明了TreadMarks API在Jacobi迭代和解决旅行商问题（TSP）中的使用。 我们很清楚这些示例代码的过于简单。 但在这里仅用于演示目的。 实际应用将在第7节中讨论。 Jacobi是一种求解偏微分方程的方法。我们的示例遍历二维数组。在每次迭代期间，每个矩阵元素都会更新为其最近元素的平均值（上方，下方，左侧和右侧）。Jacobi使用临时数组存储每次迭代期间计算的新值，以避免元素的旧值在其被临近元素使用之前被覆盖。在并行版本中，为所有处理器分配的数量大致相等的行。 边界上的行由两个相邻进程共享。 图3中的TreadMarks版本使用两个数组：一个网格和一个临时数组。网格的保存在共享内存中，而临时的是每个进程专有的。网格由进程0分配和初始化。Jacobi中的同步是通过屏障实现的。 Tmk_barrier(0) 确保在开始计算之前，进程0的初始化对所有进程可见。Tmk_barrier(1) 确保所有进程在上一次迭代中完成读取值之前，不会有进程会覆盖网格中的任何值。Tmk barrier(2) 可防止当前迭代计算中的所有网格的值被设置前，任何进程都不可以开始下一次迭代。 旅行商问题（TSP）是查找从指定城市开始，经过地图上所有其他城市恰好一次并返回开始城市的最短路径。使用了一种简单的分支定界算法。每次局部旅行一次就扩展一个城市。如果局部旅行的长度加上路径剩余部分的下限大于当前最短旅行路径的话，则不会再进一步进行局部旅行了，因为他不会产生比当前最短路径更短的路径了。 该程序维护一个局部旅行的列队，保持最短的局部旅行在列队最前面。它不断往列队中添加可能最优的局部旅行，直到从列队的开头开始找到一个比阈值长的路径为止。然后程序将它从列队中移除，并尝试剩余程序的所有排列。最终，它将比较当前路径和包含此路径的最短路径，且如果有必要的话就更新当前最短路径。列队和当前最短路径是共享的，通过锁来保护对他们的访问。 123456789101112131415161718192021222324/* the maximum number of parallel processes supported by TreadMarks */#define TMK_NPROCS/* the actual number of parallel processes after Tmk_startup */extern unsigned Tmk_nprocs;/* the process id, an integer in the range 0 ... Tmk_nprocs - 1 */extern unsigned Tmk_proc_id;/* the number of lock synchronization objects provided by TreadMarks */#define TMK_NLOCKS/* the number of barrier synchronization objects provided by TreadMarks */#define TMK_NBARRIERS/* Initialize TreadMarks and start the remote processes */void Tmk_startup(argc, argv) int argc; char **argv;/* Terminate the calling process. Other processes are unaffected. */void Tmk_exit(status) int status;/* Block the calling process until every other process arrives at the barrier. */void Tmk_barrier(id) unsigned id;/* Block the calling process until it acquires the specified lock. */void Tmk_lock_acquire(id) unsigned id;/* Release the specified lock. */void Tmk_lock_release(id) unsigned id;/* Allocate the specified number of bytes of shared memory */char *Tmk_malloc(size) unsigned size;/* Free shared memory allocated by Tmk_malloc. */void Tmk_free(ptr) char *ptr; 图 2 TreadMarks C语言接口 1234567891011121314151617181920212223242526272829#define M 1024#define N 1024float **grid;float scratch[M][N];Tmk_startup();if (Tmk_proc_id == 0)&#123; grid = Tmk_malloc(M * N * sizeof(float)); initialize grid;&#125;Tmk_barrier(0);length = M / Tmk_nprocs;begin = length * Tmk_proc_id;end = length * (Tmk_proc_id + 1);for (number of iterations)&#123; for (i = begin; i &lt; end; i++) for (j = 0; j &lt; N; j++) scratch[i][j] = (grid[i - 1][j] + grid[i + 1][j] + grid[i][j - 1] + grid[i][j + 1]) / 4; Tmk_barrier(1); for (i = begin; i &lt; end; i++) for (j = 0; j &lt; N; j++) grid[i][j] = scratch[i][j]; Tmk_barrier(2);&#125; 图 3 The TreadMarks Jacobi 程序 实现的挑战（Implementation Challenges）提供内存一致性是DSM系统的核心：DSM软件必须以一种提供全局共享内存的方式在处理器之间移动数据。在李的原始IVY系统中[13]，虚拟内存硬件用于维护内存一致性。每个处理器的本地（物理）存储器形成全局虚拟地址空间的高速缓存（请参见图5）。如果一个页面不在本地处理器的内存中，则会产生缺页错误。DSM软件将该页面的最新副本从其远程位置导入本地内存，然后重新启动该过程。例如，图5显示了处理器1发生缺页错误，然后从处理器3的本地内存中复制需要页面的副本。IVY进一步将读取错误与写入错误区分开。发生读取错误时，页面将所有副本以只读方式进行复制，而发生写入错误时，所有现有副本都将无效，并且写的处理器保留唯一副本。 1234567891011121314151617181920212223242526272829queue_type *Queue;int *Shortest_length;int queue_lock, tour_lock;main()&#123; Tmk_startup(); queue_lock = 0; tour_lock = 1; if (Tmk_proc_id == 0) Queue = Tmk_malloc(sizeof(queue_type)); Shortest_length = Tmk_malloc(sizeof(int)); initialize Heap and Shortest_length Tmk_barrier(0); Tmk_lock_acquire(queue_lock); Exit if queue is empty; Keep adding to queue until a long, promising tour appears at the head; Path = Delete the tour from the head; Tmk_lock_release(queue_lock); length = recursively try all cities not on Path, find the shortest tour length Tmk_lock_acquire(tour_lock); if (length &lt; *Shortest_length) *Shortest_length = length; Tmk_lock_release(tour_lock);&#125; 图4 The TreadMarks TSP 程序 尽管很简单，但DSM的IVY实现可能产生大量通信。而在工作站网络上通信的成本很高。发送消息可能涉及到切入操作系统内核，中断，上下文切换以及可能经过多层网络协议的处理。因此，必须保持较少的消息的数量。 我们使用图3和图4的示例来说明IVY中的一些通信问题。 第一个问题与IVY的一致性模型有关，通常称为顺序一致性（sequential consistency） [9]。粗略地说，顺序一致性要求对共享内存的写入对于其他处理器“立即”变为可见。这就是为什么IVY在写入共享内存之前发送无效消息的原因。在许多情况下，顺序一致性提供了过强的保证。 例如，思考下图4中TSP中最佳巡回的更新及其长度。在IVY中，两个共享内存更新将导致发送无效信息到所有其他缓存了这些变量的页面的处理器。 但是，其实只会在有相应锁保护的临界区内访问这些变量，因此仅将无效发送给获取锁的下一个处理器就足够了，并且仅在获取锁时才发送。 第二个问题涉及到潜在的伪共享（false sharing）的问题。当位于同一个页面的两个或两个以上不相关的数据对象由不同的处理器并发写入时，就会导致伪共享，从而导致该页在处理器之间像乒乓球一样来回移动。由于一致性单元很大（虚拟内存页），所以伪共享是一个潜在的严重问题。图6展示了Jacobi中网格数组可能的页面布局。 当两个处理器都更新其网格数组的一部分时，它们正在同时写入同一页面，并导致该页面在处理器间来回移动多次。 图５ IVH DSM系统操作 4 懒惰更新释放一致性（Lazy Release Consistency）4.1 释放一致性（Release Consistency）释放一致性[5]是一个宽松的内存一致性模型。它不能始终保证一致性。 对共享内存更新的传播可能会延迟一段时间。这样，释放一致性的实现可以将多个更新消息合并为单个消息，从而减少消息的数量。但是，通过在某些同步操作中强制执行一致性，至少对于正确同步的程序而言，对程序员屏蔽了潜在的不一致性。我们都在直观层面上讨论这些问题。关于更详细的说明，读者可以参考Gharachorloo 等人的论文 释放一致性的直觉如下。并行程序不应出现数据竞争，因为这会导致不确定的结果。因此，必须充分的使用同步来防止数据竞争。更具体地说，在对共享内存的两个冲突性访问之间必须存在同步（如果两个访问相同的内存位置，并且其中至少一个是写操作，则这两个访问是冲突性的）。由于存在这种同步，因此DSM系统可以延迟将更新从一个进程发送到另一个进程，直到发生这样的同步事件为止，因为只有执行了同步操作，第二个进程才会访问数据。对于没有数据竞争的程序，顺序一致性和释放一致性的结果是相同的。 我们将通过第2节中的Jacobi和TSP示例来说明这个原理。在Jacobi中，执行barrier 1 后才进行写共享内存操作，此时将新计算的值从临时数组复制到网格数组中。执行barrier 2时计算阶段才结束。执行barrier 2是为了避免在所有新值被写入网格数组前进程就开始下一次迭代。不管是什么存储模型，都必须需要barrier来确保正确性。但是，它的存在使我们可以推迟网格数组元素新值的传播，直到遇到更低的barrier。 在TSP中，任务队列是主要的共享数据结构。处理器从队列中获取任务并对其进行处理，也会创建新的任务。 这些新创建的任务将插入队列。任务队列结构的更新需要对共享内存进行一系列的写入，例如其大小，位于队列开头的任务等。原子地对任务队列数据结构进行访问是为了保证程序的正常运行。一次仅允许一个处理器访问任务队列数据结构。这些保证通过在操作前加锁，操作后解锁来实现。因此只有获得锁的进程才能访问数据结构，也无须立即将更新传播给其他进程。这些处理器需要首先获得锁。因此在获得锁的时候传播数据结构的更新就可以了。 这两个示例说明了释放一致性的基本原理。在共享内存的并行程序中引入了同步，以防止进程在同步操作完成之前查看某些内存。由此得出结论，在同步操作完成之前，不必传播那些内存操作的值。相反，顺序一致性会立即更新每个远程内存。 释放一致性需要少得多的消息，并导致更少的消息延迟到达。 程序员不必对此太在意。 如果程序没有数据竞争，则它的行为就好像在普通的共享内存系统上执行一样，其条件是：所有同步都必须使用TreadMarks提供的原语来完成。 否则，TreadMarks无法知道何时应该让共享内存保持一致性。 图６ Jacobi的伪共享例子 4.2 懒惰更新释放一致性（Lazy Release Consistency）TreadMarks使用懒惰更新释放一致性算法[6]来实现释放一致性。粗略地说，懒惰更新释放一致性在获取锁时确保一致性，相反的，Munin早期实现的释放一致性在释放锁时实施一致性。懒惰更新释放一致性发送的消息更少。在锁释放时，Munin的释放锁的处理器发送缓存了修改数据的所有其他处理器。比较起来，在懒惰更新释放一致性中，一致性消息仅在锁的最后一个释放者和新获取者之间传播。 懒惰更新释放一致性（lazy release consistency）比急迫更新释放一致性（eager release consistency）更复杂一些。在释放后，Munin可以忘记释放的处理器在释放之前所做的所有修改。懒惰更新释放一致性不是这种情况，因为第三个处理器以后可能会获取锁并需要查看修改。在实践中，我们的经验表明，对于工作站网络而言，发送消息的成本很高，减少消息数量所带来的收益超过了更复杂的实现的成本。 5 多写协议（Multiple-Writer Protocols）大多数硬件缓存和DSM系统使用单写程序协议。这些协议允许多个读同时访问同一页面，但是在执行任何修改之前，只允许一个写对页面进行修改。单写程序协议易于实现，因为给定页面的所有副本始终都是相同的，并且始终可以通过从当前具有有效副本的任何其他处理器中获取页面的副本来解决缺页错误。不幸的是，这种简单性通常以牺牲消息流量为代价的。 在写页面之前，所有其他副本都必须无效。如果其页面已失效的处理器仍在访问该页面的数据，这些失效则可能导致随后的访问丢失。 伪共享会由于无关访问之间的干扰而导致单写程序协议的性能更差。DSM系统通常比硬件系统遭遇更多的伪共享，因为它们以虚拟内存页面而不是高速缓存行的粒度跟踪数据访问。 顾名思义，多写协议允许多个写同时修改同一页面，并延后发送保证一致性的消息，尤其是直到同步发生为止 TreadMarks使用虚拟内存硬件来检测对共享内存页面的访问和修改。图7显示了如何使用保护故障（protection faults）来创建差异（diffs）。有效页面最初被写保护。 发生写操作时，TreadMarks会创建虚拟内存页的一个副本，或一个twin（双胞胎），并将其保存在系统空间中。当需要将修改发送到另一个处理器时，则将页面的当前副本与twin逐字比较，并将变化的字节保存到diff数据结构中。一旦创建diff后，twin将被丢弃。 除了处理器第一次访问页面外，其页面副本仅通过应用diff进行更新。 不再需要页面的新完整副本。 使用diff的主要好处是它们可用于实现多写协议，由于diff通常比页面小得多，因此它们还可显着降低总体带宽需求。 图７ DiffCreation ６ TreadMarks系统（The TreadMarks System）TreadMarks完全以Unix上用户库的方式实现的。不需要修改Unix内核，因为现代Unix提供了在用户级别实现TreadMarks所需的所有通信和内存管理功能。用C，C++或者FORTRAN编写的程序可以用这些语言标准的编译器来编译和连接TreadMarks库。该系统是相对便携式的。 当前，它可以在SPARC，DECStation，DEC / Alpha，IBM RS-6000，IBM SP-1和SGI平台以及以太网和ATM网络上运行。 在本节中，我们简要描述如何实现TreadMarks的通信和内存管理。 TreadMarks使用Berkeley套接字接口实现了机器间通信。取决于底层的网络硬件，例如以太网或ATM，TreadMarks使用UDP / IP或AAL3 / 4作为消息传输协议。默认情况下，除非计算机通过ATM LAN连接，否则TreadMarks使用UDP / IP。AAL3 / 4是ATM标准规定的面向连接的最大努力交付协议（best-efforts delivery protocol）。 由于UDP / IP和AAL3 / 4都不保证可靠的传递，因此TreadMarks使用轻量，特定操作的用户级协议来确保消息到达。 TreadMarks发送的每个消息要么是请求消息要么是响应消息。TreadMarks发送的消息可能是显式调用TreadMarks库产生的或者是缺页错误产生的。一旦机器发送了请求消息，它将阻塞直到有一个请求消息或者预期的响应消息到达。为了将请求处理的延迟最小化，TreadMarks使用SIGIO信号的处理程序来处理请求。消息到达用于接收请求消息的任何套接字都会产生SIGIO信号。由于AAL3 / 4是面向连接的协议，因此与其他每台机器都有一个相对应的套接字。为了确定哪个套接字处理请求，处理程序将执行select系统调用。处理程序收到请求消息后，将执行指定的操作，发送响应消息，然后返回到中断的进程。 为了实现一致性协议，TreadMarks使用mprotect系统调用来控制对共享页面的访问。尝试在共享页面上执行受限访问都会产生SIGSEGV信号。SIGSEGV信号处理程序检查本地数据结构以确定页面的状态。如果本地副本是只读的，则该处理程序从空闲页面池中分配一个页面，并执行bcopy来创建一个twin。最后，处理程序将访问权限升级到原来的页面并返回。如果本地页面无效，则处理程序执行一个过程（procedure）通过从最少的远程计算机集中获取对共享内存的必要更新。 7 应用（Applications）使用TreadMarks已实现了许多应用程序，并且一些基准测试的性能已在之前进行了报道[7]。在这里，我们描述最近使用TreadMarks实现的两个大型应用程序的经验。在本文作者的帮助下，顺序执行代码的作者将混合整数编程（mixed integer programming）和遗传连锁分析（genetic linkage analysis）这些应用程序从现有的顺序代码进行了并行处理。虽然很难量化所涉及的工作量，但已经证明为获得有效的并行代码而进行的修改的工作量相对很小，这将在本节的其余部分中演示。 7.1 混合整数编程（Mixed Integer Programming）混合整数编程（MIP）是线性编程（LP）的一个版本。在LP中，在由一组线性不等式描述的区域中优化了目标函数。在MIP中，部分或全部变量被约束为只能用整数值（有时仅是0或1）。 图8显示了一个精确的数学公式，图9显示了一个简单的二维实例。 图 8 混合整数编程问题(MIP) ​ 图 9 MIP二维实例 图10 解决MIP问题的分支界定法 Lee等人实现了用于解决MIP问题的TreadMarks代码 [11]。该代码使用分支剪切法。首先将MIP问题简化为相应的LP问题。通常，这个LP问题的解决方案将为某些约束为整数的变量产生非整数值。下一步是选择这些变量中的一个，并分支出两个新的LP问题，一个问题具有 $x_{i} = \left \lfloor x_{i} \right \rfloor$ 的附加约束，另一个具有 $x_{i} = \left \lceil x_{i} \right \rceil$ 的附加约束（见图10）。慢慢地，该算法会生成此类分支的树。找到解决方案后，此解决方案便会在解决方案上建立界限。分支中的LP问题的解决方案产生的结果低于此边界则无需再进入下一步了。为了加快此过程，该算法使用一种称为plunging的技术，本质上是对树进行深度优先搜索，以找到整数解并尽快建立边界。最后一种感兴趣的改进的算法是cutting planes的使用。 这些是添加到LP问题的附加约束，以加强对整数问题的描述。 该代码用于解决MIPLIB库中的所有51个问题。该库包括航空人员调度，网络流量，工厂位置，机队调度等方面的代表性示例。图11显示了在8台运行Ultrix 4.3且通过100Mbps Fore ATM交换机连接的DecStation-5000 / 240机器的网络上获得的加速，这些问题的顺序运行时间超过2,000秒。对于大多数问题，加速几乎是线性的。 一个问题表现为超线性加速，这是因为并行代码命中了前期运行的解决方案，从而裁剪掉了大多数分枝定界树。对于另一个问题，几乎没有加速，因为在预处理步骤之后不久就找到了解决方案，此时还没有进行并行化。除了MIPLIB库中的问题外，该代码还可用于解决以前无法解决的多商品流问题[2]。 该问题在具有8个处理器的IBM SP-上花费的CPU时间为30天，并且还表现出接近线性的加速。 图11 MIPLIB库的结果 7.2 遗传连锁（Genetic Linkage）遗传连锁分析是一种统计技术，使用家族谱系信息来绘制人类基因图谱并在人类基因组中定位疾病基因。生物学和遗传学的最新进展已提供了大量可用的遗传材料，使计算成为进一步研究的瓶颈。 在Mendelian经典的继承理论中，孩子的染色体接收父母一方每个染色体的一条链。考虑重组时，情况会有所不同。如果发生重组，则孩子的染色体链将包含父母染色体的每一条链中的每一条（见图12）。连锁分析的目的是推导我们寻找的基因与已知位置的基因之间发生重组的概率。 从这些概率可以计算出基因在染色体上的大概位置。 ILINK [4]是广泛使用的遗传连锁分析程序的一个并行版本，它是LINKAGE [10]程序包的一部分。ILINK将称为谱系的家谱作为输入，并增加了有关该家族成员的一些遗传信息。它计算重组概率θ的最大似然估计。从最上层来看，ILINK由一个优化θ的循环组成。在优化循环的每次迭代中，程序遍历整个谱系，一次遍历一个核心家庭，在已知家庭成员的遗传信息的情况下，计算当前θ的可能性。对于核心家庭的每个成员，该算法都会更新一系列条件概率，这些条件概率表示个体具有特定遗传特征的概率，其条件取决于θ和已经遍历的部分家谱。 通过以均衡负载的方式在可用处理器之间分配每个核心家庭的迭代空间，可以并行化上述算法。负载均衡是必不可少的，且依赖于阵列元素中表示的遗传信息的知识。另一个方式是拆分树，它未能产生良好的加速效果，因为大多数计算都发生在树的一小部分上（通常，这些节点靠近树的根部，他们表示已知遗传信息很少的已故者）。另外，无法并行评估不同的θ值，因为优化程序会从一个θ值顺序移至下一个θ值。 图13显示了使用ILINK为各种数据集获得的加速。数据集来自对真实的疾病基因定位的研究。对于运行时间较长的数据集，可以实现良好的加速。 对于最小的数据集BAD，由于通信与计算的比率变大，因此加速要少得多。 图12 DNA重组 8 相关工作（Related Work）本节的目标不是进行广泛的并行编程研究，而是以一个独特的示例说明替代性方案，并将最终的系统与TreadMarks进行比较。 消息传递（PVM）。 当前，消息传递是分布式存储系统的主要编程范例。便携式虚拟机（PVM）[15]是一种流行的消息传递软件。它允许将异构的计算机网络视为单个并发计算引擎。尽管PVM中的编程比底层机器的本机消息传递范例中的编程容易得多，并且可移植性强，但是应用程序程序员仍然需要编写代码来显式交换消息。TreadMarks的目标是减轻程序员的负担。 对于具有复杂数据结构和复杂并行化策略的程序，我们认为这是一个主要优势。 隐式并行（HPF）。 如HPF [8]中所示，隐式并行性依赖于用户提供的数据分布，编译器随后使用这些数据分布来生成消息传递代码。这种方法适用于数据并行程序，例如Jacobi。 动态并行性的程序，例如TSP，ILINK或MIP，很难在HPF框架中表达。 面向对象的并行计算（Orca）。Orca [16]和其他面向对象的并行计算系统一样没有为程序员提供共享的存储空间，而是支持对象的共享空间，每个对象都可以通过适当同步的方法进行访问。除了从编程的角度来看的优点之外，这种方法还允许编译器推断出某些优化方法，这些优化方法可用于减少通信量。缺点是，顺序程序中“自然”的对象通常不是正确的并行化对象，需要更多的更改才能获得有效的并行程序 硬件共享内存实现（DASH）。 另一种方法是在硬件中实现共享内存，对少量处理器使用侦听总线协议，或对大量处理器使用基于目录的协议（例如[12]）。我们使用这种方法来共享编程模型，但是我们的实现避免了昂贵的缓存控制器硬件。 另一方面，硬件实现可以有效地支持具有更细粒度并行性的应用程序。 条目一致性（Entry Consistency）（中途）。 条目一致性是另一种宽松的内存模型[1]。它要求所有共享数据都与同步对象相关联。当获取同步对象时，只传输与该同步对象关联的修改后的数据，从而进一步减少通信数据量。但是，条目一致性内存模型比释放一致性弱，这可能会使编程更加困难。 图13 ILINK结果 9 总结和下一步工作（Conclusions and Further Work）我们的经验表明，使用适当的实现技术，分布式共享内存可以为工作站网络上的并行计算提供有效的平台。多数应用程序移植到TreadMarks分布式共享内存系统上，几乎没有困难，并且性能良好。在下一步的工作中，我们打算尝试其他实际应用，包括地震建模代码。我们还在开发各种工具，以进一步减轻编程负担并提高性能。特别是，我们正在研究使用诸如支持预取的编译器，以及使用性能监视工具来消除不必要的同步。 引用（References）[1] B.N. Bershad, M.J. Zekauskas, and W.A. Sawdon. The Midway distributed shared memory system. In Proceedings of the ‘93 CompCon Conference, pages 528-537, February 1993.[2] D. Bienstock and O. Gumluk. Computational experience with a difficult mixed-integer multi-commodity flow problem. To appear in Mathematical Programming, 1994.[3] J.B. Carter, J.K. Bennett, and W. Zwaenepoel. Implementation and performance of Munin. In Proceedings of the 13th ACM Symposium on Operating Systems Principles, pages 152-164, October 1991.[4] S. Dwarkadas, A.A. Schäffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. Parallelization of general linkage analysis problems. Human Heredity, 44:127-141, 1994.[5] K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. Memory consistency and event ordering in scalable shared-memory multiprocessors. In Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 15-26, May 1990.[6] P. Keleher, A. L. Cox, and W. Zwaenepoel. Lazy release consistency for software distributed shared memory. In Proceedings of the 19th Annual International Symposium on Computer Architecture, pages 13-21, May 1992.[7] P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: Distributed shared memory on standard workstations and operating systems. In Proceedings of the 1994 Winter Usenix Conference, pages 115-131, January 1994.[8] C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. The High Performance Fortran Handbook. The MIT Press, 1994.[9] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess programs. IEEE Transactions on Computers, C-28(9):690{691, September 1979.[10] G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. Strategies for multilocus linkage analysis in humans. Proc. Natl. Acad. Sci. USA, 81:3443-3446, June 1984.[11] E. Lee, R. Bixby, W. Cook, and A.L. Cox. Parallelism in mixed integer programming. Submitted for publication, 1994.[12] D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. The directory-based cache coherence protocol for the DASH multiprocessor. In Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 148-159, May 1990.[13] K. Li and P. Hudak. Memory coherence in shared virtual memory systems. ACM Transactions on Computer Systems, 7(4):321-359, November 1989.[14] M. Litzkow, M. Livny, and M. Mutka. Condor - a hunter of idle workstations. In Proceedings of the 8th International Conference on Distributed Computing Systems, pages 104-111, June 1988.[15] V. Sunderam. PVM: A framework for parallel distributed computing. Concurrency:Practice and Experience, 2(4):315-339, December 1990.[16] A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal. Parallel programming using shared ob jects and broadcasting. IEEE Computer, 25(8):10-20, August 1992.[17] A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, J. Jansen, and G. van Rossum. Experiences with the Amoeba distributed operating system. Communications of the ACM, 33(12):46-63, December 1990.]]></content>
      <categories>
        <category>2020</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[percolator的理解与开源实现分析]]></title>
    <url>%2F2019%2F02%2F17%2Fpercolator%2F</url>
    <content type="text"><![CDATA[对percolator的理解与tidb实现 论文论文地址https://research.google/pubs/pub36726/ 概述设计percolator的目的为大数据集群进行增量处理更新的系统，主要用于google网页搜索索引服务。使用基于Percolator的增量处理系统代替原有的批处理索引系统后，Google在处理同样数据量的文档时，将文档的平均搜索延时降低了50%。 基于bigtable单行事务实现跨行事务Percolator 在 Bigtable 之上实现的，以client library 的方式实现。Percolator 利用 Bigtable 的单行事务能力，依靠client的协议和一个全局的授时服务器 TSO 以及两阶段提交协议来实现了跨机器的多行事务。 MVCC与snapshot isolationPercolator依据全局时间戳和MVCC实现 Snapshot Isolation 隔离级别的并发控制协议。 percolator特点 事务: 跨行、跨表的、基于快照隔离的ACID事务 观察者(observers)：一种类似触发器的通知机制 设计隔离等级通过MVCC来实现SI(Snapshop isolation)隔离等级。 Percolator 使用Bigtable的时间戳记维度实现了数据的多版本化。优点如下： 读操作：可以读取任何指定时间戳版本的记录 写操作，能很好的应对写写冲突：若两个事务操作同一记录，只有一个会提交成功 I存在write skew(写偏斜) 锁机制 Because it is built as a client library accessing Bigtable, rather than controlling access to storage itself, Percolator faces a different set of challenges implementing distributed transactions than traditional PDBMSs. Other parallel databases integrate locking into the system component that manages access to the disk: since each node already mediates access to data on the disk it can grant locks on requests and deny accesses that violate locking requirements. Percolator锁的管理必须满足以下条件： 能应对机器故障：若一个锁在两阶段提交时消失，系统可能将两个有冲突的事务都提交 高吞吐量：上千台机器会同时请求获取锁 低延时 锁服务要实现： 多副本 ： survive failure distributed and balanced ： handle load 写入持久化存储系统 时间戳Timestamp Oracle(不是Oracle数据库)： TSO通过统一中心授权可以保证按照递增的方式分配逻辑时钟,任何事件申请的时钟都不会重复，能够保证事务版本号的单调递增，确保分布式事务的时序。 所以TSO是一个分配严格的单调递增时间戳的服务器。 优化因为每个事务都需要调用oracle两次，所以这个服务必须有很好的可伸缩性。Oracle会定期分配一个范围的时间戳，然后将范围中的最大值写入持久化，Oracle在内存中原子递增来快速分配时间戳，查询时不涉及磁盘I/O。如果oracle重启，将以存储中的最大值作为开始值。worker会维持一个长连接RPC到oracle，低频率的、批量的获取时间戳。 性能Oracle中单台机器每秒向外分配接近两百万的时间戳。 关于批量获取时间戳 批量获取时间戳并不会造成乱序问题，因为就算事务A先获取时间戳T1，事务B后获取时间戳T2，T1&lt;T2，那么分布式系统中，也无法保证事务A先执行，事务B后执行。如果事务B先执行，那么事务A势必能发现写冲突从而rollback。 单点 为了保证单调递增的特性，所以很多TSO的开源实现都存在单点问题。如tidb的TSO。而且，一般TSO也存在跨数据中心高延迟的问题。 其他时序方案 Logic Clock: dynamoDB True Time : spanner Hybrid Logic Clock : cockroachDB，没有单点问题，但是为了解决时钟误差而无法避免的时延问题。 数据存储percolator定义了5个列 Column Use c:lock An uncommitted transaction is writing this cell; contains the location of primary lock c:write Committed data present; stores the Bigtable timestamp of the data c:data Stores the data itself c:notify Hint: observers may need to run c:ack O Observer “O” has run ; stores start timestamp of successful last run Lock 事务的锁，key value映射 1(key,start_ts) ==&gt; (primary_key,lock_type) key：数据的key start_ts：事务开始时间 primary：该锁的primary的引用。事务从待修改的keys中选择一个作为primary,其余的则作为secondary，secondary的primary_key指向primary的key，事务的上锁和解锁都由primary key决定。 Write已提交的数据对应的时间戳。key value映射 1(key,commit_ts) ==&gt; (start_ts) key：数据的key commit_ts：事务的提交时间 start_ts：事务的开始时间（此数据在data中的时间戳版本） Data 存储数据的列，key value映射 1(key,start_ts) ==&gt; (value) key：对应的主键 start_ts：事务的开始时间 value：除主键外的数据列 Notifynotify列仅仅是一个hint值（可能是个bool值），表示是否需要触发通知。 Ackack列是一个简单的时间戳值，表示最近执行通知的观察者的开始时间。 案例以银行转账为案例Bob 向 Joe 转账7元。事务开始时间：start timestamp =7 ，提交时间：commit timestamp=8。 Bob有10元：查询column write获取最新时间戳版本的数据(data@5),然后从column data里面获取时间戳为5的数据($10），Joe($2) stat timestamp=7 作为当前事务的开始时间戳，将Bob选为此事务的primary key，再写入column:lock对Bob上锁，同时将column:data列更新为7:$3。 start timestamp=7作为锁定Joe账户的时间戳，更新其column:data为$9，其锁是secondary指向primary 当前时间戳commit timestamp=8作为事务提交时间戳：删除primary所在的lock，在write列中写入commit_ts：data@7 在所有secondary中写入column:write且清理column:lock，事务完成。 流程伪代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106class Transaction &#123; struct Write&#123; Row row; Column: col; string value;&#125;; vector&lt;Write&gt; writes_; int start_ts_; Transaction():start_ts_(oracle.GetTimestamp()) &#123;&#125; // 往事务中添加一行数据 void Set(Write w) &#123;writes_.push_back(w);&#125; bool Get(Row row, Column c, string* value) &#123; while(true) &#123; // 开启bigtable事务 bigtable::Txn = bigtable::StartRowTransaction(row); // Check for locks that signal concurrent writes. // 检查[0, start_ts]内是否有锁，有则等待再重试 if (T.Read(row, c+&quot;locks&quot;, [0, start_ts_])) &#123; // There is a pending lock; try to clean it and wait BackoffAndMaybeCleanupLock(row, c); continue; &#125; &#125; // Find the latest write below our start_timestamp. // 读取column:wriet的[0, start_ts]的最新提交记录版本 latest_write = T.Read(row, c+&quot;write&quot;, [0, start_ts_]); if(!latest_write.found()) return false; // no data // 根据此时间戳版本去column:data读取数据 int data_ts = latest_write.start_timestamp(); *value = T.Read(row, c+&quot;data&quot;, [data_ts, data_ts]); return true; &#125; // prewrite : 尝试上锁+写数据，如果锁冲突则返回失败 bool Prewrite(Write w, Write primary) &#123; Column c = w.col; // bigtable 事务 bigtable::Txn T = bigtable::StartRowTransaction(w.row); // 检查写写冲突：从column:write获取最新数据，如果commit_ts&gt;=事务开始时间，则冲突 if (T.Read(w.row, c+&quot;write&quot;, [start_ts_, max])) return false; // 检查column:lock，如果存在已存在，则锁冲突 if (T.Read(w.row, c+&quot;lock&quot;, [0, max])) return false; // 往column:lock写入锁，若为secondary，则指向primary T.Write(w.row, c+&quot;lock&quot;, start_ts_, &#123;primary.row, primary.col&#125;) // 往column:data写入(key, start_ts = value) T.Write(w.row, c+&quot;data&quot;, start_ts_, w.value); // 提交bigtable事务 return T.Commit(); &#125; // 此commit整合 2PC的prewrite和commit， // 对外caller而言不需要关注是2PC提交，直接commit就好 bool Commit() &#123; // ***** Prewrite *****// // 2PC 第一阶段：prewrite // 第一行为primary Write primary = write_[0]; // 其余为secondary vector&lt;Write&gt; secondaries(write_.begin() + 1, write_.end()); if (!Prewrite(primary, primary)) return false; for (Write w : secondaries) if (!Prewrite(w, primary)) return false; // ***** Commit *****// // 2PC第二阶段：commit // 从oracle获取commit timestamp int commit_ts = oracle.GetTimestamp(); // Commit primary first. Write p = primary; // primary : 先开始bigtable的事务 bigtable::Txn T = bigtable::StartRowTransaction(p.row); // primary : 如果primiary的锁不存在，则abort if (!T.Read(p.row, p.col+&quot;lock&quot;, [start_ts_, start_ts_])) return false; // aborted while working // primary : 在column:write 列写入开始时间（Pointer to data written at start_ts_） T.Write(p.row, p.col+&quot;write&quot;, commit_ts, start_ts_); // primary : 移除column:lock 列 T.Erase(p.row, p.col+&quot;lock&quot;, commit_ts); // primary : 提交bigtable 事务 if(!T.Commit()) return false; // Second phase: write our write records for secondary cells. // 遍历所有secondary，写入column:write数据同时删除column:lock for (Write w:secondaries) &#123; bigtable::write(w.row, w.col+&quot;write&quot;, commit_ts, start_ts_); bigtable::Erase(w.row, w.col+&quot;lock&quot;, commit_ts); &#125; return true; &#125;&#125;; 事务 第一阶段 获取时间戳T1, 写入column:data和锁：时间戳都为T1 第二阶段 获取时间戳T2 写入column:write：key:commit_ts:start_ts (key:T2:T1) 删除锁 读取 获取时间戳Tx 从column:write 读取key[0, Tx]的最大时间戳数据(获取到事务写入的commit_ts=T2) 从T2中提取出start_ts为T1 从column:data中读取 key:T1的数据 所以读取到的数据是commit时间戳的数据。 清理锁若客户端在Commit一个事务时，出现了异常，Prepare时产生的锁会被留下。为避免将新事务挂住，Percolator必须清理这些锁。 Percolator用lazy方式来处理未处理的锁：当事务在执行时，发现其他事务造成的锁未处理掉，事务将决定其他事务是否失败，以及清理其他事务的那些锁。 当客户端在执行两阶段提交的commit阶段crash时，事务会留下一个提交点commit point(至少已经写入一条write记录)，但可能会留下一些lock未被处理掉 如果priarmy lock 已被write所替代：意味着该事务已被提交，事务需要roll forword，也就是对所有涉及到的、未完成提交的数据，用write记录替代标准的锁standed lock。 如果primary lock存在：事务将roll back(因为总是最先提交primary，所以primary未被提交时，可以安全地执行回滚) 这些都是基于bigtable的事务中的。 清理操作在primary锁上是同步的，所以清理alive客户端持有的锁是安全的；然而回滚会强迫事务取消，这会严重影响性能。所以，一个事务将不会清理一个锁除非它猜测这个锁属于一个僵死的worker。Percolator使用简单的机制来确定另一个事务的活跃度。运行中的worker会写一个token到Chubby锁服务来指示他们属于本系统，token会被其他worker视为一个代表活跃度的信号（退出时token会被自动删除）。有些worker是活跃的，但不在运行中，为了处理这种情况，我们附加的写入一个wall time到锁中；一个锁的wall time如果太老，即使token有效也会被清理。有些操作运行很长时间才会提交，针对这种情况，在整个提交过程中worker会周期的更新wall time。 通知用户对感兴趣的列编写观察者function注册到percolator，当列发生改变时，percolator通知percolator的worker运行用户function。 通知与写操作不是原子的通知类似于数据库中的触发器，然而不同的是，通知在其他事务(worker)中执行，所以写操作与观察者执行不是原子的，且观察者的执行会有时效性问题。 这和传统关系型数据库的ACID的C有一定的差别，我猜: 所以这里不叫trigger而是通知的原因 通知与观察者无限循环编写观察者时，用户要自己考虑通知与观察者进入无限循环的情况(通知-&gt;观察者-&gt;通知-&gt;观察者…..)。 通知的”丢失”一个列的多次更改只会触发一次通知，所以通知和操作系统的中断一样会存在“丢失”的问题。 实现通知机制 为了实现通知机制，Percolator需要高效找到被观察的脏cell。Percolator在Bigtable维护一个“notify”列(notify列为一个独立的Bigtable locality group)，表示此cell是否为脏。当事务修改被观察的cell时，则设置cell的notify。worker对notify列执行一个分布式扫描来找到脏cell。找到notify则触发观察者并且等到观察者事务提交成功后，会删除对应的notify cell。 tidb的实现percolator定义了5个列：data, write, lock, ack, notify。tidb定义了其中3个：data, write, lock。所以tidb没有实现notify功能(tidb不需要增量处理能力)。 tidb定义了3个rocksdb的column family： CF_DEFAULT：对应percolator的data列 CF_LOCK：对应percolator的lock列 CF_WRITE：对应percolator的write列 CF_DEFAULT 1(key, start_ts) ==&gt; value CF_LOCK 1key ==&gt; lock_info 同一时刻一个key最多只有一个锁，所以，tidb的锁没有start_ts。 CF_WRITE 1(key, commit_ts) ==&gt; write_info tidb 1 client 向 tidb 发起开启事务 begin 2 tidb 向 pd 获取 tso 作为当前事务的 start_ts 3 client 向 tidb 执行以下请求： 读操作，从 tikv 读取版本 start_ts 对应具体数据. 写操作，写入 memory 中。 4 client 向 tidb 发起 commit 提交事务请求 5 tidb 开始两阶段提交。 6 tidb 按照 region 对需要写的数据进行分组。 7 tidb 开始 prewrite 操作：向所有涉及改动的 region 并发执行 prewrite 请求。若其中某个prewrite 失败，根据错误类型决定处理方式： KeyIsLock：尝试 Resolve Lock 后，若成功，则重试当前 region 的 prewrite[步骤7]。否则，重新获取 tso 作为 start_ts 启动 2pc 提交（步骤5）。 WriteConfict 有其它事务在写当前 key, abort事务 其它错误，向 client 返回失败。 8 commit : tidb 向 pd 获取 tso 作为当前事务的 commit_ts。 9 tidb 开始 commit:tidb 向 primary 所在 region 发起 commit。 若 commit primary 失败，则先执行 rollback keys,然后根据错误判断是否重试: LockNotExist abort事务 其它错误，向 client 返回失败。 10 tidb 向 tikv 异步并发向剩余 region 发起 commit。 11 tidb 向 client 返回事务提交成功信息。 所有涉及重新获取 tso 重启事务的两阶段提交的地方，会先检查当前事务是否可以满足重试条件：只有单条语句组成的事务才可以重新获取tso作为start_ts。 tikvPrewrite伪代码 -&gt; 代表rpc调用, 例如tidb-&gt;tikv.Prewrite tidb调用tikv的Prewrite接口 . 代表进程内调用, 例如memory.Put往内存模型写数据 1234567891011121314151617181920212223242526272829303132333435363738start_ts = tidb-&gt;pd.GetTso() // get start_ts// tidb调用tikv prewrite接口 tidb-&gt;tikv.Prewrite(start_ts, data_list)&#123; // tikv prewrite实现 keyIsLockedArray = [] // prewrite each key with start_ts in memory，中间出现失败，则整个prewrite失败 for key in data_list &#123; // check write conflict: 通过raft获取key的数据 record = raft.Get(WriteColumn, key, start_ts) if record.commit_ts &gt;= start_ts &#123; return error(write conflict, END) &#125; // check lock lock = raft.Get(LockColumn, key) if lock != null &amp;&amp; lock.ts != start_ts // lock已存在且为其他tx的锁 &#123; keyIsLockedArray.append(key) continue &#125; // 往内存中的 lock 列写入 lock(start_ts,key) 为当前key加锁, // 若当前key被选为 primary, 则标记为 primary, // 若为secondary,则标明指向primary的信息。 memory.Put(LockColumn, key, start_ts, (primary|secondary), ttl, short_value) memory.Put(DataColumn, key, start_ts, long_value) &#125; if len(keyIsLockedArray) &gt; 0 &#123; return error(keyIsLockedArray) &#125; // 将此事务在内存模型中写入的数据 持久化到raft中 raft.Commit(memory_data) return ok&#125; Commit伪代码 1234567891011121314151617181920212223242526272829// tidb调用rikv的Commit接口，进行2PC的Commit阶段tidb-&gt;tikv.Commit(keys, start_ts, commit_ts)&#123; for key in keys // do commit &#123; lock = raft.Get(LockColumn, key) // lock存在且匹配，则提交 if lock != null &amp;&amp; lock.ts == start_ts &#123; memory.Put(WriteColumn, key, commit_ts, start_ts) memory.Del(LockColumn, key, start_ts) &#125; // lock does not exist or tx dismatch else if lock == null || lock.ts != start_ts &#123; record = raft.Get(WriteColumn, key, start_ts, commit_ts) if record != null &amp;&amp; record.write_type == (PUT|DELETE|Lock) &#123; continue; // already commited &#125; else if record == null || record.write_type == RollBack &#123; return error(tx conflict, lock not exist) &#125; &#125; &#125; // commit to raft raft.Save(memory.data) return ok&#125; Rollback当事务在两阶段提交过程中失败时， tidb 会向当前事务涉及到的所有 tikv 发起回滚操作。 伪代码 123456789101112131415161718192021222324252627282930313233343536373839404142// tidb调用tikv的Rollback接口tidb-&gt;tikv.Rollback(keys)&#123; // Rollback接口实现 // 检查合法性 for key in keys &#123; // 检查当前key的锁 lock=memory.GetLockColumn(start_ts, key) if lock != null and lock.ts = start_ts &#123; // 如果锁还存在且是之前的锁，则删除锁，写入的数据 // 且在WriteColumn写入rollback记录防止后面commit请求的到来 memory.Del(DataColumn, key, start_ts) memory.Put(WriteColumn, key, start_ts, rollback) meomry.Del(LockColumn, key, start_ts) continue &#125; // 检查提交情况 record = raft.Get(WriteColumn, key, start_ts) if record != null &#123; if record.status == (PUT|DELETE) &#123; return error(transaction is already commited.) &#125; else if record.status == RollBack &#123; continue; // already rollbacked &#125; &#125; else &#123; // record is null // 提交纪录不存在，说明当前 key 尚未被 prewrite 过， // 为预防 prewrite 在rollback之后过来(可能网络原因)， // 在这里留下 (key,start_ts,rollback)记录 memory.Put(WriteColumn, key, start_ts, rollback) continue &#125; // persist raft-&gt;Save(memory.data) &#125; return ok&#125; Resolve Lock若客户端在Commit一个事务时，出现了异常，Prepare 时产生的锁会被留下。为避免将新事务hang住，Percolator必须清理这些锁。 Percolator用lazy方式处理这些锁：当事务A在执行时，发现事务B造成的锁冲突，事务A将决定事务B是否失败，以及清理事务B的那些锁。 tidb 在执行 prewrite, get 过程中，若遇到锁，在锁超时的情况下，会向 tikv 发起清锁操作。 1234567891011121314151617181920// tidb调用tikv的ResolveLock接口tidb-&gt;tikv.ResolveLock(start_ts, commit_ts)&#123; // 找出所有 lock.ts==start_ts 的锁并执行清锁操作 locks = raft.Scan(LockColumn, lock.ts = start_ts) for (lock in locks) &#123; // commit_ts存在，则说明已提交 if (commit_ts != null) &#123; // 对已上锁的key进行提交 memory.Commit(lock.key, commit_ts) &#125; else &#123; memory.Rollback(lock.key) &#125; &#125; raft-&gt;Save(memory.data) return ok&#125; Get12345678910111213141516171819202122232425262728293031323334// tidb调用tikv的Get接口tidb-&gt;tikv.Get(key, start_ts)&#123; // check lock: 如果锁存在且在此之前加的锁，则返回锁冲突 lock = raft-&gt;Get(LockColumn, key) if (lock != null &amp;&amp; lock.ts &lt;= start_ts) return error(isLocked); version = start_ts - 1; RAFTGET: // get writeColumn： 获取小于start_ts的最新提交记录 data = raft-&gt;Get(WriteColumn, key, version) if (data != null) &#123; if (data.writeType == PUT) &#123; if (data.isShortValue) return data.shortValue; // long value return raft-&gt;Get(DataColumn, key, start_ts) &#125; else if (data.writeType == DELETE) &#123; // 没有出现过该值，或该值最近已被删除，返回tidb空 return ok(None)； &#125; else if (data.writeType == &quot;LOCK | ROLLBACK&quot;) &#123; // version=commit_ts-1, 继续查找下一个最近版本 version=commit_ts-1 goto RAFTGET &#125; &#125; else &#123; return ok(None) &#125;&#125; GCTiDB 的事务的实现采用了MVCC机制，当新写入的数据覆盖旧的数据时，旧的数据不会被替换掉，而是与新写入的数据同时保留，并以时间戳来区分版本。GC 的任务便是清理不再需要的旧数据。 一个 TiDB 集群中会有一个 TiDB 实例被选举为 GC leader，GC 的运行由 GC leader 来控制。 GC 会被定期触发。每次 GC 时，首先，TiDB 会计算一个称为 safe point 的时间戳，接下来 TiDB 会在保证 safe point 之后的快照全部拥有正确数据的前提下，删除更早的过期数据。 每一轮 GC 分为以下三个步骤： Resolve Locks：该阶段会对所有 Region 扫描 safe point 之前的锁，并清理这些锁 Delete Ranges：该阶段快速地删除由于 DROP TABLE/DROP INDEX 等操作产生的整区间的废弃数据 Do GC：该阶段每个 TiKV 节点将会各自扫描该节点上的数据，并对每一个 key 删除其不再需要的旧版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344// tidb 向 tikv 发起 GC操作，要求清理 safe-point 版本之前的所有无意义版本// tidb调用tikv的GC接口tidb-&gt;tikv.Gc(savePoint)&#123; startKey=null; for &#123; // scan (startKey, maxKey] keys = raft.Scan(WriteColumn, startKey, batchSize) if (batch == null) return ok; // 对每个key进行gc操作 for (key in keys) &#123; bool remove_older = false; // 清理write中commit_ts &lt;= startPoint的每个item // item: 一个key的所有write记录 for (item in key) &#123; if (remove_older is true) &#123; memory.Del(WriteColumn, item); memory.Del(DataColumn, item); &#125; // check if is ordest version to save if (writeType == PUT) &#123; // 保留该提交，清理所有该提交之前的数据 memory.Set(memoryOrder, true); &#125; else if (writeType == DELETE) &#123; // 清理所有safe-point 之前的数据 remove_older = true; memory.Del(WriteColumn, item); &#125; else if (writeType == ROLLBACK | LOCK) &#123; // 清理所有小于 safe-point 的 Rollback 和 Lock memory.Del(WriteColumn, item); &#125; &#125; &#125; startKey = keys[keys.length() - 1]; &#125;&#125; 优化Parallel Prewritetikv分批的并发进行prewrite，不会像percolator要先prewrite primary，再去prewrite secondary。如果事务冲突，导致rollback，在tikv的rollback实现中，其会留下rollback记录，这样就会导致事务的prewrite失败，而不会产生副作用。 Short Value对于percolator，先读取column:write 列，提取到key的start_ts，再去column:data列读取key数据本身。这样会造成两次读取，tidb的优化是，如果数据本身很小，那么就直接存储在colulmn:write中，只需读取一次即可。 Point Read Without Timestamp为了减少一次RPC调用和减轻TSO压力，对于单点读，并不需要获取timestamp。 因为单点读不存在跨行一致性问题(读取多行数据时，必须是同一个版本的数据)，所以直接可以读取最新的数据即可。 Calculated Commit Timestamp如果不通过TSO获取commit_ts，则会减少一次RPC交互从而降低事务的时延。 然而，为了实现SI的RR特性(repeatable read)，所以commit_ts需要确保其他事务多次读取的值是一样的。那么commit_ts就和其他事务的读取有相关性。 下面公式可以计算出一个commit_ts 1max&#123;start_ts, max_read_ts_of_written_keys&#125; &lt; commit_ts &lt;= now 由于不可能记录每个key的最大的读取时间，但是可以记录每个region的最大读取时间，所以公式转换为： 1commit_ts = max&#123;start_ts, region_1_max_read_ts, region_2_max_read_ts, ...&#125; + 1 region_x_max_read_ts : 事务涉及到的key的region。 Single Region 1PC对于事务只涉及到一个Region，那么其实是没有必要走2PC流程的。直接提交事务即可。]]></content>
      <categories>
        <category>2019</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
        <tag>percolator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对领域驱动设计的理解与实践]]></title>
    <url>%2F2018%2F08%2F19%2Fddd%2F</url>
    <content type="text"><![CDATA[对领域驱动设计的理解与实践 更新于：2020-06-14 什么是DDD领域驱动设计（Domain-Driven-Design）是一种针对大型复杂系统的领域建模与分析方法论。2003 年，Eric Evans 发布《Domain-Driven Design: Tackling Complexity in the Heart of Software》（领域驱动设计：软件核心复杂性应对之道），其中定义了DDD。 DDD改变了传统软件开发针对数据库进行的建模方法；DDD先对业务领域进行分析，建立领域模型，根据领域模型驱动代码设计。合理运用面向对象的高内聚低耦合设计要素，降低整个系统的业务复杂性，并使得系统具有更好的扩展性，更好的应对多变的业务需求。 领域 Domain一个领域就是一个问题域，只要是同一个领域，那问题域就相同。只要确定了系统所属的领域，那么这个系统的核心业务，即要解决的问题以及问题的边界就基本确定了。 举例陌生人社交领域，包含有聊天，用户推荐，朋友圈等核心环节。只要是这个领域，一般都会有这些相同的核心业务，因为他们要解决问题的本质是一样的，就是交友。 每一个领域，都有一个对应的领域模型，领域模型能够很好的帮我们解决复杂的业务问题。 驱动设计在DDD中，以领域(domain)为边界，分析领域的核心问题，再设计对应的领域模型，最后通过领域模型驱动代码设计的实现。这样设计的系统才有合理的分层与解耦，对以后业务的迭代开发，代码的维护才更加容易。 然而很多互联网公司，为了追求快速的上线，都是模型都没有想清楚就开始写代码，这就导致了后续代码维护困难，无法扩展。修改bug同时又引入新的bug，反反复复，进入恶性循环。当然，这跟梳理清楚领域模型需要一定时间，这与初创型的互联网公司需求快速上线有点相悖，但是，这点时间的投入是非常值得的。因为可以避免了系统上线后不久又得重构的问题。 概念总结 领域就是问题域 模型驱动的思想：通过建立领域模型来解决领域中的核心问题 领域建模的目标：针对我们在领域中核心问题，而不是整个领域中的所有问题 领域模型设计：设计时应考虑一定的抽象性、通用性，以及复用价值 代码实现：通过领域模型驱动代码的实现，确保代码让领域模型落地，代码最终能解决问题 为什么需要DDD系统复杂性耦合随着产品不断的迭代，业务逻辑变得越来越复杂，系统也越来越庞大。模块彼此互相关联、耦合。导致增加或修改一个功能变得异常艰难，同时功能间的界限也变得模糊，职责不再清晰。这个时候就需要进行重构，拆分。 虽然架构本身是随着业务进行不断演进的；但是，如果架构初始设计不体现出业务的模型，那么新需求就无法体现在现有架构上，导致不断腐化，不断重构。 内聚贫血模型 Anemic Domain Object domain object仅用作数据载体，而没有行为和动作的领域对象。 指领域对象里只有get和set方法，没有相关领域对象的业务逻辑。业务逻辑放在业务层。 充血模型 Rich Domain Object 将业务逻辑和对象存储放在domain object里面，业务层只是简单进行小部分业务的封装及其他domain的编排。面向对象设计，符合单一职责设计。 贫血 vs 充血 贫血模型的domain object很轻量，这导致业务层的复杂，domain object相关的业务逻辑散布在各个业务层，造成业务逻辑的冗余以及原本domain object的定义就变得相对模糊，这就是贫血症引起的失忆症。 而采用领域开发的方式，将数据和行为封装在一起，与业务对象相映射；领域对象职责清晰，将相关业务聚合到领域对象内部。 微服务DDD 的本质是一种软件设计方法论，而微服务架构是具体的实现方式。微服务架构并没有定义对复杂系统进行分解的具体方法论，而 DDD 正好就是解决方案。 微服务架构强调从业务维度来分治系统的复杂度，而DDD也是同样的着重业务视角。 DDD能带来什么 建立通用语言： 围绕领域模型建立的一种语言，团队所有成员都使用这种语言进行沟通和活动 驱动代码设计：领域建立模型，模型指导设计，设计产出代码 解决核心问题：模型的设计中心就是核心域，就是解决核心的问题 DDD建模战略设计战略设计就是从宏观角度对领域进行建模。划分出业务的边界，组织架构，系统架构。 DDD中，对系统的划分是基于领域的，也是基于业务的。 通用语言Ubiquitous Language通用语言是指确定统一的领域术语，提高开发人员与领域专家之间的沟通效率。 一旦确定了统一语言，无论是与领域专家的讨论，还是最终的实现代码，都可以通过使用相同的术语，清晰准确地定义领域知识。 当确认整个团队统一的语言后，就可以开始进行领域建模。 领域和子域领域Domain一个领域本质上可以理解为就是一个问题域。只要我们确定了系统所属的领域，那这个系统的核心业务，即要解决的关键问题、问题的范围边界就基本确定了。 举例 社交领域：关键问题是用户推荐，聊天 电商领域：关键问题是购物，订单，物流 子域Subdomain 如果一个领域过于复杂，涉及到的领域概念、业务规则、交互流程太多，导致没办法直接针对这个大的领域进行领域建模。这时就需要将领域进行拆分，本质上就是把大问题拆分为小问题，把一个大的领域划分为了多个小的领域（子域），那最关键的就是要理清每个子域的边界 子域可以根据自身重要性和功能属性划分为三类子域： 核心域：公司核心产品和业务的领域 支撑子域：不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域 通用子域：被多个子域使用的通用功能子域 每个领域的划分都不一样。对相同领域公司而言，其核心，支撑，通用的子域也可能有不一样的地方，但大体上基本都是一样的。 举例社交领域的划分 核心域：用户推荐，聊天 支撑子域：客服，反垃圾 通用子域：消息推送 限界上下文Bounded Context限界上下文 限界指划分边界，上下文对应一个聚合，限界上下文可以理解为业务的边界。一个子域对应一个或多个限界上下文。如果对应多个上下文，则可以考虑子域是否要再进行细粒度的拆分。 限界上下文的目的是为了更加明确领域模型的职责和范围 划分限界上下文 三个原则： 概念相同，含义不同(通用语言)：如果一个模型在一个上下文里面有歧义，那有歧义的地方就是边界所在，应该把它们拆到不同的限界上下文中。 外部系统：有时候系统需要同外部系统交互，这时可以把与外部系统交互的那部分拆分出去以实现更好的扩展性。这样一旦外部系统发生了变化，就不会影响到我们的核心业务逻辑。 组织扩展：尽量不要两个团队一起在一个限界上下文里面开发，因为这样可能会存在沟通不顺畅、集成困难等问题。 组织架构 康威定律任何组织在设计一套系统时，所交付的设计方案在结构上都与该组织的沟通结构保持一致。 团队结构就是组织结构，限界上下文就是系统的业务结构。所以，团队结构应该尽量和限界上下文保持一致。 举例社交领域中，订单子域对应订单上下文 上下文映射 从宏观上看每个上下文之间的关系，可以更好理解各个上下文之间的依赖关系。 梳理清楚上下文之间的关系是为了： 任务更好拆分，一个开发人员可以全身心的投入到相关的一个单独的上下文中 沟通更加顺畅，一个上下文可以明确自己对其他上下文的依赖关系，从而使得团队内开发直接更好的对接 每个团队在它的上下文中能够更加明确自己领域内的概念，因为上下文是领域的解系统 举例聊天上下文依赖消息推送，推广上下文也依赖消息推送 战术建模战术建模是从微观角度对上下文进行建模。梳理清楚聚合根，实体，值对象，领域服务，领域事件，资源库等。 实体Entity当一个对象可以由标识进行区分时，这种对象称为实体 和数据库中的实体是不同的，这里的实体是从业务角度进行划分的。 实体： 具有唯一标识 持久化 可变 举例社交中的用户即为实体，可以通过用户唯一的id进行区分。 值对象value object当一个对象用于对事物进行描述而没有唯一标识时，它被称作值对象。在实践中，需要保证值对象创建后就不能被修改，即不允许外部再修改其属性。 例如：年龄，聊天表情符号（ 😛: 吐舌 (U+1F61B)） 习惯了使用数据库的数据建模后，很容易将所有对象看作实体 聚合根Aggregate Root聚合是一组相关对象的集合，作为一个整体被外界访问，聚合根是这个聚合的根节点。 聚合由根实体，值对象和实体组成。(聚合根里面有多少个实体，由领域建模决定) 外部对象需要访问聚合内的实体时，只能通过聚合根进行访问，而不能直接访问 举例一个订单是一个聚合根，订单购买的商品是实体，收货地址是值对象。 领域服务Domain Service领域服务一些既不是实体，也不是值对象的范畴的领域行为或操作，可以放到领域服务中。用来处理业务逻辑，协调领域对象来完成相关业务。 例如，有些业务逻辑不适合放到领域对象中，或实体之间的业务协调，这些业务逻辑都可以放到领域服务中。 特征 与领域相关的操作如执行一个业务操作过程，但它又并不适合放入实体与值对象中 操作是无状态的 对领域对象进行转换，或以多个领域对象作为输入进行计算，结果产生一个值对象 当采用微服务架构风格，一切领域逻辑的对外暴露均需要通过领域服务来进行。如原本由聚合根暴露的业务逻辑也需要依托于领域服务。 举例必须通过订单领域服务来创建和访问订单 领域事件领域事件是对领域内发生的活动进行的建模。捕获一些有价值的领域活动事件。 作用 解耦：可以通过发布订阅模式，发布领域事件 一致性：通过领域事件来达到最终一致性 事件溯源 举例发送聊天消息，这属于一个领域事件；撤回消息，也属于一个领域事件。推送服务订阅消息事件，然后将消息推送给用户端。这样就解耦了消息服务与推送服务之间的强依赖关系。 资源库Repository资源库用于保存和获取聚合对象。 领域模型 vs 数据模型资源库介于领域模型(业务模型)和数据模型(数据库)之间，主要用于聚合对象的持久化和检索。 资源库隔离了领域模型和数据模型，以便上层只需要关注于领域模型而不需要考虑如何进行持久化。 分层架构把一系列相同的对象进行分类放在同一层，然后根据他们之间的依赖关系再确定上下层次关系。 在实际决策时，我们需要知道各层的职责、意义以及相应的场景；落实到代码层面时，我们还需要知道各层所包含的具体内容、各层的一些常见的具体策略/模式、层次之间的交互/依赖关系。 DDD经典分层架构 用户接口层（interfaces）：处理显示和用户请求，以及一些基本的参数检查，不包括业务逻辑 应用层（application）：主要协调领域对象的操作；处理持久化事务、发送消息、安全认证等 领域层（domain）：处理核心业务逻辑，不包括技术实现细节。领域层是业务软件的核心 基础设施层（infrastructure）：处理纯技术细节，为其他层提供技术支撑，也可用于封装调用的外部系统细节。例如：持久化的实现，消息中间件的实现，工具类，rpc等 个人理解：这种分层，既可以在一个单体应用中，也可以是微服务的形式。DDD分层并不一定要按微服务的服务粒度进行分层。如果一个业务逻辑非常简单的子域，则可以将几层都放进一个单体应用中，在应用中进行分层。如果业务较为复杂，则可以按服务进行拆分，每层都有自己对应的服务。 其他架构 对称性架构 洋葱架构 整洁架构 CQRS架构 DDD工程实践以一个简化的社交领域的例子来实践DDD。 核心概念 用户（User）： 一个账户，并以用户id识别 关系（Relationship）：用户之间的关系 动态（Feed）： 用户发布文字，图片，视频，评论等内容 会话（Conversation）：用户之间的聊天会话 领域设计战略建模领域就是社交领域，核心问题和绝大部分社交系统一样。 子域 核心域：聊天，动态 支撑子域：反作弊，推广 通用子域：用户，关系，消息推送 上下文 消息上下文 会话上下文 动态上下文 推送上下文 用户上下文 战术建模以会话上下文为例子来进行战术建模 会话上下文 会话：聚合根 用户：实体 用户：实体 消息列表：实体 发送人：实体 接收人：实体 消息内容：值对象 消息在会话上下文属于实体，在消息上下文属于聚合根。 结构以会话子域为例 架构分层 interfaces 接口层 RESTful RPC application 应用层 Conversation Message domain_service 领域服务层 model Conversation Message repository Conversation Message infrastructure 基础设施层 store ConversationRepository MessageRepository message SendMessage utils 领域 1234567891011121314151617package domain// 聚合根type Conversation struct &#123; ID int User1 User User2 User Messages list.List&#125;// 实体type Message struct &#123; ID int From User // 实体 To User Body Content // 值对象&#125; 用户接口层 1234567891011121314151617181920212223242526type ChatInferface struct &#123; // 应用层 app app.ChatApplication&#125;func (c *ChatInferface) Route() &#123; c.route(&quot;POST&quot;, &quot;/api/message&quot;, c.SendMessage) c.route(&quot;PATCH&quot;, &quot;/api/message&quot;, c.RecallMessage)&#125;// POST /api/messagefunc (c *ChatInferface) SendMessage(ctx *Context) &#123; if !c.validateRequest(ctx) &#123; return &#125; message := c.parseMessage(ctx) app.SendMessage(message)&#125;func (c *ChatInferface) RecallMessage(ctx *Context) &#123; if !c.validateRequest(ctx) &#123; return &#125; messageID := c.parseMessage(ctx) app.RecallMessage(messageID)&#125; 应用层 1234567891011121314type ChatApplication struct &#123; user service.UserService chat service.ChatService // 这里领域事件由应用层发布 // publisher EventPublisher lbs LBSFacade&#125;func (c *ChatApplication) SendMessage(msg *Message) &#123; if !c.user.CheckUser(msg.UserID) &#123; return &#125; c.chat.SendMessage(msg)&#125; 领域服务层 12345678910111213141516type ChatService struct &#123; // 领域事件 publisher MessageEventPublisher repo MessageRepository&#125;func (c *ChatService SendMessage(msg *Message) &#123; // 业务逻辑 ... // 领域资源持久化 c.repo.Save(msg) // 发布领域事件 c.publisher.Publish(msg)&#125; 基础设施层 123456789101112131415161718package infrastructuretype MessageRepository struct &#123; db MessageDatabase cache MessageCache&#125;func (m *MessageRepository) Save(msg *Message) &#123; db.Save(m.ToPO(msg))&#125;func (m MessageRepository) Get(msgID int) *Message &#123; msg := m.cache.Get(msgID) if msg != nil &#123; return m.FromPO(msg) &#125; return m.FromPO(m.db.Get(msgID))&#125; 总结在设计和实现一个系统的时候，这个系统所要处理问题的领域专家和开发人员以一套统一语言进行协作，共同完成该领域模型的构建，在这个过程中，业务架构和系统架构等问题都得到了解决，之后将领域模型中关于系统架构的主体映射为实现代码，完成系统的实现落地。]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>DDD</tag>
        <tag>领域驱动设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grpc]]></title>
    <url>%2F2018%2F06%2F02%2Fgrpc%2F</url>
    <content type="text"><![CDATA[grpc 特性、原理、实践、生态 gRPC概述gRPC是一个由google设计开发基于HTTP/2协议和Protobuf序列化协议的的高性能、多语言、通用的开源 RPC 框架。 跨语言、跨平台插件化 ： 负载均衡，tracing，健康检查，认证等等编码压缩 ： 节省带宽多路复用 ： 降低的 TCP 链接次数 使用场景 低延迟、高扩展的分布式系统 与云服务通信 设计一个需要准确，高效且与语言无关的新协议 分层设计，以实现扩展，例如：身份验证，负载平衡，日志记录和监控等 特性基于HTTP/2 HTTP/2 提供了 链接多路复用、双向流、服务器推送、请求优先级、首部压缩等机制。gRPC 协议使用了HTTP2 现有的语义，请求和响应的数据使用HTTP Body 发送，其他的控制信息则用Header 表示。 IDL使用ProtoBuffer gRPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议（类似于XML、JSON）。ProtoBuf能够将数据进行序列化，并广泛应用在数据存储、通信协议等方面。压缩和传输效率高，向后兼容，语法简单，表达力强。 多语言支持 gRPC支持多种语言，并能够基于语言自动生成客户端和服务端。 目前支持： C#, C++, Dart, Go, Java, Node, Objective-C, PHP, Python, Ruby 等。 详见官网 HTTP/2HTTP/2HTTP/1.x 是超文本传输协议第1版，可读性好，但效率不高。而HTTP/2 是超文本传输协议第2版，是一个二进制协议。 HTTP/1 和 HTTP/2 的基本语义并没有改变，如方法语义（GET/PUST/PUT/DELETE），状态码（200/404/500等），Range Request，Cacheing，Authentication、URL路径。 HTTP/2通用术语： Stream： 流，一个双向流，一条连接可以有多个 streams。 Message： 逻辑上面的 request，response。 Frame：帧，HTTP/2 数据传输的最小单位。每个 Frame 都属于一个特定的 stream。一个 message 可能由多个 frame 组成。 HTTP/2 流、帧 HTTP/2连接上传输的每个帧(frame)都关联到一个流，一个连接上可以同时有多个流，同一个流的帧按序传输，不同流的帧交错混合传输，客户端、服务端双方都可以建立流，流也可以被任意一方关闭。客户端发起的流使用奇数流ID，服务端发起的使用偶数。 Frame结构 : 123456789+-----------------------------------------------+| Length (24) |+---------------+---------------+---------------+| Type (8) | Flags (8) |+-+-------------+---------------+-------------------------------+|R| Stream Identifier (31) |+=+=============================================================+| Frame Payload (0...) ...+---------------------------------------------------------------+ Length ： 也就是 Frame 的长度 Type ：Frame 的类型，有 DATA，HEADERS，SETTINGS 等 Flags ：帧标志位，8个比特表示可以容纳8个不同的标志：stream是否结束(END_STREAM)，header是否结束(END_HEADERS)，priority等等 R：保留位 Stream Identifier：标识frame所属的 stream，如果为 0，则表示这个 frame 属于整条连接(如SETTINGS帧) Frame Payload：帧内容 帧类型 HEADERS 类似于HTTP/1的 Headers DATA 类似于HTTP/1的 Body CONTINUATION 头部太大，分多个帧传输（一个HEADERS+若干CONTINUATION） SETTINGS 连接设置 WINDOW_UPDATE 流量控制 PUSH_PROMISE 服务端推送 PRIORITY 流优先级更改 PING 心跳或计算RTT RST_STREAM 马上中止一个流 GOAWAY 关闭连接并且发送错误信息 HTTP/2 特性新的二进制格式（Binary Format） HTTP/1 的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同。基于这种考虑HTTP/2的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing） HTTP/1 的request是阻塞的，如果想并发发送多个request，必须使用多个 TCP connection。这样会消耗更多资源，且浏览器为了控制资源，会对单个域名有TCP connection请求限制。 HTTP/2 一个TCP connection可以有多个streams(最大数量由参数SETTINGS_MAX_CONCURRENT_STREAMS控制)， 多个streams 并行发送不同的请求的frames。 可以在SETTINGS帧中设置SETTINGS_MAX_CONCURRENT_STREAMS。而此值是针对一端而言的，客户端可以告知服务器最大的streams并发数，服务端也可以告知客户端。 如果一条链接上 ID 分配完了， server 则会给 client 发送一个 GOAWAY frame 强制让 client 新建一条连接。 header压缩 HTTP/1 是使用文本协议，而且header每次都要重复发送，浪费了带宽也导致资源加载过慢。 HTTP/2 采取了压缩和缓存来避免重复发送和带宽问题： 对消息头采用HPACK 进行压缩传输来节省消息头占用的网络的流量。 对这些headers采取了压缩策略来减少重复headers的请求数 HTTP/2在客户端和服务器端使用 headlist 来存储之前发送过的 header，对于相同的header，不再通过每次请求和响应发送； HPACK: Header Compression for HTTP/2 服务端推送 server push功能 : 在无需客户端请求资源的情况下，服务端会直接推送客户端可能需要的资源到客户端。 当服务器想用Server Push推送资源时，会先向客户端发送PUSH_PROMISE帧。推送的响应必须与客户端的某个请求相关联，因此服务器会在客户端请求的流上发送PUSH_PROMISE帧。 优先级排序 设置优先级的目的是为了告诉对端在并发的多个流之间如何分配资源的行为，同时当发送容量有限时，可以使用优先级来选择用于发送帧的流。 客户端可以通过 HEADERS 帧的 PRIORITY 信息指定一个新建立流的优先级，也可以发送 PRIORITY 帧调整流优先级。 参考官网 Flow Control HTTP/2 支持流控，receiver 端可以对某些stream进行流控也可以针对整个connection流控。而TCP层只能针对整个connection进行流控。 特性 ： Flow control 是由方向的 : Receiver 可以选择给 stream 或者整个连接设置接收端的 window size。 Flow control 是基于信任的 : Receiver 只是会给 sender 建议 连接和 stream 的 flow control window size。 Flow control 无法禁止 Flow control 是基于WINDOW_UPDATE帧的 Flow control 是 hop-by-hop的，而不是 end-to-end 的。例如，用nginx做proxy，则flow control作用于nginx到server和client到nginx这两个connection。 Connection 和 stream 的初始 flow-control window 大小都是 65535。Connection 的初始窗口大小不能改变，但 stream 的可以(所有stream)，通过发送 SETTINGS 帧，携带 SETTINGS_INITIAL_WINDOW_SIZE，这个值即为新的 stream flow-control window 初始大小。 增加flow control window size能加快数据传输，但同时会消耗更多资源。 主动重置链接 HTTP/1 的body的length的被送给客户端后，服务端就无法中断请求了，只能断开整个TCP connection，但这样导致的代价就是需要重新通过三次握手建立一个新的TCP连接。 HTTP/2 引入了一个 RST_STREAM frame 来让客户端在已有的连接中发送重置请求，从而中断或者放弃响应。当浏览器进行页面跳转或者用户取消下载时，它可以防止建立新连接，避免浪费所有带宽。 HTTP/2 站点demoHTTP/1 和 HTTP/2 加载速度比较：https://http2.akamai.com/demo 访问http2站点 ：https://http2.golang.org/ ProtoBufProtoBufGoogle Protocol Buffer 是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化。适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。 描述简单，对开发人员友好 跨平台、跨语言，不依赖于具体运行平台和编程语言 高效自动化解析和生成 压缩比例高 可扩展、兼容性好 gRPC与protobuf gRPC使用 protobuf 作为IDL来定义数据结构和服务。 可以定义数据结构，也可以定义rpc 接口。然后用proto编译器生成对应语言的框架代码。 定义数据结构 ： 生成对象的 序列化 代码 定义rpc接口 ： 生成 gRPC服务端、客户端响应的代码 protobuf 基本数据类型https://developers.google.com/protocol-buffers/docs/proto#scalar 数据结构定义user.proto 1234567891011121314151617181920212223242526272829syntax = &quot;proto3&quot;;import &quot;google/protobuf/any.proto&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; google.protobuf.Any details = 11;&#125; package package声明符，用来防止不同的消息类型有命名冲突。生成的代码将会包含再package(go等语言)或者命名空间(c++, java等)中。 option go_package = &quot;protos_golang/user&quot;;$LANGUAGE_package 是指定生成的代码的import path和package。 import 要导入其他.proto文件的定义，在文件中添加一个导入声明。使用导入proto的类型 package名字.结构名 来使用导入proto的类型。如上面common.Flag 分配字段编号 每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的。为了保证向后兼容，一旦开始使用就不要再改变。 文件版本申明 syntax = &quot;proto2&quot;; 指定使用proto2语法syntax = &quot;proto3&quot;; 指定为proto3语法 标识符修饰符 required 和 optional 是proto2的语法，proto3已经不支持。proto3中所有的字段都是optional的。具体原因见 required : 必须字段。 optional ：可选字段。 repeated ：数组类型字段。 reserved ：保留字段。指出这些字段编号已经删除，不要再重用这些编号了。因为如果这些编号被重新定义成其他类型，那么对于旧版本的protobuf数据，会导致解码错误。 枚举 与数据结构中 enum 类似。字段编号从0开始。 对于protobuf兼容性问题，第一个枚举值应该考虑用unknown这种字段：因为如果在.proto枚举中增加了一个值，而protobuf解析的一方如果没有升级新版本的.proto，则无法解析出此枚举值，而直接使用第一个枚举值，为了避免这种情况，所以最好一般将第一个枚举值设置为unknown。 oneof oneof与数据结构联合体(UNION)类似，一次最多只有一个字段有效。 map map 类型则可以用来表示键值对。key_type 可以是任何 int 或者 string 类型，float、double 和 bytes除外 any Any类型包括: bytes : 被序列化为bytes类型的任意消息 URL : 全局标识符 使用import google/protobuf/any.proto来导入any类型 any可以用来替换proto2中的extension 嵌套类型 可以在消息类型中定义其他消息类型 服务定义12345678910111213141516syntax = &quot;proto2&quot;;import &quot;user.proto&quot;;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; user.User user = 1;&#125; 如果在 .proto 文件中定义了 RPC 服务接口， 编译器将使用生成服务接口代码和 stubs。 import &quot;user.proto&quot;; 导入user结构定义的proto文件。 插件protoc编译器通过插件机制实现对不同语言的支持。protoc会先查找是否有内置的语言插件，如果没有，则会去查找系统中是否存在protoc-gen-$LANGUAGE 的插件。例如：如果指定--go_out参数，那么protoc会查询是否有内置的go插件，如果没有则继续查询系统中是否存在protoc-gen-go的可执行程序，再通过插件来生成相关的语言代码。 插件运行流程： protoc 启动 protoc-gen-xx 将CodeGeneratorRequest的protobuf二进制传入到 protoc-gen-xx的标准输入 protoc-gen-xx读取标准输入再反序列化成CodeGeneratorRequest 遍历CodeGeneratorRequest的FileDescriptorProto数组，其描述了proto文件的语法树 将FileDescriptorProto编译成语言源码 生成CodeGeneratorResponse对象输出到标准输出 protoc根据protoc-gen-xx的标准输出再生成源码文件 plugin.proto定义了CodeGeneratorRequest 和 CodeGeneratorResponse，是protoc与插件交互的对象。 descriptor.proto描述的是一个.proto文件的语法树 插件的plugins 插件本身的也是支持以内部plugins形式进行扩展的。 例如：生成go grpc的命令中： 1protoc --go_out=plugins=grpc:. pb/user.proto grpc就是 proto-gen-go的plugin。 代码 Name()返回grpc命名就是plugin的名字，就是上面plugins=grpc 1234// Name returns the name of this plugin, &quot;grpc&quot;.func (g *grpc) Name() string &#123; return &quot;grpc&quot;&#125; gRPC 原理概念 gRPC 定义服务，服务包含远程调用的方法。在服务器端，服务器实现rpc接口并运行一个gRPC服务器来处理客户端请求。在客户端，客户端有一个”存根stub”，提供与服务器相同签名的方法，来处理客户端请求的编码、解码等，再将请求转发到服务器端，这样客户端调用rpc方法就像调用本地函数一样。 实现gRPC把HTTP2的steam identifier当作请求ID，每一次请求都发起一个新的stream。 请求的方法、响应的状态码等都放在HEADER frame中。而请求内容和响应内容由protobuf序列化后使用DATA frame中。 请求Request主要由 Request-Headers 和 Data 以及 EOS (END_STREAM)组成。 如下图： Request-Headers Request-Headers 由 HEADERS 和 CONTINUATION frames 组成。如果Flags有设置标志位END_HEADERS则代表Request-Headers结束。 Request-Headers 主要有 Call-Definition 以及 Custom-Metadata : Call-Definition : 包括 Method, Scheme, Path, TE, Authority, Timeout, Content-Type ,Message-Type, Message-Encoding, Message-Accept-Encoding, User-Agent Custom-Metadata : 应用层自定义的任意 key-value，key 不要使用gRPC保留的key前缀字符 grpc- 。 Data 请求体，由一个或多个 Data frame组成。如果Flags有设置标志位END_STREAM则代表Data结束，请求结束。 request格式大致如下 1234567891011121314151617# request-headers HEADERS (flags = END_HEADERS):method = POST:scheme = http:path = /user.UserService/GetUserInfo:authority = localhost:50000grpc-timeout = 999127ucontent-type = grpc-go/1.20.0-dev## 自定义metadataservice : test_clienttraceid : xxxx# dataDATA (flags = END_STREAM)&lt;Length-Prefixed Message&gt; 响应Response 主要由 Response-Headers 和 Data 以及 Trailers 组成。如果遇到了错误，也可以直接返回 Trailers-Only。 如下图： Response-Headers Response-Headers 包含 : HTTP-Status, Message-Encoding, Message-Accept-Encoding, Content-Type, Custom-Metadata等。 Data 响应体，由一个或多个 Data frame组成。如果Flags有设置标志位END_STREAM则代表Data结束。 Trailers Trailers-Only 包含 HTTP-Status, Content-Type, Trailers等。 Trailers 包含 Status, Status-Message, Custom-Metadata等。 Trailers作用主要是给响应包含一些额外的动态生成的信息。如：消息body发送后，再发送一些信息 如数字签名，后处理状态等 格式大致如下： 123456789101112131415161718192021# response-headersHEADERS (flags = END_HEADERS):status = status: 200 content-type = application/grpc## 自定义metadataservice: server_testspanid: xxxx# dataDATA&lt;Length-Prefixed Message&gt;# headersHEADERS (flags = END_STREAM, END_HEADERS)grpc-status: 0## trailers 自定义metadatatimestamp: 1560656283730441829 Status code HTTP状态码对应的gRPC状态码 gRPC通信方式gRPC有四种通信方式: 1、 unary RPC 一般的rpc调用，客户端发送一个请求对象，然后等待服务端返回一个响应对象 123# 获取用户信息# protorpc GetUserInfo (UserRequest) returns (UserResponse) &#123;&#125; 2、 Server-side streaming RPC 服务端流式rpc 客户端发起一个请求到服务端，服务端返回一段连续的数据流响应。 123# 获取一个用户的所有地理位置历史记录# protorpc UserLocationsStream(UserRequest) returns (stream LocationsResponse) &#123;&#125; 3、 Client-side streaming RPC 客户端流式rpc 客户端将一段连续的数据流发送到服务端，服务端返回一个响应。 123# 客户端将所有数据备份到服务端# protorpc BackupStream(stream BackupRequest) returns (BackupResponse) &#123;&#125; 4、 Bidirectional streaming RPC 双向流式rpc 客户端将连续的数据流发送到服务端，服务端返回交互的数据流。 123# 在线聊天# protorpc LiveChat(stream Message) returns (stream Message) &#123;&#125; 配置waitForReady 发送请求时，如果connection没有ready，则会一直等待connection ready 或直到超时(达到deadline)。也常称为fail fast。 timeout 请求超时时间。如果超时，则会中止请求且返回DEADLINE_EXCEEDED 错误。 maxRequestMessageBytes 请求体的最大payload size(没有压缩的)。如果客户端请求大于此值的请求会返回RESOURCE_EXHAUSTED错误。 maxResponseMessageBytes 响应体的最大payload size(没有压缩的)。如果服务端响应大于此值，响应将发送失败。且客户端会得到RESOURCE_EXHAUSTED错误。 gRPC 实践实践部分以go语言进行demo 环境安装protoc mac 1brew install protobuf linux 123456PROTOC_ZIP=protoc-3.5.1-linux-x86_64.zipcurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/$PROTOC_ZIPsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protocsudo unzip -o $PROTOC_ZIP -d /usr/local include/*rm -f $PROTOC_ZIP golang的protobuffers插件 1go get -u github.com/golang/protobuf/&#123;protoc-gen-go,proto&#125; Coding定义proto文件123456789101112131415161718192021222324252627282930313233343536373839404142syntax = &quot;proto3&quot;;import &quot;google/protobuf/any.proto&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; google.protobuf.Any details = 11;&#125;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; User user = 1;&#125; 生成代码生成代码的导入路径和包名 123## protos_golang ： 生成代码的路径## user : golang package 名option go_package = &quot;protos_golang/user&quot;; 目标代码 如果包含rpc接口：则需要指定插件plugins=grpc --go_out=. ： 生成的代码在当前目录; 也可以指定其他目录，如:--go_out=/tmp 代码路径 ： 如果.pb中指定了go_package : 代码路径是 ./$go_package/user.pb.go 如果.pb中没有指定go_package : 则代码路径是 ./pb/user.pb.go 1234protoc --go_out=plugins=grpc:. pb/user.proto# 如果没有rpc定义protoc --go_out=. pb/user.proto 服务端1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mainimport ( &quot;context&quot; &quot;log&quot; &quot;net&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( port = &quot;:50000&quot;)// grpc servertype server struct&#123;&#125;// 实现gRPC接口func (s *server) GetUserInfo(ctx context.Context, in *pb.UserRequest) (*pb.UserResponse, error) &#123; return &amp;pb.UserResponse&#123; User: &amp;pb.User&#123; Name: &quot;test_user&quot;, &#125;, &#125;, nil&#125;// 拦截器，简单打印下日志func LogUnaryInterceptorMiddleware() grpc.UnaryServerInterceptor &#123; return func(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (r interface&#123;&#125;, err error) &#123; r, err = handler(ctx, req) fmt.Printf(&quot;fullMethod(%s), errCode(%v)\n&quot;, info.FullMethod, err) return r, err &#125;&#125;func main() &#123; lis, err := net.Listen(&quot;tcp&quot;, port) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; // 拦截器 options := grpc.UnaryInterceptor(LogUnaryInterceptorMiddleware()) s := grpc.NewServer(options) // 注册服务器实现 pb.RegisterUserServiceServer(s, &amp;server&#123;&#125;) // 注册服务端反射 reflection.Register(s) // 启动服务器 if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 客户端123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;google.golang.org/grpc&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( address = &quot;localhost:50000&quot;)func main() &#123; conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := pb.NewUserServiceClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.GetUserInfo(ctx, &amp;pb.UserRequest&#123;&#125;) if err != nil &#123; log.Fatalf(&quot;fatal: %v&quot;, err) &#125; log.Printf(&quot;response: %s&quot;, r)&#125; 调试为了方便调试服务端，所以服务端需要支持reflection功能。 1reflection.Register(grpcServer) 两款比较著名的调试工具： [grpc_cli](https://github.com/grpc/grpc/blob/master/doc/command_line_tool.md : 官方的 grpcurl : go的，安装简单 列出服务端注册的service 如果没有配置好公钥和私钥文件，也没有忽略证书的验证过程，则需要加-plaintext 123$ grpcurl -plaintext localhost:50000 list grpc.reflection.v1alpha.ServerReflectionuser.UserService 列出服务的接口 12$ grpcurl -plaintext localhost:50000 list user.UserServiceuser.UserService.GetUserInfo 获取接口的签名 123$ grpcurl -plaintext localhost:50000 describe user.UserService.GetUserInfouser.UserService.GetUserInfo is a method:rpc GetUserInfo ( .user.UserRequest ) returns ( .user.UserResponse ); 获取类型信息 12345$ grpcurl -plaintext localhost:50000 describe .user.UserRequestuser.UserRequest is a message:message UserRequest &#123; uint32 id = 1;&#125; 调试接口 请求体以json的形式描述类型。 123456$ grpcurl -plaintext -d &apos;&#123;&quot;id&quot;:1&#125;&apos; localhost:50000 user.UserService.GetUserInfo&#123; &quot;user&quot;: &#123; &quot;name&quot;: &quot;test_user&quot; &#125;&#125; go gRPC 生态服务组件上下文信息传递 rpc客户端将上下文信息传递给服务端。链路调用信息，服务信息，认证信息等等。 官方实现 服务器反射 服务端反射协议， 可以用途于: 服务端调试 : grpcurl 工具就是用reflection协议来进行服务端调试的。可以list出服务端的接口定义，以及命令行构造请求进行调试。 运行时构造gRPC请求 ：客户端可以运行时根据反射的接口定义构造请求。 官方实现 负载均衡 客户端负载均衡器 官方实现 认证 gRPC主要的两种认证方式： 基于SSL/TLS认证方式 Token认证方式 两种方式可以同时应用 官方实现 实现了几种认证方式： alts google oauth 自定义认证方式 go-grpc-middleware的实现 健康检查 服务端提供一个Check接口返回其状态信息。客户端调用此接口获取到服务健康状态，是否可以继续提供服务。 官方实现 keepalive 定期发送HTTP/2.0 pings帧来检测 connection 是否存活，如果断开则进行重新连接。与健康检查区别在于keepalive是检查connection而健康检查是检查服务是否可用。 官方实现 naming 命名解析。通过服务命名来获取服务相关的信息来达到服务发现目的。 与balancer结合使用来实现进程内负载均衡与服务发现。 官方实现 限流 限制流量来保护服务端以防止服务过载。 可以在客户端，balancer，服务端 进行限流。 go-grpc-middleware实现服务端限流 recovery 将服务内部的错误转换成gRPC错误码。 go-grpc-middleware实现 ： recover go的panic， 并转换成gRPC错误。 重试 客户端对于返回某些gRPC错误码的请求进行重试。 go-grpc-middleware tracing 在链路上下文携带tracing信息，以及将信息以opentracing的规范发送给分布式链路分析服务。 tracing信息包含traceid,spanid,请求时间,错误信息,日志等等。如：通过设置客户端spanid为服务端spanid的parent_spanid，这样就能知道是客户端调用了服务端rpc请求。 go-grpc-middleware实现opentracing的middleware open-tracing 微服务框架、组件go-kit : 微服务组件micro : 微服务框架go-chassis : 华为开发的go微服务框架go-grpc-middleware : 服务端和客户端的一些中间件，认证、日志、分布式追踪跟重试等grpc-gateway ：一个 protoc 的插件，可以将 gRPC 接口转换为对外暴露 RESTful API 的工具，同时还能生成 swagger 文档 gRPC 与 负载均衡进程内LB(Balancing-aware Client) 需要实现： 服务注册 健康检查 服务发现 负载均衡 缺点： 开发成本：要实现上述功能 维护成本：不同语言栈的sdk维护与升级 官方已经提供接口来实现进程内的负载均衡。同时结合服务发现，健康检查一起使用。 集中式LB(Proxy Model) proxy 实现服务发现，健康检查，负载均衡等等。还方便做限流等控制和其他统一控制策略。 缺点： 单点问题 多一层性能开销 不方便调试 Nginx Nginx(1.13.10已经支持gRPC) 1234567891011121314151617181920212223upstream grpcservers &#123; server localhost:50000; server localhost:50001;&#125;server &#123; listen 9000 http2; # router location /user.UserService &#123; grpc_pass grpc://grpcservers; error_page 502 = /error502grpc; &#125; # 将默认错误页面更改成gRPC状态码 location = /error502grpc &#123; internal; default_type application/grpc; add_header grpc-status 14; add_header grpc-message &quot;unavailable&quot;; return 204; &#125;&#125; nginx gRPC module 独立LB进程(External Load Balancing Service) 在主机上部署独立的LB进程，来实现服务发现，健康检查，负载均衡等功能。不用对于不同语言维护不同sdk版本；常常用于微服务service mesh。 缺点： 单点问题：但是只影响本机 不方便调试 常用的组件： Istio Envoy gRPC 生态环境组件 grpc 只是实现了 RPC 核心功能，缺少很多微服务的特性（服务注册发现、监控、治理、管理等），而基于 HTTP/2 相对来说比较容易进行扩展。 grpc-ecosystem 上有一些比较优秀的外围组件来完善gRPC的生态体系 awesome-grpc 收集了一些优秀的gRPC项目 grpc 文档与交流文档 官网文档 : https://grpc.io/docs/ github 上 grpc 仓库下的 doc ： https://github.com/grpc/grpc/tree/master/doc 博客 : https://grpc.io/blog/ 交流 https://grpc.io/community/ 交流的方式有： 邮件列表 Gitter Reddit Meetup Group 参考grpc.io developers.google.com gRPC github doc http2 specsorgithub http2 spec]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用之限流]]></title>
    <url>%2F2018%2F04%2F18%2Frate_limit%2F</url>
    <content type="text"><![CDATA[限流的概念，算法，分布式限流以及微服务架构下限流的难点 限流概念目的 通过对并发/请求进行限速来保护系统，防止系统过载。 做到有损服务，而不是不服务。 负载过高时，优先保护核心服务或业务 限流方式限流的方式有很多： QPS：限制每秒的请求数 并发数：避免开启过多线程导致资源耗尽 连接数：限制TCP连接数 以下指的限流都是指QPS的限制，不涉及并发数和连接数。 限流规则限流策略是根据业务特征来设置的，可以设置静态策略，也可以动态变化的。 规则包含有： 限流算法 参数，限流阈值 相应策略等(处理方式等) 静态规则限流阈值服务端提前做好整个服务端链路的性能压测，先了解业务特征，需要请求API的QPS，估算好整个服务集群的水位及下游水位(下游服务和相关存储等中间件)。服务端集群的限流阈值建立在压测的数据基础上。 压测需要得到的相关指标 服务：对于计算型服务，需要考虑服务本身计算带来的系统资源消耗 缓存及数据库：需要访问存储的服务，需要考虑数据库的负载 消息队列：虽然一般消息队列可以用来作为削峰用途，但是有的消息队列是作为其他业务事务等用途的，也需要考虑其负载情况 其他：下游链路中涉及到的各种服务或者中间件，都应该考虑到其负载 需要考虑： 不同API的不同阈值 整个链路涉及到的服务阈值：如果某个请求链路需要请求10个服务，则阈值计算需要考虑这10个服务能够提供给此服务的流量配额 不同的限流策略有不同的限流阈值 理论上每次API实现的业务逻辑的改变，以及增加、减少API都要进行重新压测，相对比较麻烦 阈值的估算应该考虑整个服务的安全水位，阈值定义过高则容易产生过载，过低则浪费资源 动态规则动态规则的两种实现： 自适应：服务根据RT，负载，QPS等等指标动态变更限流规则 外部通知：通过调用服务API来通知服务更新或服务自己轮询规则服务器 限流阈值根据响应时间或在监控系统以及服务治理中心之上根据服务及下游的负载来动态计算阈值 服务端限流 vs 客户端限流 vs 网关限流绝大多数情况下限流都是发生在服务端的，因为很多情况下客户端的数量是不确定的。但有时候为了防止单个客户端过度使用服务，那么此处可以在客户端来完成，当然在服务端也可以同时进行。 一般都推荐在服务端做限流 服务端限流服务在处理请求前，应该对请求进行限流计算，防止系统过载。同时也要考虑到为不同的业务的客户端提供不同的限流策略，不能因为某个业务的问题达到达到限流阈值而造成其他业务无法请求服务。 服务端限流优点 更好控制整个服务的负载情况：服务端的限流阈值不会因为客户端数量增加或减少而改变 方便对不同上游服务进行不同阈值的限流策略：可以对不同的调用者进行不同的限流配额，也可以给不同业务打上不同的tag再根据tag来限流。 缺点 如果服务端只针对QPS限流，而不考虑连接数：服务在建连过程中也会产生一些资源消耗，而这些压力往往可能会成为瓶颈。特别是短连接，不断的建链过程会产生大量的资源消耗 如果服务端也针对连接数进行限制：则不好对不同链路或服务进行配额区分。容易造成某个业务或服务的连接过多而导致其他服务也被限制 客户端限流客户端调用下游服务时，以每个服务集群的限流配额对下游服务进行过载保护。 优点 达到阈值不会请求服务端，避免服务端产生额外的资源消耗，如建立连接 缺点 客户端的数量的增加或减少需要重新计算每个客户端的限流阈值 客户端限流可能出现bug，或者客户端负载均衡产生倾斜导致限流失效 服务不同API不同限流阈值：下游服务较多，而每个服务的不同API有不同限流配额，则客户端的限流较为复杂 网关限流请求通过网关来请求服务端，在网关中对不同服务及不同的API进行限流。 优点 能很好的保护整个集群的负载压力，服务端数量增加或减少，则网关进行相应的阈值调整即可 对不同的上游业务的服务设置不同的限流配额和不同的限流策略 缺点 需要网关资源 网关本身高可用性 限流粒度限流的粒度可以分为： 服务：对服务所有API进行统一的限流策略 API：每个API会有不同的请求链路，则相应会有不同的限流策略(阈值等) API参数：很多时候我们希望能够对某个热点数据中访问频次最高的 Top K 数据进行限制。例如：秒杀，大促等场景，不要因为某个商品的频繁访问引起的限流导致其他商品无法访问。 服务粒度一个服务提供一个统一的限流的策略。优点是非常简单，但很容易造成限流失效，无法保护服务本身及下游。 如：服务提供两种API，都是访问数据，两种API的查询语句并不一致，API1 查询非常复杂，数据库安全水位只能提供10/s的TPS，而对于API2，数据库可以提供1000/s的TPS，这种情况下，如果按照服务粒度进行限流，则只能提供10/s QPS的限流阈值。所以是非常不合理的。 API粒度不同的API进行不同的限流策略，这种方式相对复杂些，但是更为合理，也能很好的保护服务。要考虑几种情况： 增加或减少API，则限流策略要做相应的调整 API实现的改变：请求处理实现变化则可能需要重新对限流阈值进行调整，避免因为增加一些业务逻辑而导致服务本身或者下游服务过载。 大多数情况下，都应该进行API粒度的限流，这样才能更好的保护服务本身及服务的下游服务和中间件，达到更好的限流效果。 限流的处理方式如果达到了流量限制的阈值，一般处理方式有： 直接返回错误码：可以返回资源耗尽或者busy等状态码，或者带backoff的重试 等待及重试：要注意不要超过请求超时时间 限流算法固定窗口算法Fixed window通过维护一个单位时间内的计数值，每当一个请求通过时，就将计数值加1，当计数值超过阈值时，就进入限流处理流程。如果单位时间已经结束，则将计数器清零，开启下一轮的计数。 这种方式的缺点是：窗口是固定的，会存在两个窗口边界突发流量问题。当然，取决于窗口的大小，如果足够小，则这种问题是可以忽略的 如下图：时间窗口为1s，1s的限制阈值是4，如果一个恶意用户在1s的最后的最后500毫秒发送3个请求，在下一秒的前500毫秒发送3个请求，那么实际在1秒内发送了6个请求，就超过了流量的限制。 滑动窗口算法Sliding window将时间窗口在进行细化，分为N个小窗口，窗口以小窗口为最小滑动单位，这样就可以避免在两个时间窗口之间产生毛刺现象。（当然在小窗口之间仍然会产生毛刺） 令牌桶算法Token Bucket令牌桶算法是网络流量整形（Traffic Shaping）和速率限制（Rate Limiting）中最常使用的一种算法。 有3个阶段 令牌的产生：以r/s的速率将token放入bucket中，如果bucket中token数已到达bucket的容量b，则丢弃token 获取令牌：在请求处理前从bucket中获取(移除)一个令牌，只有获取到令牌才进行处理 无法获取令牌：当bucket中令牌不够，请求进入限流处理流程（阻塞等待或者直接返回失败，等待应该计算好超时时间） 【图来源于dev.to】 token bucket特点 最常见的单机限流算法，开源组件很多，使用也简单 token bucket可以应对短时间的突发流量。例如：bucket容量为10，速率为2/s，而请求QPS为2/s，那么bucket中有8个tokens可以用来应对突发流量。 漏桶算法Leaky Bucket 图左边a：一个桶，不断的倒水进来，桶底下有个洞，按照固定的速率把水漏走，如果水进来的速度比漏走的快，桶可能就会满了，水就溢出了 图右边b：类似于左边a，将请求放入桶中，以固定的速率从桶中拿出请求进行处理，当桶满了，请求将被阻塞或直接拒绝服务 【图来源于dev.to】 leaky bucket特点 不能很好应付突发的流量：例如，桶容量为10，请求处理速率为2/s；请求放入桶的速率为2/s，那么可以容纳8个突发请求放入桶中，其他等待或者丢弃。 桶可以是普通队列形式，也可以是优先队列：优先队列是很有必要的 放入桶中的请求应该计算好其超时时间，如果请求放入桶中到请求被处理，已经超过请求的超时间，则是没有意义的，反而阻塞了其他请求。 和令牌桶区别是：令牌桶是固定速率往桶里放令牌，请求来了取走令牌，桶空了，请求阻塞；漏桶是请求来了往桶里放请求，以固定速率取走请求，桶满了，请求阻塞。令牌桶允许突发流量，而漏桶是不允许的。 动态限流算法前面的算法都是以恒定速率进行限流的(令牌桶以固定速率将令牌放入桶中，固定和滑动窗口限制阈值都是固定的)，这种最大的问题就是每次业务逻辑变更，都要重新进行压力测试以计算出最新的阈值。在TCP拥塞控制算法中，流量发送速率是跟网络环境相关的。那其实服务的限流也可以根据服务处理请求的时延或负载等指标来进行动态调整。 预热期慢启动类似于TCP的慢启动，定义一个启动时的限制阈值，在定义的预热时间周期内逐步提升限制阈值，直到周期结束达到定义的正常值。这非常适合于服务本身或其依赖的存储等需要进行预热的场景。 可以参考 guava 例如：以令牌桶为例，开始时限制阈值为4/s，预热时间为3/s，正常限制阈值为7/s，那么就在3秒内，将限制阈值每秒增加1最终达到7/s的阈值。 根据响应时间这种方式根据请求响应时间来实时调整限流的规则，相对较为合理。请求响应快则平滑调整增加阈值，响应慢则减少阈值，以及定义一个最大安全阈值。 关键在于如何量化快与慢： 根据压测得到服务的安全水位，估算出最大阈值。 启动时设置一个保守的阈值 与前一个时间窗口内响应时间比较，比之前快则可以调高阈值。 那么随着响应时间的变化，阈值也在不断的变化中，阈值的范围在[1, max_value]之间调整。 基于监控系统如果服务是消耗CPU资源的计算型或者消耗IO资源的存储型等，则基于监控系统更为合理。 根据CPU，load，内存，IO等系统指标和请求响应时间等业务指标综合考虑，随着监控指标的变化动态改变限流规则，这种限流相对较为复杂，根据自己业务设计适合自己的计算方式。如果是IO型，则IO指标的权重相对要高一些，如果是计算型，则CPU和Load要权重高一些。 分布式限流单节点限流最大的问题是当服务节点动态添加或减少后，每个服务的限流配额也要跟随动态改变。如果服务节点增加了，而原来节点限流配额没有减少，则下游服务就可能过载。 而分布式限流则避免了这种问题，通过像redis集群或发票服务器这种取号的方式来限制某个资源的流量。 redis限流基于redis的单线程及原子操作特性来实现限流功能，这种方式可以实现简单的分布式限流。但是redis本身也容易成为瓶颈，且redis不管是主从结构还是其cluster模式，都存在主节点故障问题。 方案1：固定窗口计数将要限制的资源名+时间窗口为精度的时间戳 作为redis 的key，设置略大于时间戳的超时时间，然后用redis的incrby的原子特性来增加计数。 如果限流的时间窗口以秒为单位，则 redis key : 资源名 + unix timestamp count : incrby key count expire 2 检查count是否达到阈值 1234567891011local key = KEYS[1]local limit = tonumber(ARGV[1])local acquireCount = tonumber(ARGV[2])local current = tonumber(redis.call(&apos;get&apos;, key) or &quot;0&quot;)if current + acquireCount &gt; limit then return 0else redis.call(&quot;incrby&quot;, key, acquireCount) redis.call(&quot;expire&quot;, key, &quot;2&quot;) return 1end 这种方案存在的问题： 要和redis进行交互：时延较差 热点资源redis容易成为瓶颈 redis进行主从切换会导致限流失效 服务的时钟会有误差：由于lua中有写操作就不能使用带随机性质的读操作所以不能通过redis lua获取 属于固定窗口算法，在窗口之间容易产生突发流量问题 方案2：令牌桶1234567891011121314151617181920212223242526272829303132333435local function acquire(key, acquireTokens, currentTimeMillSecond) local rateLimiterInfo = redis.pcall(&quot;HMGET&quot;, key, &quot;lastTimeMilliSecond&quot;, &quot;availableTokens&quot;, &quot;maxLimit&quot;, &quot;rate&quot;) local lastTimeMilliSecond = rateLimiterInfo[1] local availableTokens = tonumber(rateLimiterInfo[2]) local maxLimit = tonumber(rateLimiterInfo[3]) local rate = rateLimiterInfo[4] local currentTokens = availableTokens; local result = -1 if (type(lastTimeMilliSecond) ~= &apos;boolean&apos; and lastTimeMilliSecond ~= false and lastTimeMilliSecond ~= nil) then local diffTime = currentTimeMillSecond - lastTimeMilliSecond if diffTime &gt; 0 then local fillTokens = math.floor((diffTime / 1000) * rate) local allTokens = fillTokens + availableTokens; currentTokens = math.min(allTokens, maxLimit); end end if (currentTokens - acquireTokens &gt;= 0) then result = 1 redis.pcall(&quot;HMSET&quot;, key, &quot;lastTimeMilliSecond&quot;, currentTimeMillSecond, &quot;availableTokens&quot;, currentTokens - acquireTokens) end return resultendlocal key = KEYS[1]local acquireTokens = ARGV[1]local currentTimeMillSecond = ARGV[2]local ret = acquire(key, acquireTokens, currentTimeMillSecond)return ret 这种方案存在的问题： 要和redis进行交互：时延较差 热点资源redis容易成为瓶颈 redis进行主从切换会导致限流失效 服务的时钟会有误差：由于lua中有写操作就不能使用带随机性质的读操作所以不能通过redis lua获取 发票服务器上述redis方案，是将redis作为一种发票服务器，但是由于redis这种方案本身存在可用性问题(主从切换等)，控制规则也比较简单，所以对于可用性要求比较高且规则复杂的需求，都选择自己开发服务器程序来作为发票服务器。如阿里开源的sentinel 发票服务器一般由一些服务进程组成一个或多个发票集群。而服务通过RPC向发票服务器领票，成功则可以执行，否则则进入限流机制。为了减少RPC通信带来的延迟，一般可以批量获取。 发票规则发票规则(限流算法)可以存储到一致性存储或者数据库等，发票服务器定期更新或者监听通知来获取规则的变化。也可以通过其他服务来动态调整算法和阈值，然后通知发票服务器，也可以发票服务器自己根据负载情况来计算。 发票服务器特点： 发票服务器可用性高：通过集群模式，且可以持久化到数据库。 发票服务器负载均衡：服务从发票服务集群领票要注意发票服务器负载均衡，避免造成有的发票服务器发票领完有的却有大量剩余发票 发票服务器高性能：因为发票服务器的计算和存储都基于内存，所以性能不容易成为瓶颈 发票服务器一致性：类似于ID生成器，对于极高要求的场景，可以定期将发票服务器发票的信息等进行持久化存储，故障时再从中进行恢复 微服务架构下限流的难点在微服务架构下，调用链路很长，服务的调用关系复杂，上游服务一个小的改动，将影响所有下游服务，还会产生叠加效应。 流控规则微服务架构下，固定流控规则是不合适的，固定规则往往根据压测结果进行计算得来，而微服务架构下，链路上一个节点的变化都会导致固定规则的失效。如：服务某个链路为 A-&gt;B-&gt;C-&gt;D而B做了一些业务逻辑的变更，那么链路就有可能产生变化，导致之前的链路压测结果不准确，如果还按照之前的阈值，则可能导致流控失效。 微服务架构下，应该采用动态规则，让服务自适应或者规则服务器来通知服务改变限流规则 根据调用链路的限流规则下游服务进行限流时，也要考虑给予上游服务的流量配额，否则很容易因为由于某个上游服务的故障，导致整个下游链路不可用。 如：链路1：A-&gt;B-&gt;C-&gt;D链路2：D-&gt;B-&gt;CA如果故障，导致调用B流量暴涨，那么调用C的流量也会暴涨。这个时候链路2的D服务则会收到B或C的流控影响。 所以B和C应该根据给予链路1和链路2进行不同的配额，链路1达到阈值则对链路1的调用进行限流，不影响链路2的调用。 考虑调用链路优先级一般微服务场景会根据业务来定义其可用性权重。权重高的业务往往要优先保证其可用性。那么就存在复杂调用链路上，针对不同业务的请求来进行限流，服务压力过大时，优先保证重要的业务可运行。 如：链路1：A-&gt;B-&gt;C-&gt;D链路2：D-&gt;B-&gt;C链路1的重要性高于链路2，那么如果C的负载升高时，C就要降低整体的限流阈值，那么就要降低链路2的限流阈值，牺牲链路2的可用性来保证链路1。 所以就需要将链路进行tag标识，服务C根据tag来区分链路。 总结 限流算法往往相对简单且开源方案很多。而难点在于如何选择适合自己的限流算法。 不管是单节点线路还是分布式限流，都有各自的优缺点，需要根据自身业务特性、规模以及架构复杂性来选择适合自己的方案。 微服务架构下，限流变得相当复杂，要考虑清楚各种可能导致限流失效的场景。]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>SOA</tag>
        <tag>microservice</tag>
        <tag>rate limit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NUMA笔记]]></title>
    <url>%2F2018%2F04%2F12%2Fnuma%2F</url>
    <content type="text"><![CDATA[NUMA 概念、历史、问题 NUMA 概念NUMA的几个概念（Node，socket，core，thread） socket就是主板上的CPU插槽; core就是socket里独立的一组程序执行的硬件单元，比如寄存器，计算单元等; thread：就是超线程hyperthread的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。 NUMA体系结构中多了Node的概念，这个概念其实是用来解决core的分组的问题，具体参见下图来理解（图中的OS CPU可以理解thread，那么core就没有在图中画出），从图中可以看出每个Socket里有两个node，共有4个socket，每个socket 2个node，每个node中有8个thread，总共4（Socket）× 2（Node）× 8 （4core × 2 Thread） = 64个thread。 另外每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量，因为Node内有自己内部总线，所以增加CPU数量可以通过增加Node的数目来实现，如果单纯的增加CPU的数量，会对总线造成很大的压力，所以UMA结构不可能支持很多的核。 《NUMA Best Practices for Dell PowerEdge 12th Generation Servers》 根据上面提到的，由于每个node内部有自己的CPU总线和内存，所以如果一个虚拟机的vCPU跨不同的Node的话，就会导致一个node中的CPU去访问另外一个node中的内存的情况，这就导致内存访问延迟的增加。在有些特殊场景下，比如NFV(Network Function Virtualization)环境中，对性能有比较高的要求，就非常需要同一个虚拟机的vCPU尽量被分配到同一个Node中的pCPU上，所以在OpenStack的Kilo版本中增加了基于NUMA感知的虚拟机调度的特性。 查看机器的NUMA拓扑结构123456789101112131415161718192021222324[root@local ~]$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 48 // 48个逻辑CPU（threads）On-line CPU(s) list: 0-47Thread(s) per core: 2 // 每个core有2个threadsCore(s) per socket: 12 // 每个socket有12个coresSocket(s): 2 // 共总有2个socketsNUMA node(s): 2 // 2个NUMA nodesVendor ID: GenuineIntelCPU family: 6Model: 63Model name: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHzStepping: 2CPU MHz: 2500.089BogoMIPS: 4999.27Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 30720KNUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47 可以看出当前机器有2个sockets，每个sockets包含1个numa node，每个numa node中有12个cores，每个cores包含2个thread，所以总的threads数量=2x1x12x2=48. NUMA 历史在若干年前，对于x86架构的计算机，那时的内存控制器还没有整合进CPU，所有内存的访问都需要通过北桥芯片来完成。此时的内存访问如下图所示，被称为UMA（uniform memory access, 一致性内存访问）。这样的访问对于软件层面来说非常容易实现：总线模型保证了所有的内存访问是一致的，不必考虑由不同内存地址之前的差异。 之后的x86平台经历了一场从“拼频率”到“拼核心数”的转变，越来越多的核心被尽可能地塞进了同一块芯片上，各个核心对于内存带宽的争抢访问成为了瓶颈；此时软件、OS方面对于SMP多核心CPU的支持也愈发成熟；再加上各种商业上的考量，x86平台也搞了NUMA（Non-uniform memory access, 非一致性内存访问）。 NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die(核心)上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）。所以NUMA（Non-Uniform Memory Access）就此得名 在这种架构之下，每个Socket都会有一个独立的内存控制器IMC（integrated memory controllers, 集成内存控制器），分属于不同的socket之内的IMC之间通过QPI link通讯。 然后就是进一步的架构演进，由于每个socket上都会有多个core进行内存访问，这就会在每个core的内部出现一个类似最早SMP架构相似的内存访问总线，这个总线被称为IMC bus。 于是，很明显的，在这种架构之下，两个socket各自管理1/2的内存插槽，如果要访问不属于本socket的内存则必须通过QPI link。也就是说内存的访问出现了本地/远程（local/remote）的概念，内存的延时是会有显著的区别的。 以Xeon 2699 v4系列CPU的标准来看，两个Socket之之间通过各自的一条9.6GT/s的QPI link互访。而每个Socket事实上有2个内存控制器。双通道的缘故，每个控制器又有两个内存通道（channel），每个通道最多支持3根内存条（DIMM）。理论上最大单socket支持76.8GB/s的内存带宽，而两个QPI link，每个QPI link有9.6GT/s的速率（~57.6GB/s）事实上QPI link已经出现瓶颈了。 核心数还是源源不断的增加，Skylake桌面版本的i7 EE已经有了18个core，Skylake Xeon 28个Core(2017)。为了塞进更多的core，原本核心之间类似环网的设计变成了复杂的路由。由于这种架构上的变化，导致内存的访问变得更加复杂。两个IMC也有了local/remote的区别，在保证兼容性的前提和性能导向的纠结中，系统允许用户进行更为灵活的内存访问架构划分。于是就有了“NUMA之上的NUMA”这种妖异的设定（SNC）。 性能提升内核调度和操作方式 在一个启用了NUMA支持的Linux中，Kernel不会将任务内存从一个NUMA node搬迁到另一个NUMA node。 一个进程一旦被启用，它所在的NUMA node就不会被迁移，为了尽可能的优化性能，在正常的调度之中，CPU的core也会尽可能的使用可以local访问的本地core，在进程的整个生命周期之中，NUMA node保持不变。 一旦当某个NUMA node的负载超出了另一个node一个阈值（默认25%），则认为需要在此node上减少负载，不同的NUMA结构和不同的负载状况，系统见给予一个延时任务的迁移——类似于漏杯算法。在这种情况下将会产生内存的remote访问。 NUMA node之间有不同的拓扑结构，各个 node 之间的访问会有一个距离（node distances）的概念，如numactl -H命令的结果有这样的描述： 123456789101112[root@local ~]$ numactl -Havailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73363 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117527 MBnode distances:node 0 1 0: 10 21 1: 21 10 可以看出：0 node 到0 node之间距离为10，是最近的距离。 上图记录了某个Benchmark工具，在开启/关闭NUMA功能时QPI带宽消耗的情况。很明显的是，在开启了NUMA支持以后，QPI的带宽消耗有了两个数量级以上的下降，性能也有了显著的提升！ 通常情况下，用户可以通过numactl来进行NUMA访问策略的手工配置，cgroup中cpuset.mems也可以达到指定NUMA node的作用。 Numa内存分配策略有四种: 缺省default:总是在本地节点分配(当前进程运行的节点上)。 绑定bind:强制分配到指定节点上。 交叉interleavel:在所有节点或者指定节点上交叉分配内存。 优先preferred:在指定节点上分配，失败则在其他节点上分配 以numactl命令为例，它有如下策略： –interleave=nodes //允许进程在多个node之间交替访问 –membind=nodes //将内存固定在某个node上，CPU则选择对应的core。 –cpunodebind=nodes //与membind相反，将CPU固定在某（几）个core上，内存则限制在对应的NUMA node之上。 –physcpubind=cpus //与cpunodebind类似，不同的是物理core。 –localalloc //本地配置 –preferred=node //按照推荐配置 对于某些大内存访问的应用，比如Mongodb，将NUMA的访问策略制定为interleave=all则意味着整个进程的内存是均匀分布在所有的node之上，进程可以以最快的方式访问本地内存。北桥有一个功能就是PCI/PCIe控制器，南桥（PCH）整合了PCIe控制器。在PCIe channel上也是有NUMA亲和性的。 比如：查看网卡em1的NUMA 12345678[root@local ~]$ numactl --prefer netdev:em1 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 PCI address 为00:1f.2的SATA控制器，用到了pci:00:1f.2 SATA controller: Intel Corporation C610/X99 series chipset 6-Port SATA Controller [AHCI mode] (rev 05) 1234567[root@local ~]$ numactl --prefer pci:00:1f.2 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前系统numa策略： 1234567[root@local ~]$ numactl --showpolicy: defaultpreferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前numa的节点情况： 123456789101112[root@local ~]$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73338 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117521 MBnode distances:node 0 1 0: 10 21 1: 21 10 NUMA带来的问题 MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases 这些问题都是：“因为CPU亲和策略导致的内存分配不平均”及“NUMA Zone Claim内存回收”有关，而和数据库种类并没有直接联系。 数据库与NUMAMySQL在NUMA架构上遇到的典型问题 The MySQL “swap insanity” problem and the effects of the NUMA architecture A brief update on NUMA and MySQL 大致分析如下： CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核CPU通过一条总线共享内存成为瓶颈 于是NUMA出现了，CPU平均划分为若干个Chip（不多于4个），每个Chip有自己的内存控制器及内存插槽 CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存（下文称Remote Access）的速度相较慢三倍左右 于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中 由于内存页没有动态调整策略，使得大部分内存页都集中在CPU 0上 又因为Reclaim默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出 当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现了 解决方案： numactl –interleave=all 在MySQL进程启动前，使用sysctl -q -w - vm.drop_caches=3清空文件缓存所占用的空间 Innodb在启动时，就完成整个Innodb_buffer_pool_size的内存分配 不过这种三合一的解决方案只是减少了NUMA内存分配不均，导致的MySQL SWAP问题出现的可能性。如果当系统上其他进程，或者MySQL本身需要大量内存时，Innodb Buffer Pool的那些Page同样还是会被Swap到存储上。于是又在这基础上出现了另外几个进阶方案 配置vm.zone_reclaim_mode = 0使得内存不足时去remote memory分配优先于swap out local page echo -15 &gt; /proc//oom_adj调低MySQL进程被OOM_killer强制Kill的可能 memlock 对MySQL使用Huge Page（黑魔法，巧用了Huge Page不会被swap的特性） 为什么Interleave的策略就解决了问题？借用两张 Carrefour性能测试 的结果图，可以看到几乎所有情况下Interleave模式下的程序性能都要比默认的亲和模式要高，有时甚至能高达30%。究其根本原因是Linux服务器的大多数workload分布都是随机的：即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分布的。而Interleave模式就恰恰是针对这种特性将内存page随机打散到各个CPU Core上，使得每个CPU的负载和Remote Access的出现频率都均匀分布。相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多。 也就是说，像MySQL这种外部请求随机性强，各个线程访问内存在地址上平均分布的这种应用，Interleave的内存分配模式相较默认模式可以带来一定程度的性能提升。此外各种论文 中也都通过实验证实，真正造成程序在NUMA系统上性能瓶颈的并不是Remote Acess带来的响应时间损耗，而是内存的不合理分布导致Remote Access将interconnect这个小水管塞满所造成的结果。而Interleave恰好，把这种不合理分布情况下的Remote Access请求平均分布在了各个小水管中。所以这也是Interleave效果奇佳的一个原因。 那是不是简简单单的配置个Interleave就已经把NUMA的特性和性能发挥到了极致呢？答案是否定的，目前Linux的内存分配机制在NUMA架构的CPU上还有一定的改进空间。例如：Dynamic Memory Loaction, Page Replication。 Dynamic Memory RelocationMySQL的线程分为两种，用户线程（SQL执行线程）和内部线程（内部功能，如：flush，io，master等）。对于用户线程来说随机性相当的强，但对于内部线程来说他们的行为以及所要访问的内存区域其实是相对固定且可以预测的。如果能对于这把这部分内存集中到这些内存线程所在的core上的时候，就能减少大量Remote Access，潜在的提升例如Page Flush，Purge等功能的吞吐量，甚至可以提高MySQL Crash后Recovery的速度（由于recovery是单线程）。那是否能在Interleave模式下，把那些明显应该聚集在一个CPU上的内存集中在一起呢？很可惜，Dynamic Memory Relocation这种技术目前只停留在理论和实验阶段。我们来看下难点：要做到按照线程的行为动态的调整page在memory的分布，就势必需要做线程和内存的实时监控（profile）。对于Memory Access这种非常异常频繁的底层操作来说增加profile入口的性能损耗是极大的。 Page Replication一些动态加载的库，把他们放在任何一个线程所在的CPU都会导致其他CPU上线程的执行效率下降。而这些共享数据往往读写比非常高，如果能把这些数据的副本在每个Memory Zone内都放置一份，理论上会带来较大的性能提升，同时也减少在interconnect上出现的瓶颈。由于缺乏硬件级别（如MESI协议的硬件支持）和操作系统原生级别的支持，Page Replication在数据一致性上维护的成本显得比他带来的提升更多。因此这种尝试也仅仅停留在理论阶段。当然，如果能得到底层的大力支持，相信这个方案还是有极大的实际价值的。 关闭NUMA特性的方法 硬件层，在BIOS中设置关闭 OS内核，启动时设置numa=off 进程，numactl 进程启动时。numactl –interleave=all NUMA取舍指定numa在运行程序的时候使用numactl -m和-physcpubind就能制定将这个程序运行在哪个cpu和哪个memory中:numactl –physcpubind=2,6 ./program 玩转cpu-topology(站点已经无法访问) 的测试中显示当程序只使用一个node资源和使用多个node资源的比较表（差不多是38s与28s的差距）。所以限定程序在numa node中运行是有实际意义的。 指定numa带来的问题SWAP的罪与罚 文章就说到了一个numa的陷阱的问题。现象是当你的服务器还有内存的时候，发现它已经在开始使用swap了，甚至已经导致机器出现停滞的现象。如果一个进程限制它只能使用自己的numa节点的内存，那么当自身numa node内存使用光之后，就不会去使用其他numa node的内存了，会开始使用swap，甚至更糟的情况，机器没有设置swap的时候，可能会直接死机！所以你可以使用numactl --interleave=all来取消numa node的限制。 根据具体业务决定NUMA的使用: 如果你的程序是会占用大规模内存的，你大多应该选择关闭numa node的限制。因为这个时候你的程序很有几率会碰到numa陷阱。 如果你的程序并不占用大内存，而是要求更快的程序运行时间。你大多应该选择限制只访问本numa node的方法来进行处理。 推荐阅读: NUMA-aware scheduler for Go PostgreSQL, NUMA and zone reclaim mode on linux NUMA and Java Databases MySQL Server and NUMA architectures]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[顺序、时钟与分布式系统]]></title>
    <url>%2F2018%2F03%2F15%2Fordering_clock%2F</url>
    <content type="text"><![CDATA[顺序、时钟与分布式系统 Ordering现实生活中时间可以记录事情发生的时刻、比较事情发生的先后顺序。分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序。如数据写入先后顺序，事件发生的先后顺序等等。 关系复习下离散数学中关系： 假设A是一个集合 {1,2,3,4} ；R是集合A上的关系，例如{&lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt;,&lt;1,2&gt;,&lt;1,4&gt;,&lt;2,4&gt;,&lt;3,4&gt;} 自反性：任取一个A中的元素x，如果都有&lt;x,x&gt;在R中，那么R是自反的。 &lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt; 对称性：任取一个A中的元素x,y，如果&lt;x,y&gt; 在关系R上,那么&lt;y,x&gt; 也在关系R上，那么R是对称的。 反对称性：任取一个A中的元素x,y(x!=y)，如果&lt;x,y&gt; 在关系R上,那么&lt;y,x&gt; 不在关系R上，那么R是反对称的。 对于 &lt;1,2&gt;，有 &lt;2,1&gt; 不在R中；对于&lt;2,4&gt; 有&lt;4,2&gt;不在R中；对于&lt;3,4&gt; 有&lt;4,3&gt; 不在 R中，满足。 传递性：任取一个A中的元素x,y,z，如果&lt;x,y&gt;,&lt;y,z&gt; 在关系R上，那么 &lt;x,z&gt; 也在关系R上，那么R是对称的。 &lt;1,1&gt;&lt;1,2&gt;在R中，并且&lt;1,2&gt;在R中；&lt;1,1&gt;&lt;1,4&gt;在R中，并且&lt;1,4&gt;在R中；&lt;2,2&gt;&lt;2,4&gt;在R中，并且&lt;2,4&gt;在R中；&lt;3,3&gt;&lt;3,4&gt;在R中，并且&lt;3,4&gt;在R中；等等其他，满足。 完全性（全关系）：包含了自反性；对集合A中所有&lt;x,y&gt;，都有关系x到y或y到x； R中并没有&lt;1, 3&gt;，所以不满足完全性 偏序The Partial Ordering集合内只有部分元素之间是可以比较的。 偏序关系的定义（R为A上的偏序关系）：设R是集合A上的一个二元关系，若R满足： 反对称性：对任意x,y∈A，若xRy，且yRx，则x=y； 传递性：对任意x, y,z∈A，若xRy，且yRz，则xRz 自反性：对任意x∈A，有xRx； 一个partitial ordering关系满足的条件是自反的，反对称的和可传递的，因此在partitial ordering中，可能有两个元素之间是不相关的。 全序The Total Ordering集合内只有部分元素之间是可以比较的。比如：比如复数集中并不是所有的数都可以比较大小，那么“大小”就是复数集的一个偏序关系。 全序关系的定义： 反对称性：对任意x,y∈A，若xRy，且yRx，则x=y； 传递性：对任意x, y,z∈A，若xRy，且yRz，则xRz 完全性(total relation全关系)：对任意x,y∈A，由xRy或yRx （包括了自反性） 完全性本身也包括了自反性，所以全序关系是偏序关系。 所以偏序中满足完全性就是全序了。 一个total ordering关系满足的条件是反对称的，可传递的和完全性，因此在total ordering中，两个元素一定是有关系的，要么是a&lt;&gt;b或b&lt;&gt;a。 happens before在分布式系统中，一个进程包含一系列的事件，对于同一进程内的事件，如果a happens before b，那么a发生在b之前。并且，假定收或发消息都是一个事件。 happens before的定义如下(用-&gt;表示) 如果a和b在同一进程中，并且a发生在b之前，那么a-&gt;b 如果a是一个进程发消息的事件，b是另一个进程接收这条消息的事件，则a-&gt;b 如果a-&gt;b且b-&gt;c，那么a-&gt;c。 如果同时不满足a-&gt;b，且b-&gt;a，那么说a和b是并发的concurrent [图来自Time, Clocks, and the Ordering of Events in a Distributed System] 以一个例子来说明happens before关系，如上图，垂直线上代表一个进程，从下往上，时间依次增加，水平的距离代表空间的隔离。原点代表一个事件，而曲线代表一条消息。 从图中很容易地看出，如果一个事件a，能通过进程的线和消息线，到达b，那么a-&gt;b。 在图中，p3和q4是并行的事件，因为，只有到了p4才能确定q4的发生，而q3也只能确定p1发生。 clock时钟物理时钟晶振和时钟偏移计算机有固定频率晶体的震荡次数，晶体的振荡周期决定了单机的时钟精度。 时钟频率也可能因为温度等外部因素导致时钟偏移，普通的石英晶体的漂移大约$10^{-6}$ 原子钟的漂移约为 $10^{-13}$所以原子钟精度远远高于石英晶体。 分布式下带来的问题不同机器上的物理时钟难以同步，导致无法区分在分布式系统中多个节点的事件时序。即使设置了 NTP 时间同步节点间也存在毫秒级别的偏差，因而分布式系统需要有另外的方法记录事件顺序关系。 1978年Lamport在《Time, Clocks and the Ordering of Events in a Distributed System》中提出了逻辑时钟的概念，来解决分布式系统中区分事件发生的时序问题。 逻辑时钟Logical clocks逻辑时钟指的是分布式系统中用于区分事件的发生顺序的时间机制。 从某种意义上讲，现实世界中的物理时间其实是逻辑时钟的特例。 Logical Clock解决的问题是找到一种方法，给分布式系统中所有时间定一个序，这个序能够正确地排列出具有因果关系的事件（注意，是不能保证并发事件的真实顺序的），使得分布式系统在逻辑上不会发生因果倒置的错误。因果一致性 Lamport timestamps论文Time, Clocks, and the Ordering of Events in a Distributed System Lamport timestampsLeslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)。 分布式系统中按是否存在节点交互可分为三类事件: 发生在节点内部 发送事件 接收事件 时钟的定义如下 对于一个进程i，Ci(a)表示进程i中事件a的发生时间 对于整个系统来讲，对于任意的事件b，其发生时间为C(b)，当b为进程j的事件时，则C(b) = Cj(b)为了使得事件按照正确的排序，需要使得如果事件a发生在事件b之前，那么a发生的时间要小于b，如下 12for any events a, bif a-&gt;b then C(a) &lt; C(b) 根据关系-&gt;的定义，我们可以得出 如果a和b都是进程i中的事件，且a发生在b之前，那么Ci(a) &lt; Ci(b) 如果事件a发送消息给事件b，a属于进程i，b属于进程j，那么Ci(a) &lt; Cj(b) 为了让系统满足上述条件，在实现中，需要满足以下原则 对于每个进程，相邻的事件的时钟要增加1 (a) 如果事件a是进程i发送消息m的事件，发送时带时间戳Tm = Ci(a)，(b)事件b是进程j接受消息m的事件，那么事件b的取值为max(进程b的当前时钟，Tm+1) 假设有事件a、b，C(a)、C(b)分别表示事件a、b对应的Lamport时间戳，如果a-&gt;b,则C(a) &lt; C(b)，a发生在b之前(happened before)。 所以Lamport timestamps原理如下： 每个事件对应一个Lamport时间戳，初始值为0 如果事件在节点内发生，时间戳加1 如果事件属于发送事件，时间戳加1并在消息中带上该时间戳 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1 通过该定义，事件集中Lamport时间戳不等的事件可进行比较，我们获得事件的偏序关系(partial order)。 上图更形象的解释了事件之间的关系。 以B4事件为基准： B4左边深灰色的区域的事件，都发生在B4前，和B4具有因果关系，这些事件属于与B4因果关系中的因(cause) B4右边的深红色区域的事件，都发生在B4后，和B4具有因果关系，这些事件属于与B4因果关系中的果(effect) B4上下的白色区域是跟B4无关的事件，可以认为是并发关系(concurrent) 在浅灰色和浅红色区域中的事件，C2、A3两个事件与B4是并行关系，根据Lamport timestamps的定义，将他们判定为与B4具前后关系。（所以Lamport timestamps并不能严格的表示并行关系） Lamport timestamps与偏序关系Lamport timestamps只保证因果关系（偏序）的正确性，不保证绝对时序的正确性。 Lamport logical clock由于Lamport timestamps只能得到偏序关系，如果要得到全序关系，就需要给Ci(a) = Cj(b)的事件定一个先后顺序。 total order的事件关系=&gt;定义如下：如果事件a发生在进程Pi，事件b发生在进程Pj，那么当满足下列两者条件之一时，a=&gt;b Ci(a) &lt; Cj(b) Ci(a) = Cj(b) 且 Pi &lt; Pj 根据以上条件，对于任意的两个事件，都能判断出它们之间的关系，因此是total ordering的。 当Lamport timestamp一致时，通过义Pi &lt; Pj来定义顺序，确保分布式场景下各个进程间发生的事件的全序定义。至于Pj &lt; Pj：可采用不同的方式，Lamport Logical Clock提到的 arbitrary total ordering。 vector clockLamport timestamp得到的是全序关系，但无法严格表示对于没有因果关系、存在同时发生关系(concurrent)的事件。 Vector clock是在Lamport timestamp基础上改进的一种逻辑时钟方法，它构不但记录本节点的Lamport timestamp，同时也记录了其他节点的Lamport timestamp。 原理如下: 本地vector clock的clock数组中每一个逻辑时间(clock)对应一个进程的clock 初始化vector clock中每一个逻辑时间为0； 每一次处理内完内部事件，将vector clock中自己的逻辑时间戳+1； 每发送一个消息的时候，将vector clock中自己的逻辑时间+1，且将其和消息一起发送出去 每接收到一个消息的时候，需要将本地的vector clock中自己的逻辑时间戳+1，且将自己vector clock中的逻辑时间和消息中携带的进行比较，取最大的更新本地vector clock中的逻辑时间。 图来源于wikipedia vector clock判定并发关系： 事件i、事件j对应的vector clock中，每一个进程Pk的逻辑时间戳都满足Vi[Pk]&lt;Vj[Pk]时，我们称事件i happen before事件j； vector clock中，存在P1、P2，使得Vi[P1]&lt;Vj[P1]，Vi[P2]&gt;Vj[P2]，我们称事件i和事件j是并发关系(没有因果关系)； 和之前lamport timestamp的一样，以B4事件为基准(vector clock为[A:2,B:4,C:1])，根据vector clock的判定，可以判断出 灰色区域的事件happens before B4事件，B4事件happens before红色区域的事件 白色区域与B4事件没有因果关系。 特性： vector clock不需要在节点之间同步时钟，不需要在所有节点上维护一段数据的版本数； 缺点是时钟值的大小随着节点增多和时间不断增长 version vector分布式系统多个副本被同时更新时，会导致副本之间数据的不一致。version vector用于来发现这些不一致的冲突。 version vector只能发现冲突，无法解决冲突；当然也可以通过再添加一个维度信息timestamp，发生冲突时进行比较，但是又回到了物理时钟不同步的问题。 下图展示了数据由不同副本处理后导致的不同版本冲突。D5时发现了数据的冲突，这时会将不同版本数据都存储下来，一般由客户端来解决冲突。 version vector与vector clock的差异 vector clocks 使用 receive和send 方法来更新clock,而version vector使用sync方法来更新。 vector clocks是给事件定序的，确定事件的因果关系；而version vector是确定同一个数据不同版本的因果关系。 分布式与时钟分布式系统中，每个节点的物理时钟是不同步的，都有一定的差异。 这样就带来了一些分布式系统实现的难题，如基于MVCC实现的事务，基于MVCC实现事务会要求版本之间能判断先后顺序，只有确定先后才知道应该用哪一个版本的数据，确定先后顺序就涉及到时间，而不同机器之间的本地时钟是无法保证一致的，所以这就需要确保时钟的同步。 而通常解决方案有两种： 中心化的时钟方案，如Timestamp oracle(TSO) 无中心化的时钟方案，如google True Time，Hybrid Logic Time Timestamp oracle如果我们整个系统不复杂，而且没有跨全球的需求，这时用一台中心授时服务就可以了。 如TiDB使用的就是TSO方案，tipb作为一个TSO集群，来提供授时服务。 使用TSO的好处在于因为只有一个中心授时，所以我们一定能确定所有时间的时间，但TSO需要关注几个问题： 网络延时：因为所有的事件都需要从TSO获取时间，所以TSO只适合小集群部署，不能是那种全球级别的数据库 性能：每个事件都需要从TSO获取时间，所以TSO需要非常高的性能 容错：TSO是一个单点，需要考虑节点的failover True Time由于节点间NTP是有偏差的，且可能出现时间回退的情况，所以NTP无法准确的判定事件的全序关系。在Google Spanner里面，通过引入True Time来解决了分布式时间问题。 True Time实现Spanner通过使用GPS + 原子钟atomic clock来对集群的机器时间进行校对，保证了集群机器的时间戳差距不会超过一个上限值(ε)。 用两种技术来处理，是因为导致这两种技术的失败的原因是不同的。 GPS会有一个天线，电波干扰会导致其失灵。原子钟很稳定。 当GPS失灵的时候，原子钟仍然能保证在相当长的时间内，不会出现偏差。 API TT.now() : 返回一个当前时间，其位于范围区间[earliest,latest] TT.after(t) : 当前时间是否在t之后 TT.before(t) : 当前时间是否在t之前 虽然spanner引入了TrueTime可以得到全球范围的时序一致性，但由于TrueTime返回的时间仍然有一定的偏差，如果要给两个事件定序，就需要等待2个偏差的时间间隔，来确保其先后顺序。 事件a：[Tai, Taj], Taj-Tai=ε 事件b：[Tbi, Tbj], Tbj-Tbi=ε 所以要确定b&gt;a, 那么就要确保Tbi &gt; Taj, 就需要在事件b进行等待，以确保：事件b时间 - 事件a时间 &gt; 2ε Hybrid logical clockHLCLogical Physical Clocks and Consistent Snapshots in Globally Distributed Databases TrueTime 需要硬件的支持，所以有一定的成本，而HLC无需硬件支持也能解决分布式下时间问题。 HLC同时使用了物理时钟和逻辑时钟（physical clock + logical clock），能够保证单点的时间发生器是单调递增的，同时能够尽量控制不同节点之间的时钟偏差在规定的偏差范围内。 判断两个事件的先后顺序：先判断物理时间，再判断逻辑时间。 HLC的算法l.j维护的是节点j当前已知的最大的物理时间(wall time)，c.j则是当前的逻辑时间。 1234567891011121314151617181920212223242526272829303132333435// 在节点j上面：初始化: l.j = 0，c.j = 0。initially l.j :=0; c.j := 0// 本地事件或者发送消息时，// 如果本地时钟pt大于当前的混合逻辑时钟的l，// 则将l更新成本地时钟，将c清零。// 否则，l保持不变，将c加1。Send or local event&#123; l&apos;.j := l.j; l.j := max(l&apos;.j, pt.j); // 本地物理时间pt if (l.j = l&apos;.j) then c.j := c.j+1 else c.j := 0; Timestamp with l.j, c.j&#125;// 收到消息时// l在 当前的逻辑时钟的l、机器的本地时钟pt、收到消息里面带的l，三者中取最大的。// 如果l部分是更新为本地时钟了，则将c清零。否则，c取较大的那个l对应到的c加1。Receive event of message m&#123; l&apos;.j := l.j; l.j := max(l&apos;.j, l.m, pt.j); if (l.j = l&apos;.j = l.m) then c.j := max(c.j, c.m) + 1 elseif (l.j=l&apos;.j) then c.j := c.j + 1 elseif (l.j=l.m) then c.j := c.m + 1 else c.j := 0 Timestamp with l.j, c.j&#125; 特性HLC算法保证了HLC时间有如下特性： 事件e发生在事件f之前，那么事件e的HLC时间一定小于事件f的HLC时间：(l.e, c.e) &lt; (l.f, c.f) 本地WallTime大于等于本地物理时间(l.e ≥ pt.e)：HLC时间总是不断递增，不会随着物理时间发生回退。 对事件e，l.e是事件e能感知的到的最大物理时间值：如果l.e &gt; pt.e，那么一定存在着一个发生在e之前的事件g，有pt.g=l.e。简单来说是如果出现l.e &gt; pt.e肯定是因为有一个HLC时间更大的的节点把当前节点的HLC时间往后推了。 WallTime和物理时钟的偏差是有界的(ε ≥ |pt.e - l.e| )：因为节点之间通过NTP服务校时，那么节点之间的物理时钟偏差一定小于某个值ε。那么对于任一事件b和e，如果b hb e，那么事件b的物理时间pt.b一定满足pt.e + ε ≥ pt.b。结合特性3存在一个事件g满足，l.e = pt.g。那么 pt.e + ε ≥ l.e=pt.g &gt; pt.e。 开源实现CockroachDB采用基于NTP时钟同步的HLC去中心化方案。 时钟同步所有节点间的RPC消息都会把时间戳带入到消息中，接收到消息的节点会通过消息中的时间戳更新自己的时间, 从而达到节点间时间同步的效果。 代码分析参考：https://github.com/cockroachdb/cockroach/blob/v1.1.3/pkg/util/hlc/hlc.go HLC定义 1234567891011// Timestamp represents a state of the hybrid logical clock.type Timestamp struct &#123; // Holds a wall time, typically a unix epoch time // expressed in nanoseconds. WallTime int64 `protobuf:&quot;varint,1,opt,name=wall_time,json=wallTime&quot; json:&quot;wall_time&quot;` // The logical component captures causality for events whose wall // times are equal. It is effectively bounded by (maximum clock // skew)/(minimal ns between events) and nearly impossible to // overflow. Logical int32 `protobuf:&quot;varint,2,opt,name=logical&quot; json:&quot;logical&quot;`&#125; WallTime：本地已知物理时钟 Logical：逻辑时钟 Timestamp：HLC，单调递增 获取物理时钟 12345678910111213141516171819202122232425262728293031323334// PhysicalNow returns the local wall time. It corresponds to the physicalClock// provided at instantiation. For a timestamp value, use Now() instead.func (c *Clock) PhysicalNow() int64 &#123; c.mu.Lock() defer c.mu.Unlock() return c.getPhysicalClockLocked()&#125;// getPhysicalClockLocked returns the current physical clock and checks for// time jumps.func (c *Clock) getPhysicalClockLocked() int64 &#123; // physicalClock 就是 UnixNano newTime := c.physicalClock() if c.mu.lastPhysicalTime != 0 &#123; interval := c.mu.lastPhysicalTime - newTime // 检查时钟是否回退 if interval &gt; int64(c.maxOffset/10) &#123; c.mu.monotonicityErrorsCount++ log.Warningf(context.TODO(), &quot;backward time jump detected (%f seconds)&quot;, float64(-interval)/1e9) &#125; &#125; c.mu.lastPhysicalTime = newTime return newTime&#125;// UnixNano returns the local machine&apos;s physical nanosecond// unix epoch timestamp as a convenience to create a HLC via// c := hlc.NewClock(hlc.UnixNano, ...).func UnixNano() int64 &#123; return timeutil.Now().UnixNano()&#125; 获取当前HLC时钟 123456789101112131415161718// Now returns a timestamp associated with an event from// the local machine that may be sent to other members// of the distributed network. This is the counterpart// of Update, which is passed a timestamp received from// another member of the distributed network.func (c *Clock) Now() Timestamp &#123; c.mu.Lock() defer c.mu.Unlock() if physicalClock := c.getPhysicalClockLocked(); c.mu.timestamp.WallTime &gt;= physicalClock &#123; // The wall time is ahead, so the logical clock ticks. c.mu.timestamp.Logical++ &#125; else &#123; // Use the physical clock, and reset the logical one. c.mu.timestamp.WallTime = physicalClock c.mu.timestamp.Logical = 0 &#125; return c.mu.timestamp&#125; 如果当前物理时钟小于WallTime，则将逻辑时钟+1 如果当前物理时钟大于WallTime，则更新WallTime为当前物理时钟，且将逻辑时钟设置为0 节点时钟同步 节点之间通过在RPC请求中携带HLC时间来进行时钟同步。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// sendSingleRange gathers and rearranges the replicas, and makes an RPC call.func (ds *DistSender) sendSingleRange( ctx context.Context, ba roachpb.BatchRequest, desc *roachpb.RangeDescriptor,) (*roachpb.BatchResponse, *roachpb.Error) &#123; ...... br, err := ds.sendRPC(ctx, desc.RangeID, replicas, ba) if err != nil &#123; log.ErrEvent(ctx, err.Error()) return nil, roachpb.NewError(err) &#125; // If the reply contains a timestamp, update the local HLC with it. if br.Error != nil &amp;&amp; br.Error.Now != (hlc.Timestamp&#123;&#125;) &#123; ds.clock.Update(br.Error.Now) &#125; else if br.Now != (hlc.Timestamp&#123;&#125;) &#123; ds.clock.Update(br.Now) &#125; ......&#125;// Update takes a hybrid timestamp, usually originating from// an event received from another member of a distributed// system. The clock is updated and the hybrid timestamp// associated to the receipt of the event returned.// An error may only occur if offset checking is active and// the remote timestamp was rejected due to clock offset,// in which case the timestamp of the clock will not have been// altered.// To timestamp events of local origin, use Now instead.func (c *Clock) Update(rt Timestamp) Timestamp &#123; c.mu.Lock() defer c.mu.Unlock() // 如果本地物理时间pt physicalClock := c.getPhysicalClockLocked() // 大于本地WallTime且大于rt.WallTime： // 更新本地WallTime=pt，且logical=0 if physicalClock &gt; c.mu.timestamp.WallTime &amp;&amp; physicalClock &gt; rt.WallTime &#123; // Our physical clock is ahead of both wall times. It is used // as the new wall time and the logical clock is reset. c.mu.timestamp.WallTime = physicalClock c.mu.timestamp.Logical = 0 return c.mu.timestamp &#125; // In the remaining cases, our physical clock plays no role // as it is behind the local or remote wall times. Instead, // the logical clock comes into play. // 如果rt.WallTime &gt; 本地WallTime： // 检查rt.WallTime与pt是否大于时钟偏差； // 本地WallTime=rt.WallTime，logical++ if rt.WallTime &gt; c.mu.timestamp.WallTime &#123; offset := time.Duration(rt.WallTime-physicalClock) * time.Nanosecond if c.maxOffset &gt; 0 &amp;&amp; offset &gt; c.maxOffset &#123; log.Warningf(context.TODO(), &quot;remote wall time is too far ahead (%s) to be trustworthy - updating anyway&quot;, offset) &#125; // The remote clock is ahead of ours, and we update // our own logical clock with theirs. c.mu.timestamp.WallTime = rt.WallTime c.mu.timestamp.Logical = rt.Logical + 1 &#125; else if c.mu.timestamp.WallTime &gt; rt.WallTime &#123; // 如果本地WallTime&gt;rt.WallTime：logical++ // Our wall time is larger, so it remains but we tick // the logical clock. c.mu.timestamp.Logical++ &#125; else &#123; // Both wall times are equal, and the larger logical // clock is used for the update. if rt.Logical &gt; c.mu.timestamp.Logical &#123; c.mu.timestamp.Logical = rt.Logical &#125; c.mu.timestamp.Logical++ &#125; return c.mu.timestamp&#125; 参考 Lamport timestamp Time, Clocks, and the Ordering of Events in a Distributed System Vector clock Version vector Why Logical Clocks are Easy Version Vectors are not Vector Clocks Vector Clocks Revisited]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What Is the Most Important Thing in Life？]]></title>
    <url>%2F2018%2F01%2F01%2Fwhich-most-important%2F</url>
    <content type="text"><![CDATA[the most important thing in life. the most important thing in life. Keeping healthy. It is health that is real wealth and not pieces of gold and silver. Everybody needs somebody, be that a friend, a partner, or someone you’re related to. Making someone’s day full of sunshine even when yours is not. Money should not be a priority. The beautiful thing about learning is that nobody can take it away from you. Know who you are. Don’t be a victim BUT instead be a hero in your life. Don’t give up. A winner is just a loser who tried on more time. Always make time for gratitude.]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>人生</tag>
      </tags>
  </entry>
</search>
