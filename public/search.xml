<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[grpc]]></title>
    <url>%2F2018%2F06%2F02%2Fgrpc%2F</url>
    <content type="text"><![CDATA[grpc 特性、原理、实践、生态 gRPC概述gRPC是一个由google设计开发基于HTTP/2协议和Protobuf序列化协议的的高性能、多语言、通用的开源 RPC 框架。 跨语言、跨平台插件化 ： 负载均衡，tracing，健康检查，认证等等编码压缩 ： 节省带宽多路复用 ： 降低的 TCP 链接次数 使用场景 低延迟、高扩展的分布式系统 与云服务通信 设计一个需要准确，高效且与语言无关的新协议 分层设计，以实现扩展，例如：身份验证，负载平衡，日志记录和监控等 特性基于HTTP/2 HTTP/2 提供了 链接多路复用、双向流、服务器推送、请求优先级、首部压缩等机制。gRPC 协议使用了HTTP2 现有的语义，请求和响应的数据使用HTTP Body 发送，其他的控制信息则用Header 表示。 IDL使用ProtoBuffer gRPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议（类似于XML、JSON）。ProtoBuf能够将数据进行序列化，并广泛应用在数据存储、通信协议等方面。压缩和传输效率高，向后兼容，语法简单，表达力强。 多语言支持 gRPC支持多种语言，并能够基于语言自动生成客户端和服务端。 目前支持： C#, C++, Dart, Go, Java, Node, Objective-C, PHP, Python, Ruby 等。 详见官网 HTTP/2HTTP/2HTTP/1.x 是超文本传输协议第1版，可读性好，但效率不高。而HTTP/2 是超文本传输协议第2版，是一个二进制协议。 HTTP/1 和 HTTP/2 的基本语义并没有改变，如方法语义（GET/PUST/PUT/DELETE），状态码（200/404/500等），Range Request，Cacheing，Authentication、URL路径。 HTTP/2通用术语： Stream： 流，一个双向流，一条连接可以有多个 streams。 Message： 逻辑上面的 request，response。 Frame：帧，HTTP/2 数据传输的最小单位。每个 Frame 都属于一个特定的 stream。一个 message 可能由多个 frame 组成。 HTTP/2 流、帧 HTTP/2连接上传输的每个帧(frame)都关联到一个流，一个连接上可以同时有多个流，同一个流的帧按序传输，不同流的帧交错混合传输，客户端、服务端双方都可以建立流，流也可以被任意一方关闭。客户端发起的流使用奇数流ID，服务端发起的使用偶数。 Frame结构 : 123456789+-----------------------------------------------+| Length (24) |+---------------+---------------+---------------+| Type (8) | Flags (8) |+-+-------------+---------------+-------------------------------+|R| Stream Identifier (31) |+=+=============================================================+| Frame Payload (0...) ...+---------------------------------------------------------------+ Length ： 也就是 Frame 的长度 Type ：Frame 的类型，有 DATA，HEADERS，SETTINGS 等 Flags ：帧标志位，8个比特表示可以容纳8个不同的标志：stream是否结束(END_STREAM)，header是否结束(END_HEADERS)，priority等等 R：保留位 Stream Identifier：标识frame所属的 stream，如果为 0，则表示这个 frame 属于整条连接(如SETTINGS帧) Frame Payload：帧内容 帧类型 HEADERS 类似于HTTP/1的 Headers DATA 类似于HTTP/1的 Body CONTINUATION 头部太大，分多个帧传输（一个HEADERS+若干CONTINUATION） SETTINGS 连接设置 WINDOW_UPDATE 流量控制 PUSH_PROMISE 服务端推送 PRIORITY 流优先级更改 PING 心跳或计算RTT RST_STREAM 马上中止一个流 GOAWAY 关闭连接并且发送错误信息 HTTP/2 特性新的二进制格式（Binary Format） HTTP/1 的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同。基于这种考虑HTTP/2的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing） HTTP/1 的request是阻塞的，如果想并发发送多个request，必须使用多个 TCP connection。这样会消耗更多资源，且浏览器为了控制资源，会对单个域名有TCP connection请求限制。 HTTP/2 一个TCP connection可以有多个streams(最大数量由参数SETTINGS_MAX_CONCURRENT_STREAMS控制)， 多个streams 并行发送不同的请求的frames。 可以在SETTINGS帧中设置SETTINGS_MAX_CONCURRENT_STREAMS。而此值是针对一端而言的，客户端可以告知服务器最大的streams并发数，服务端也可以告知客户端。 如果一条链接上 ID 分配完了， server 则会给 client 发送一个 GOAWAY frame 强制让 client 新建一条连接。 header压缩 HTTP/1 是使用文本协议，而且header每次都要重复发送，浪费了带宽也导致资源加载过慢。 HTTP/2 采取了压缩和缓存来避免重复发送和带宽问题： 对消息头采用HPACK 进行压缩传输来节省消息头占用的网络的流量。 对这些headers采取了压缩策略来减少重复headers的请求数 HTTP/2在客户端和服务器端使用 headlist 来存储之前发送过的 header，对于相同的header，不再通过每次请求和响应发送； HPACK: Header Compression for HTTP/2 服务端推送 server push功能 : 在无需客户端请求资源的情况下，服务端会直接推送客户端可能需要的资源到客户端。 当服务器想用Server Push推送资源时，会先向客户端发送PUSH_PROMISE帧。推送的响应必须与客户端的某个请求相关联，因此服务器会在客户端请求的流上发送PUSH_PROMISE帧。 优先级排序 设置优先级的目的是为了告诉对端在并发的多个流之间如何分配资源的行为，同时当发送容量有限时，可以使用优先级来选择用于发送帧的流。 客户端可以通过 HEADERS 帧的 PRIORITY 信息指定一个新建立流的优先级，也可以发送 PRIORITY 帧调整流优先级。 参考官网 Flow Control HTTP/2 支持流控，receiver 端可以对某些stream进行流控也可以针对整个connection流控。而TCP层只能针对整个connection进行流控。 特性 ： Flow control 是由方向的 : Receiver 可以选择给 stream 或者整个连接设置接收端的 window size。 Flow control 是基于信任的 : Receiver 只是会给 sender 建议 连接和 stream 的 flow control window size。 Flow control 无法禁止 Flow control 是基于WINDOW_UPDATE帧的 Flow control 是 hop-by-hop的，而不是 end-to-end 的。例如，用nginx做proxy，则flow control作用于nginx到server和client到nginx这两个connection。 Connection 和 stream 的初始 flow-control window 大小都是 65535。Connection 的初始窗口大小不能改变，但 stream 的可以(所有stream)，通过发送 SETTINGS 帧，携带 SETTINGS_INITIAL_WINDOW_SIZE，这个值即为新的 stream flow-control window 初始大小。 增加flow control window size能加快数据传输，但同时会消耗更多资源。 主动重置链接 HTTP/1 的body的length的被送给客户端后，服务端就无法中断请求了，只能断开整个TCP connection，但这样导致的代价就是需要重新通过三次握手建立一个新的TCP连接。 HTTP/2 引入了一个 RST_STREAM frame 来让客户端在已有的连接中发送重置请求，从而中断或者放弃响应。当浏览器进行页面跳转或者用户取消下载时，它可以防止建立新连接，避免浪费所有带宽。 HTTP/2 站点demoHTTP/1 和 HTTP/2 加载速度比较：https://http2.akamai.com/demo 访问http2站点 ：https://http2.golang.org/ ProtoBufProtoBufGoogle Protocol Buffer 是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化。适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。 描述简单，对开发人员友好 跨平台、跨语言，不依赖于具体运行平台和编程语言 高效自动化解析和生成 压缩比例高 可扩展、兼容性好 gRPC与protobuf gRPC使用 protobuf 作为IDL来定义数据结构和服务。 可以定义数据结构，也可以定义rpc 接口。然后用proto编译器生成对应语言的框架代码。 定义数据结构 ： 生成对象的 序列化 代码 定义rpc接口 ： 生成 gRPC服务端、客户端响应的代码 protobuf 基本数据类型https://developers.google.com/protocol-buffers/docs/proto#scalar 数据结构定义user.proto 12345678910111213141516171819202122232425262728293031323334syntax = &quot;proto2&quot;;// syntax = &quot;proto3&quot;;package user;// option go_package = &quot;protos_golang/user&quot;;import &quot;common.proto&quot;;message User &#123; required int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; optional FLag flag = 4 [default = NORMAL]; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; common.Flag feature = 11;&#125; package package声明符，用来防止不同的消息类型有命名冲突。生成的代码将会包含再package(go等语言)或者命名空间(c++, java等)中。 option go_package = &quot;protos_golang/user&quot;;$LANGUAGE_package 是指定生成的代码的import path和package。 import 要导入其他.proto文件的定义，在文件中添加一个导入声明。使用导入proto的类型 package名字.结构名 来使用导入proto的类型。如上面common.Flag 分配字段编号 每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的。为了保证向后兼容，一旦开始使用就不要再改变。 文件版本申明 syntax = &quot;proto2&quot;; 指定使用proto2语法syntax = &quot;proto3&quot;; 指定为proto3语法 标识符修饰符 required 和 optional 是proto2的语法，proto3已经不支持。proto3中所有的字段都是optional的。具体原因见 required : 必须字段。 optional ：可选字段。 repeated ：数组类型字段。 reserved ：保留字段。指出这些字段编号已经删除，不要再重用这些编号了。因为如果这些编号被重新定义成其他类型，那么对于旧版本的protobuf数据，会导致解码错误。 枚举 与数据结构中 enum 类似。字段编号从0开始。 oneof oneof与数据结构联合体(UNION)类似，一次最多只有一个字段有效。 map map 类型则可以用来表示键值对。key_type 可以是任何 int 或者 string 类型，float、double 和 bytes除外 嵌套类型 可以在消息类型中定义其他消息类型 服务定义12345678910111213141516syntax = &quot;proto2&quot;;import &quot;user.proto&quot;;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; user.User user = 1;&#125; 如果在 .proto 文件中定义了 RPC 服务接口， 编译器将使用生成服务接口代码和 stubs。 import &quot;user.proto&quot;; 导入user结构定义的proto文件。 gRPC 原理概念 gRPC 定义服务，服务包含远程调用的方法。在服务器端，服务器实现rpc接口并运行一个gRPC服务器来处理客户端请求。在客户端，客户端有一个”存根stub”，提供与服务器相同签名的方法，来处理客户端请求的编码、解码等，再将请求转发到服务器端，这样客户端调用rpc方法就像调用本地函数一样。 实现gRPC把HTTP2的steam identifier当作请求ID，每一次请求都发起一个新的stream。 请求的方法、响应的状态码等都放在HEADER frame中。而请求内容和响应内容由protobuf序列化后使用DATA frame中。 请求Request主要由 Request-Headers 和 Data 以及 EOS (END_STREAM)组成。 如下图： Request-Headers Request-Headers 由 HEADERS 和 CONTINUATION frames 组成。如果Flags有设置标志位END_HEADERS则代表Request-Headers结束。 Request-Headers 主要有 Call-Definition 以及 Custom-Metadata : Call-Definition : 包括 Method, Scheme, Path, TE, Authority, Timeout, Content-Type ,Message-Type, Message-Encoding, Message-Accept-Encoding, User-Agent Custom-Metadata : 应用层自定义的任意 key-value，key 不要使用gRPC保留的key前缀字符 grpc- 。 Data 请求体，由一个或多个 Data frame组成。如果Flags有设置标志位END_STREAM则代表Data结束，请求结束。 request格式大致如下 1234567891011121314151617# request-headers HEADERS (flags = END_HEADERS):method = POST:scheme = http:path = /user.UserService/GetUserInfo:authority = localhost:50000grpc-timeout = 999127ucontent-type = grpc-go/1.20.0-dev## 自定义metadataservice : test_clienttraceid : xxxx# dataDATA (flags = END_STREAM)&lt;Length-Prefixed Message&gt; 响应Response 主要由 Response-Headers 和 Data 以及 Trailers 组成。如果遇到了错误，也可以直接返回 Trailers-Only。 如下图： Response-Headers Response-Headers 包含 : HTTP-Status, Message-Encoding, Message-Accept-Encoding, Content-Type, Custom-Metadata等。 Data 响应体，由一个或多个 Data frame组成。如果Flags有设置标志位END_STREAM则代表Data结束。 Trailers Trailers-Only 包含 HTTP-Status, Content-Type, Trailers等。 Trailers 包含 Status, Status-Message, Custom-Metadata等。 Trailers作用主要是给响应包含一些额外的动态生成的信息。如：消息body发送后，再发送一些信息 如数字签名，后处理状态等 格式大致如下： 123456789101112131415161718192021# response-headersHEADERS (flags = END_HEADERS):status = status: 200 content-type = application/grpc## 自定义metadataservice: server_testspanid: xxxx# dataDATA&lt;Length-Prefixed Message&gt;# headersHEADERS (flags = END_STREAM, END_HEADERS)grpc-status: 0## trailers 自定义metadatatimestamp: 1560656283730441829 Status code HTTP状态码对应的gRPC状态码 gRPC通信方式gRPC有四种通信方式: 1、 unary RPC 一般的rpc调用，客户端发送一个请求对象，然后等待服务端返回一个响应对象 123# 获取用户信息# protorpc GetUserInfo (UserRequest) returns (UserResponse) &#123;&#125; 2、 Server-side streaming RPC 服务端流式rpc 客户端发起一个请求到服务端，服务端返回一段连续的数据流响应。 123# 获取一个用户的所有地理位置历史记录# protorpc UserLocationsStream(UserRequest) returns (stream LocationsResponse) &#123;&#125; 3、 Client-side streaming RPC 客户端流式rpc 客户端将一段连续的数据流发送到服务端，服务端返回一个响应。 123# 客户端将所有数据备份到服务端# protorpc BackupStream(stream BackupRequest) returns (BackupResponse) &#123;&#125; 4、 Bidirectional streaming RPC 双向流式rpc 客户端将连续的数据流发送到服务端，服务端返回交互的数据流。 123# 在线聊天# protorpc LiveChat(stream Message) returns (stream Message) &#123;&#125; 配置waitForReady 发送请求时，如果connection没有ready，则会一直等待connection ready 或直到超时(达到deadline)。也常称为fail fast。 timeout 请求超时时间。如果超时，则会中止请求且返回DEADLINE_EXCEEDED 错误。 maxRequestMessageBytes 请求体的最大payload size(没有压缩的)。如果客户端请求大于此值的请求会返回RESOURCE_EXHAUSTED错误。 maxResponseMessageBytes 响应体的最大payload size(没有压缩的)。如果服务端响应大于此值，响应将发送失败。且客户端会得到RESOURCE_EXHAUSTED错误。 gRPC 实践实践部分以go语言进行demo 环境安装protoc mac 1brew install protobuf linux 123456PROTOC_ZIP=protoc-3.5.1-linux-x86_64.zipcurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/$PROTOC_ZIPsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protocsudo unzip -o $PROTOC_ZIP -d /usr/local include/*rm -f $PROTOC_ZIP golang的protobuffers插件 1go get -u github.com/golang/protobuf/&#123;protoc-gen-go,proto&#125; Coding定义proto文件1234567891011121314151617181920212223242526272829303132333435363738syntax = &quot;proto3&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10;&#125;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; User user = 1;&#125; 生成代码生成代码的导入路径和包名 123## protos_golang ： 生成代码的路径## user : golang package 名option go_package = &quot;protos_golang/user&quot;; 目标代码 如果包含rpc接口：则需要指定插件plugins=grpc --go_out=. ： 生成的代码在当前目录; 也可以指定其他目录，如:--go_out=/tmp 代码路径 ： 如果.pb中指定了go_package : 代码路径是 ./$go_package/user.pb.go 如果.pb中没有指定go_package : 则代码路径是 ./pb/user.pb.go 12345protoc --go_out=plugins=grpc:. pb/user.proto# 如果没有rpc定义protoc --go_out=. pb/user.proto 服务端123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( &quot;context&quot; &quot;log&quot; &quot;net&quot; pb &quot;testgrpc/protos_golang/user&quot; &quot;google.golang.org/grpc&quot;)const ( port = &quot;:50000&quot;)type server struct&#123;&#125;func (s *server) GetUserInfo(ctx context.Context, in *pb.UserRequest) (*pb.UserResponse, error) &#123; return &amp;pb.UserResponse&#123; User: &amp;pb.User&#123; Name: &quot;test_user&quot;, &#125;, &#125;, nil&#125;func main() &#123; lis, err := net.Listen(&quot;tcp&quot;, port) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; s := grpc.NewServer() pb.RegisterUserServiceServer(s, &amp;server&#123;&#125;) if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 客户端123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;google.golang.org/grpc&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( address = &quot;localhost:50000&quot;)func main() &#123; conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := pb.NewUserServiceClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.GetUserInfo(ctx, &amp;pb.UserRequest&#123;&#125;) if err != nil &#123; log.Fatalf(&quot;fatal: %v&quot;, err) &#125; log.Printf(&quot;response: %s&quot;, r)&#125; 调试为了方便调试服务端，所以服务端需要支持reflection功能。 1reflection.Register(grpcServer) 两款比较著名的调试工具： [grpc_cli](https://github.com/grpc/grpc/blob/master/doc/command_line_tool.md : 官方的 grpcurl : go的，安装简单 列出服务端注册的service 如果没有配置好公钥和私钥文件，也没有忽略证书的验证过程，则需要加-plaintext 123$ grpcurl -plaintext localhost:50000 list grpc.reflection.v1alpha.ServerReflectionuser.UserService 列出服务的接口 12$ grpcurl -plaintext localhost:50000 list user.UserServiceuser.UserService.GetUserInfo 获取接口的签名 123$ grpcurl -plaintext localhost:50000 describe user.UserService.GetUserInfouser.UserService.GetUserInfo is a method:rpc GetUserInfo ( .user.UserRequest ) returns ( .user.UserResponse ); 获取类型信息 12345$ grpcurl -plaintext localhost:50000 describe .user.UserRequestuser.UserRequest is a message:message UserRequest &#123; uint32 id = 1;&#125; 调试接口 请求体以json的形式描述类型。 123456$ grpcurl -plaintext -d &apos;&#123;&quot;id&quot;:1&#125;&apos; localhost:50000 user.UserService.GetUserInfo&#123; &quot;user&quot;: &#123; &quot;name&quot;: &quot;test_user&quot; &#125;&#125; go gRPC 生态服务组件上下文信息传递 rpc客户端将上下文信息传递给服务端。链路调用信息，服务信息，认证信息等等。 官方实现 服务器反射 服务端反射协议， 可以用途于: 服务端调试 : grpcurl 工具就是用reflection协议来进行服务端调试的。可以list出服务端的接口定义，以及命令行构造请求进行调试。 运行时构造gRPC请求 ：客户端可以运行时根据反射的接口定义构造请求。 官方实现 负载均衡 客户端负载均衡器 官方实现 认证 gRPC主要的两种认证方式： 基于SSL/TLS认证方式 Token认证方式 两种方式可以同时应用 官方实现 实现了几种认证方式： alts google oauth 自定义认证方式 go-grpc-middleware的实现 健康检查 服务端提供一个Check接口返回其状态信息。客户端调用此接口获取到服务健康状态，是否可以继续提供服务。 官方实现 keepalive 定期发送HTTP/2.0 pings帧来检测 connection 是否存活，如果断开则进行重新连接。与健康检查区别在于keepalive是检查connection而健康检查是检查服务是否可用。 官方实现 naming 命名解析。通过服务命名来获取服务相关的信息来达到服务发现目的。 与balancer结合使用来实现进程内负载均衡与服务发现。 官方实现 限流 限制流量来保护服务端以防止服务过载。 可以在客户端，balancer，服务端 进行限流。 go-grpc-middleware实现服务端限流 recovery 将服务内部的错误转换成gRPC错误码。 go-grpc-middleware实现 ： recover go的panic， 并转换成gRPC错误。 重试 客户端对于返回某些gRPC错误码的请求进行重试。 go-grpc-middleware tracing 在链路上下文携带tracing信息，以及将信息以opentracing的规范发送给分布式链路分析服务。 tracing信息包含traceid,spanid,请求时间,错误信息,日志等等。如：通过设置客户端spanid为服务端spanid的parent_spanid，这样就能知道是客户端调用了服务端rpc请求。 go-grpc-middleware实现opentracing的middleware open-tracing 微服务框架、组件go-kit : 微服务组件micro : 微服务框架go-chassis : 华为开发的go微服务框架go-grpc-middleware : 服务端和客户端的一些中间件，认证、日志、分布式追踪跟重试等grpc-gateway ：一个 protoc 的插件，可以将 gRPC 接口转换为对外暴露 RESTful API 的工具，同时还能生成 swagger 文档 gRPC 与 负载均衡进程内LB(Balancing-aware Client) 需要实现： 服务注册 健康检查 服务发现 负载均衡 缺点： 开发成本：要实现上述功能 维护成本：不同语言栈的sdk维护与升级 官方已经提供接口来实现进程内的负载均衡。同时结合服务发现，健康检查一起使用。 集中式LB(Proxy Model) proxy 实现服务发现，健康检查，负载均衡等等。还方便做限流等控制和其他统一控制策略。 缺点： 单点问题 多一层性能开销 不方便调试 Nginx Nginx(1.13.10已经支持gRPC) 1234567891011121314151617181920212223upstream grpcservers &#123; server localhost:50000; server localhost:50001;&#125;server &#123; listen 9000 http2; # router location /user.UserService &#123; grpc_pass grpc://grpcservers; error_page 502 = /error502grpc; &#125; # 将默认错误页面更改成gRPC状态码 location = /error502grpc &#123; internal; default_type application/grpc; add_header grpc-status 14; add_header grpc-message &quot;unavailable&quot;; return 204; &#125;&#125; nginx gRPC module 独立LB进程(External Load Balancing Service) 在主机上部署独立的LB进程，来实现服务发现，健康检查，负载均衡等功能。不用对于不同语言维护不同sdk版本；常常用于微服务service mesh。 缺点： 单点问题：但是只影响本机 不方便调试 常用的组件： Istio Envoy gRPC 生态环境组件 grpc 只是实现了 RPC 核心功能，缺少很多微服务的特性（服务注册发现、监控、治理、管理等），而基于 HTTP/2 相对来说比较容易进行扩展。 grpc-ecosystem 上有一些比较优秀的外围组件来完善gRPC的生态体系 awesome-grpc 收集了一些优秀的gRPC项目 grpc 文档与交流文档 官网文档 : https://grpc.io/docs/ github 上 grpc 仓库下的 doc ： https://github.com/grpc/grpc/tree/master/doc 博客 : https://grpc.io/blog/ 交流 https://grpc.io/community/ 交流的方式有： 邮件列表 Gitter Reddit Meetup Group 参考grpc.io developers.google.com gRPC github doc http2 specs or github http2 spec]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NUMA]]></title>
    <url>%2F2018%2F04%2F12%2Fnuma%2F</url>
    <content type="text"><![CDATA[NUMA 概念、历史、问题 NUMA 概念NUMA的几个概念（Node，socket，core，thread） socket就是主板上的CPU插槽; core就是socket里独立的一组程序执行的硬件单元，比如寄存器，计算单元等; thread：就是超线程hyperthread的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。 NUMA体系结构中多了Node的概念，这个概念其实是用来解决core的分组的问题，具体参见下图来理解（图中的OS CPU可以理解thread，那么core就没有在图中画出），从图中可以看出每个Socket里有两个node，共有4个socket，每个socket 2个node，每个node中有8个thread，总共4（Socket）× 2（Node）× 8 （4core × 2 Thread） = 64个thread。 另外每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量，因为Node内有自己内部总线，所以增加CPU数量可以通过增加Node的数目来实现，如果单纯的增加CPU的数量，会对总线造成很大的压力，所以UMA结构不可能支持很多的核。 《此图出自：NUMA Best Practices for Dell PowerEdge 12th Generation Servers》 根据上面提到的，由于每个node内部有自己的CPU总线和内存，所以如果一个虚拟机的vCPU跨不同的Node的话，就会导致一个node中的CPU去访问另外一个node中的内存的情况，这就导致内存访问延迟的增加。在有些特殊场景下，比如NFV(Network Function Virtualization)环境中，对性能有比较高的要求，就非常需要同一个虚拟机的vCPU尽量被分配到同一个Node中的pCPU上，所以在OpenStack的Kilo版本中增加了基于NUMA感知的虚拟机调度的特性。 查看机器的NUMA拓扑结构123456789101112131415161718192021222324[root@local ~]$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 48 // 48个逻辑CPU（threads）On-line CPU(s) list: 0-47Thread(s) per core: 2 // 每个core有2个threadsCore(s) per socket: 12 // 每个socket有12个coresSocket(s): 2 // 共总有2个socketsNUMA node(s): 2 // 2个NUMA nodesVendor ID: GenuineIntelCPU family: 6Model: 63Model name: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHzStepping: 2CPU MHz: 2500.089BogoMIPS: 4999.27Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 30720KNUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47 可以看出当前机器有2个sockets，每个sockets包含1个numa node，每个numa node中有12个cores，每个cores包含2个thread，所以总的threads数量=2x1x12x2=48. NUMA 历史在若干年前，对于x86架构的计算机，那时的内存控制器还没有整合进CPU，所有内存的访问都需要通过北桥芯片来完成。此时的内存访问如下图所示，被称为UMA（uniform memory access, 一致性内存访问）。这样的访问对于软件层面来说非常容易实现：总线模型保证了所有的内存访问是一致的，不必考虑由不同内存地址之前的差异。 之后的x86平台经历了一场从“拼频率”到“拼核心数”的转变，越来越多的核心被尽可能地塞进了同一块芯片上，各个核心对于内存带宽的争抢访问成为了瓶颈；此时软件、OS方面对于SMP多核心CPU的支持也愈发成熟；再加上各种商业上的考量，x86平台也搞了NUMA（Non-uniform memory access, 非一致性内存访问）。 NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die(核心)上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）。所以NUMA（Non-Uniform Memory Access）就此得名 在这种架构之下，每个Socket都会有一个独立的内存控制器IMC（integrated memory controllers, 集成内存控制器），分属于不同的socket之内的IMC之间通过QPI link通讯。 然后就是进一步的架构演进，由于每个socket上都会有多个core进行内存访问，这就会在每个core的内部出现一个类似最早SMP架构相似的内存访问总线，这个总线被称为IMC bus。 于是，很明显的，在这种架构之下，两个socket各自管理1/2的内存插槽，如果要访问不属于本socket的内存则必须通过QPI link。也就是说内存的访问出现了本地/远程（local/remote）的概念，内存的延时是会有显著的区别的。 以Xeon 2699 v4系列CPU的标准来看，两个Socket之之间通过各自的一条9.6GT/s的QPI link互访。而每个Socket事实上有2个内存控制器。双通道的缘故，每个控制器又有两个内存通道（channel），每个通道最多支持3根内存条（DIMM）。理论上最大单socket支持76.8GB/s的内存带宽，而两个QPI link，每个QPI link有9.6GT/s的速率（~57.6GB/s）事实上QPI link已经出现瓶颈了。 核心数还是源源不断的增加，Skylake桌面版本的i7 EE已经有了18个core，Skylake Xeon 28个Core(2017)。为了塞进更多的core，原本核心之间类似环网的设计变成了复杂的路由。由于这种架构上的变化，导致内存的访问变得更加复杂。两个IMC也有了local/remote的区别，在保证兼容性的前提和性能导向的纠结中，系统允许用户进行更为灵活的内存访问架构划分。于是就有了“NUMA之上的NUMA”这种妖异的设定（SNC）。 性能提升内核调度和操作方式 在一个启用了NUMA支持的Linux中，Kernel不会将任务内存从一个NUMA node搬迁到另一个NUMA node。 一个进程一旦被启用，它所在的NUMA node就不会被迁移，为了尽可能的优化性能，在正常的调度之中，CPU的core也会尽可能的使用可以local访问的本地core，在进程的整个生命周期之中，NUMA node保持不变。 一旦当某个NUMA node的负载超出了另一个node一个阈值（默认25%），则认为需要在此node上减少负载，不同的NUMA结构和不同的负载状况，系统见给予一个延时任务的迁移——类似于漏杯算法。在这种情况下将会产生内存的remote访问。 NUMA node之间有不同的拓扑结构，各个 node 之间的访问会有一个距离（node distances）的概念，如numactl -H命令的结果有这样的描述： 123456789101112[root@local ~]$ numactl -Havailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73363 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117527 MBnode distances:node 0 1 0: 10 21 1: 21 10 可以看出：0 node 到0 node之间距离为10，是最近的距离。 上图记录了某个Benchmark工具，在开启/关闭NUMA功能时QPI带宽消耗的情况。很明显的是，在开启了NUMA支持以后，QPI的带宽消耗有了两个数量级以上的下降，性能也有了显著的提升！ 通常情况下，用户可以通过numactl来进行NUMA访问策略的手工配置，cgroup中cpuset.mems也可以达到指定NUMA node的作用。 Numa内存分配策略有四种: 缺省default:总是在本地节点分配(当前进程运行的节点上)。 绑定bind:强制分配到指定节点上。 交叉interleavel:在所有节点或者指定节点上交叉分配内存。 优先preferred:在指定节点上分配，失败则在其他节点上分配 以numactl命令为例，它有如下策略： –interleave=nodes //允许进程在多个node之间交替访问 –membind=nodes //将内存固定在某个node上，CPU则选择对应的core。 –cpunodebind=nodes //与membind相反，将CPU固定在某（几）个core上，内存则限制在对应的NUMA node之上。 –physcpubind=cpus //与cpunodebind类似，不同的是物理core。 –localalloc //本地配置 –preferred=node //按照推荐配置 对于某些大内存访问的应用，比如Mongodb，将NUMA的访问策略制定为interleave=all则意味着整个进程的内存是均匀分布在所有的node之上，进程可以以最快的方式访问本地内存。北桥有一个功能就是PCI/PCIe控制器，南桥（PCH）整合了PCIe控制器。在PCIe channel上也是有NUMA亲和性的。 比如：查看网卡em1的NUMA 12345678[root@local ~]$ numactl --prefer netdev:em1 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 PCI address 为00:1f.2的SATA控制器，用到了pci:00:1f.2 SATA controller: Intel Corporation C610/X99 series chipset 6-Port SATA Controller [AHCI mode] (rev 05) 1234567[root@local ~]$ numactl --prefer pci:00:1f.2 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前系统numa策略： 1234567[root@local ~]$ numactl --showpolicy: defaultpreferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前numa的节点情况： 123456789101112[root@local ~]$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73338 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117521 MBnode distances:node 0 1 0: 10 21 1: 21 10 NUMA带来的问题 MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases 这些问题都是：“因为CPU亲和策略导致的内存分配不平均”及“NUMA Zone Claim内存回收”有关，而和数据库种类并没有直接联系。 数据库与NUMAMySQL在NUMA架构上遇到的典型问题 The MySQL “swap insanity” problem and the effects of the NUMA architecture A brief update on NUMA and MySQL 大致分析如下： CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核CPU通过一条总线共享内存成为瓶颈 于是NUMA出现了，CPU平均划分为若干个Chip（不多于4个），每个Chip有自己的内存控制器及内存插槽 CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存（下文称Remote Access）的速度相较慢三倍左右 于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中 由于内存页没有动态调整策略，使得大部分内存页都集中在CPU 0上 又因为Reclaim默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出 当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现了 解决方案： numactl –interleave=all 在MySQL进程启动前，使用sysctl -q -w - vm.drop_caches=3清空文件缓存所占用的空间 Innodb在启动时，就完成整个Innodb_buffer_pool_size的内存分配 不过这种三合一的解决方案只是减少了NUMA内存分配不均，导致的MySQL SWAP问题出现的可能性。如果当系统上其他进程，或者MySQL本身需要大量内存时，Innodb Buffer Pool的那些Page同样还是会被Swap到存储上。于是又在这基础上出现了另外几个进阶方案 配置vm.zone_reclaim_mode = 0使得内存不足时去remote memory分配优先于swap out local page echo -15 &gt; /proc//oom_adj调低MySQL进程被OOM_killer强制Kill的可能 memlock 对MySQL使用Huge Page（黑魔法，巧用了Huge Page不会被swap的特性） 为什么Interleave的策略就解决了问题？借用两张 Carrefour性能测试 的结果图，可以看到几乎所有情况下Interleave模式下的程序性能都要比默认的亲和模式要高，有时甚至能高达30%。究其根本原因是Linux服务器的大多数workload分布都是随机的：即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分布的。而Interleave模式就恰恰是针对这种特性将内存page随机打散到各个CPU Core上，使得每个CPU的负载和Remote Access的出现频率都均匀分布。相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多。 也就是说，像MySQL这种外部请求随机性强，各个线程访问内存在地址上平均分布的这种应用，Interleave的内存分配模式相较默认模式可以带来一定程度的性能提升。此外各种论文 中也都通过实验证实，真正造成程序在NUMA系统上性能瓶颈的并不是Remote Acess带来的响应时间损耗，而是内存的不合理分布导致Remote Access将interconnect这个小水管塞满所造成的结果。而Interleave恰好，把这种不合理分布情况下的Remote Access请求平均分布在了各个小水管中。所以这也是Interleave效果奇佳的一个原因。 那是不是简简单单的配置个Interleave就已经把NUMA的特性和性能发挥到了极致呢？答案是否定的，目前Linux的内存分配机制在NUMA架构的CPU上还有一定的改进空间。例如：Dynamic Memory Loaction, Page Replication。 Dynamic Memory RelocationMySQL的线程分为两种，用户线程（SQL执行线程）和内部线程（内部功能，如：flush，io，master等）。对于用户线程来说随机性相当的强，但对于内部线程来说他们的行为以及所要访问的内存区域其实是相对固定且可以预测的。如果能对于这把这部分内存集中到这些内存线程所在的core上的时候，就能减少大量Remote Access，潜在的提升例如Page Flush，Purge等功能的吞吐量，甚至可以提高MySQL Crash后Recovery的速度（由于recovery是单线程）。那是否能在Interleave模式下，把那些明显应该聚集在一个CPU上的内存集中在一起呢？很可惜，Dynamic Memory Relocation这种技术目前只停留在理论和实验阶段。我们来看下难点：要做到按照线程的行为动态的调整page在memory的分布，就势必需要做线程和内存的实时监控（profile）。对于Memory Access这种非常异常频繁的底层操作来说增加profile入口的性能损耗是极大的。 Page Replication一些动态加载的库，把他们放在任何一个线程所在的CPU都会导致其他CPU上线程的执行效率下降。而这些共享数据往往读写比非常高，如果能把这些数据的副本在每个Memory Zone内都放置一份，理论上会带来较大的性能提升，同时也减少在interconnect上出现的瓶颈。由于缺乏硬件级别（如MESI协议的硬件支持）和操作系统原生级别的支持，Page Replication在数据一致性上维护的成本显得比他带来的提升更多。因此这种尝试也仅仅停留在理论阶段。当然，如果能得到底层的大力支持，相信这个方案还是有极大的实际价值的。 关闭NUMA特性的方法 硬件层，在BIOS中设置关闭 OS内核，启动时设置numa=off 进程，numactl 进程启动时。numactl –interleave=all NUMA取舍指定numa在运行程序的时候使用numactl -m和-physcpubind就能制定将这个程序运行在哪个cpu和哪个memory中:numactl –physcpubind=2,6 ./program 玩转cpu-topology(站点已经无法访问) 的测试中显示当程序只使用一个node资源和使用多个node资源的比较表（差不多是38s与28s的差距）。所以限定程序在numa node中运行是有实际意义的。 指定numa带来的问题SWAP的罪与罚 文章就说到了一个numa的陷阱的问题。现象是当你的服务器还有内存的时候，发现它已经在开始使用swap了，甚至已经导致机器出现停滞的现象。如果一个进程限制它只能使用自己的numa节点的内存，那么当自身numa node内存使用光之后，就不会去使用其他numa node的内存了，会开始使用swap，甚至更糟的情况，机器没有设置swap的时候，可能会直接死机！所以你可以使用numactl --interleave=all来取消numa node的限制。 根据具体业务决定NUMA的使用: 如果你的程序是会占用大规模内存的，你大多应该选择关闭numa node的限制。因为这个时候你的程序很有几率会碰到numa陷阱。 如果你的程序并不占用大内存，而是要求更快的程序运行时间。你大多应该选择限制只访问本numa node的方法来进行处理。 推荐阅读: NUMA-aware scheduler for Go PostgreSQL, NUMA and zone reclaim mode on linux NUMA and Java Databases MySQL Server and NUMA architectures]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What Is the Most Important Thing in Life？]]></title>
    <url>%2F2018%2F01%2F01%2Fwhich-most-important%2F</url>
    <content type="text"><![CDATA[the most important thing in life. the most important thing in life. Keeping healthy. It is health that is real wealth and not pieces of gold and silver. Everybody needs somebody, be that a friend, a partner, or someone you’re related to. Making someone’s day full of sunshine even when yours is not. Money should not be a priority. The beautiful thing about learning is that nobody can take it away from you. Know who you are. Don’t be a victim BUT instead be a hero in your life. Don’t give up. A winner is just a loser who tried on more time. Always make time for gratitude.]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>人生</tag>
      </tags>
  </entry>
</search>
