<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据库并发控制理论]]></title>
    <url>%2F2022%2F07%2F08%2Fdatabase_concurrency_control%2F</url>
    <content type="text"><![CDATA[概述 并发控制技术，是数据库事务实现的基石，在确保事务隔离性正确的前提下，尽可能提高事务的并发度。 广义上看，并发控制属于事务调度，调度的种类非常多，串行化、可串行化、不可恢复性等等；在这里，我们更多从狭义上来讲调度，指可串行化的调度。 事务的正确性主要体现在ACID特性上，而并发控制主要涉及其中的I即Isolation，即事务隔离性，避免脏读，幻读，写偏斜等读写异常。且满足事务的可恢复性属性。 为了提高事务的并发度，则ISO定义了几种不同的隔离级别，让数据库在不同隔离级别下提供不同的正确性保证，在并发度和正确性之间取舍。 本文主要描述关系数据库的并发控制理论，不会过多涉及MySQL等数据库的实现细节，避免局限于这些数据库的具体实现。 理论 实现方向 并发控制技术，从检测是否存在冲突的角度看，包括乐观的，悲观的，处于两者之间的半乐观方法 乐观（optimistic concurrency control，OCC）：从一开始，每一项操作都允许进行，但在事务提交的时刻，进行隔离性和完整性约束的检查，如果有违反则事务被回滚 悲观（pessimistic concurrency control，PCC）：从一开始，即检查每一项操作是否会违反隔离性和完整性约束，如果可能违反，则阻塞这样的操作 从控制冲突的时机看，包括基于锁的，基于时间的，基于提交顺序的，基于串行化图测试检验的各种方法，还有基于多版本并发控制的技术。 串行化的含义是完全限制并发；可串行化是在能保证一致性的情况下，允许某些并发的操作被执行；以提高数据库整体的运行效率。数据确保事务特性的前提下，还要在不同的调度下能满足事务的属性：可串行化(serializability)，可恢复性(recoverability)。然而不是所有的调度都是满足事务属性的，有的调度可以牺牲可串行化或者可恢复性，以获得更高的并发度。 可串行化 Serial schedule 要想事务不互相影响，那么最简单的方式就是让事务的执行是串行的，不交叉执行，称这种执行调度为serial schedule，串行化调度（也有翻译称序列化调度的）。 然而多个事务在这种调度下是无法并发执行的，那么执行效率就非常低下。数据库要能够并发执行，就不能使用串行化的调度策略，但是又要保证性能和隔离性，也就是等价于serial schedule，那么我们称这种调度策略为serializable schedule，可串行化调度。 那么，如何理解"等价于"呢？也就是事务执行的正确性？ 数据库系统中，将"等价"分为三种 final state equivalence终态等价 view equivalence视图等价 conflict equivalence冲突等价 所以按照这三种等价于serial schedule的调度也分为三种： final state serializability终态串行化 view serializability视图串行化 conflict serializability冲突串行化 终态可串行化 终态可串行化，final state serializability，又称为result equivalence serializability。 如果两个调度的执行结果一样，则认为两个调度是final state equivalent的，如果调度的执行结果和serial schedule一样，则是final state serializability的。就像两个函数，输入的参数一样，如果返回的结果也一样的话，则两者是final state equivalent的。其忽略了如何实现函数的内部结构，把函数当成黑盒了。 那如何判断两个调度是否为final state equivalent的呢？把两个调度各自执行一遍？显然这是不可能的，也有研究将调度操作转换成有向图，但要确保正确性就需要将调度的所有操作（读、写和控制流）都加入到有向图中，那和执行一遍调度几乎没有什么区别。 所以final state serializability对于现实工程实现并没有什么指导意义。 冲突可串行化 冲突可串行化， conflict serializability 冲突行为 当下面三种条件都满足时，我们将两个操作视为冲突 两个操作属于不同的事务 两个操作访问和处理的数据集有重叠 至少有一个操作的是写操作 从定义中，我们可以将不同事务并发操作同一数据产生的冲突分为三类 R-W冲突：事务Ti读取了数据O但还没提交，事务Tj修改了O，（造成不可重复读） W-R冲突：事务Ti修改了数据O但还没提交，事务Tj读取了O，（造成读未提交） W-W冲突：事务Ti修改了数据O但还没提交，事务Tj也修改了O，（造成丢失更新） conflict equivalence 如果某个调度能通过交换非冲突操作来将调度转换成serial schedule，则两者是conflict equivalent的，则称这种调度为conflict serializability。 例如： 事务的执行 12T1: R(A), W(A), R(B), W(B)T2: R(A), W(A), R(B), W(B) 可能的serial schedules： 1234567891011121314151617181920// T1-&gt;T2T1 T2R1(A)W1(A)R1(B)W1(B) R2(A) W2(A) R2(B) W2(B)-------------------------// T2-&gt;T1 R2(A) W2(A) R2(B) W2(B)R1(A)W1(A)R1(B)W1(B) 现在我们通过交换非冲突操作来达到conflict serializability 12345678910// S1 T1 T2R(A)W(A) R(A) &lt;--- W(A) &lt;------R(B) &lt;--- W(B) &lt;------ R(B) W(B) T1的R(B), W(B) 和 T2的R(A), W(A)是非冲突的，所以可以将他们进行交换。 通过两次交换，可以将S1转换成 T1-&gt;T2的serial schedule，所以S1是 conflict serializability。 再看看不是conflict serializability的例子 12345678910// S2 T1 T2 R(A) W(A)R(A)W(A)R(B)W(B) R(B) W(B) 可以看出，T1对A和B的操作，都没有办法交换到T2的前面或后面。所以S2不是conflict serializability。 优先图检测冲突可串行化 然而对两三个事务进行判断conflict serializability相对还比较简单，但是对很多事务要进行判断却是非常困难的，那要如何检测呢？ 我们以事务为单位，使用事务之间的冲突行为定义事务执行的先后顺序。 如Ti的Ri(A)与Tj的Wj(A)冲突，而Ri(A)&lt;Wj(A)，所以Ti就要先于Tj执行，所以就可以使用优先图来检测冲突可串行化。而如果优先图(precedence graph)没有环，则说明调度是冲突可串行化的。因为如果Ti需要在Tj前执行，又有Tj需要在Ti前执行，显然这是矛盾的，所以存在环则不是冲突可串行化的。 1234567T1 T2R(A)W(A) R(A) R(B)R(B)W(B) 可以看出，T1先写了A，T2后读了A，所以wr冲突，T1优先于T2；而T2读取了B，后面T1又修改了B，那么rw冲突，T2优先于T1。所以形成了环，说明不是冲突可串行化的。 image 实际上，这只是一种研究的理论方向，从现有主流并发控制技术来看，基于锁的并发控制实现，将冲突操作通过锁的方式来互斥，就这天然的阻止了冲突操作中环的形成。 视图可串行化 视图可串行化, view serializability。 《concurrency control and recovery in database systems》定义两个schedule S1和 S2是 view-equivalent的，只有满足： S1和S2的transactions相同。如果在S1中，T1最先读取A，则在S2中，也必须是T1最先读取A。 读写依赖：对于S1 和 S2中的Ti和Tj，如果在S1中，Oi 从 Oj中读取，那么在S2中，Oi也从Oj中读取。Tj先写了O，而Ti后读取O，Wj(O) &lt; Ri(O) 最后写(final write)：如果在S1中Ti最后写入O，那么S2中也是Ti最后写入O。 读写 两种调度对同一对象的读写顺序必须一致，其实也就是其冲突操作是要一致的，和conflict equivelance一样。 123456 S1 S2------------------- ----------------T1 T2 T3 T1 T2 T3 W(A) W(A) W(A) R(A) R(A) W(A) 如上面两个调度就不是view-equivalent的，S1中，T3读取A的值是由T2更新的，而S2中T3读取的A是由T1更新了。 最后写 如果S1中事务Ti最后修改了数据O，则在S2最后修改数据O的也是事务Ti。 123456 S1 S2------------------- ---------------- T1 T2 T1 T2 R(A) R(A) W(A) W(A) W(A) W(A) 如上，S1和S2不是view-equivalence的，因为S1最后写A的是T1，而S2中最后写A的是T2。 从上面可以看出，view-equivalence和conflict-equivalence不一样的是，view-equivalence增加了final write的条件，这样相对于conflict-equivalence，反而约束宽松了。 如下调度 12345T1 T2 T3R(A) W(A)W(A) W(A) 由于此调度的优先图存在环(T1的R(A)或W(A)与W(A)也无法交换)，所以调度不是conflict serializability的。 但T3的W(A)在最后写（盲写blind write），所以和serial schedule(T1-&gt;T2-&gt;T3) 是view-equivalent的。 可以看出，优先图存在一定的局限性，虽然调度不是conflict serializability的，却可能是view serializability的，其实conflict serializability是view serializability的一个子集。 但现在仍没有一种高效的方法来判断调度是否为view serializability的，所以view serializability没有实用价值。 可恢复性 可恢复性的定义如下： Recoverability means that committed transactions have not read data written by aborted transactions（whose effects do not exist in the resulting database states）。 即：已经提交的事务没有读过被中止的事务写的数据。否则脏读异常发生，导致数据不一致。 其实可恢复性对事务回滚进行了定义，所以和可串行性一起保证事务正确执行 可串行性：当所有事务提交时，调度执行的顺序和串行执行的结果一样 可恢复性：当有事务回滚时，不会存在由于脏读导致数据不一致。 下面调度，是可串行化的，T1在T2前执行，但T2先提交，所以不满足可恢复性。 可以想象如果发生崩溃，T2的提交记录到达磁盘，而T1没有到达，那么恢复后，T2都处于提交状态，而T1会回滚。C指Commit。 1234567T1 T2W(A)W(B) W(A) R(B) CC 级联回滚 由于允许脏读（读取还未提交事务修改的数据），所以如果一个更新某些数据的事务回滚，则其他读取了这些数据的事务也会导致回滚。 如下，T2读取了T1更新的值，T3读取了T2更新的值，那么T1回滚，为了避免数据不一致，所以T2和T3必须回滚。A指Abort。 1234567891011T1 T2 T3R(X)W(X) R(X) W(X) R(X) W(X)A(X) A(X) A(X) 所以级联回滚对性能影响很大，应该尽量避免。 避免级联回滚 cascadeless schedule，又称为avoiding cascading aborts (ACA)，避免级联回滚。 其定义如下： A strategy to prevent cascading aborts is to disallow a transaction from reading uncommitted changes from another transaction in the same schedule. 既然脏读导致了级联回滚，那么不允许脏读就可以避免级联回滚了。也就是只允许读取已经提交事务修改的数据。 如以下调度，必须等到T1完成提交，T2才能读取A 1234567T1 T2R(A)W(A)C R(A) W(A) C 严格性 严格性strictness A schedule is strict - has the strictness property - if for any two transactions T1, T2, if a write operation of T1 precedes a conflicting operation of T2 (either read or write), then the commit or abort event of T1 also precedes that conflicting operation of T2. 避免级联回滚虽然不允许脏读，但是忽略了一个问题，就是丢失更新。不允许读取未提交事务修改的数据，没有不允许修改未提交事务修改的数据。所以严格性就对此做了限制，即不允许读取或修改未提交事务修改过的数据。 如以下调度就是ACA的，但不是strictness的 12345T1 T2R(A)W(A) W(A) // 修改了T1修改的AC 为什么不允许写未提交事务修改的数据呢？ 我们可以想象以下场景： 1234567T1 T2W(x, 1); W(y, 11); W(y, 22); A; R(x); A T1 abort时，将y从11改成1(称为before image前像)， 但当T2 abort时，T2其执行W2(y, 22)的前像是11，T2如果简单回滚到前像，则会出错。 这样就增加了回滚复杂度。 属性关系 我们引用维基百科的一张图来描述属性之间的关系 image 从串行化的角度看，严格程度的次序(前松后紧)： 1all schedules -&gt; view serializable -&gt; conflict serializable -&gt; serial 从可恢复性的角度看，严格程度的次序： 1all schedules -&gt; recoverable -&gt; avoids cascading aborts -&gt; strictness -&gt; serial 基于锁的并发控制 基于锁的并发控制相关知识有很多，如 锁的粒度，tuple，page，table等等 锁的类型：排他锁，共享锁，意向锁 锁的合并与升级 锁的管理：死锁检测、预防等 不打算在本文中来聊这些技术实现，因为涉及到的内容实在太多了， 如死锁预防，相关手段就非常多，每一种都是一个细化的研究方向。 本文只关注如何通过锁来实现可串行化的并发控制，至于其他工程优化手段等，以后再写专门的文章来讨论吧。 如果锁是普通的加锁与解锁，我们看看会有什么问题 12345678910111213141516// A = 20, B = 20T1 T2Lock(A) A=A-10Unlock(A) Lock(B) Read(B) Unlock(B) Lock(A) Read(A) Unlock(A) echo A+B // 30Lock(B)B=B+10Unlock(B) 如果是序列化执行，不管T1先执行还是T2先执行，那么T2得出的A+B结果都是40。 而此调度会造成T2的结果是30，显然不是可序列化的。 2PL 2PL，two-phase locking，两阶段封锁协议。 为了确保可串行化，所以引入两阶段锁。 将事务的获取锁和释放锁分成了增长(growing)和缩减(shrinking)两个不同的阶段 增长阶段：每个事务请求所有需要的锁资源，此阶段不允许释放任何锁 缩减阶段：事务进入释放锁的阶段，不允许再对资源进行加锁 2PL是能够保证冲突可串行化的，所以能满足可串行化。 可以看到下面调度的事务执行，结果和串行化执行结果是一样的。 1234567891011121314151617181920212223// A = 20, B = 20T1 T2Begin BeginLock(A) Lock(B)A=A-10B=B+10Unlock(A)Unlock(B) Lock(B) Read(B) Lock(A) Lock(Z) Read(A) Z=A+B // 40 Unlock(B) Unlock(A) commit commit 但是如果T1的回滚了，则导致T2也必须回滚，不然就产生脏读。 所以如上所说，虽然保证了可串行化，但不满足可恢复性，故而不能避免级联回滚。 S2PL 为了避免上面的问题，所以很多现代数据库使用S2PL或SS2PL来实现并发控制。 strict two-phase locking严格两阶段封锁。 除了封锁满足两阶段封锁条件之外，还要求持有的排它锁必须在事务提交后才能释放。这个要求保证未提交事务所写的任何数据在该事务提交之前均以排它方式加锁，从而能够避免级联回滚。 遵循严格封锁调度有两个特性 符合ACA的也符合strictness，因为由于锁的存在无法读取或修改其他未提交事务修改的值 是可串行化的 可以看到下面的例子，只有当T1 Commit后，才会释放B的写锁， 所以T2要等到T1 Commit后，才能获取到B的读锁；假设T1回滚，也不会造成T2回滚，这样就避免了级联回滚了。 12345678910111213141516171819202122// A = 20, B = 20T1 T2Begin BeginR-Lock(A) W-Lock(B) R-Lock(A) Read(A)B=A+10R-Unlock(A)CommitW-Unlock(B) R-Lock(B) Read(B) W-Lock(Z) Z=A+B R-Unlock(B) R-Unlock(A) Commit W-Unlock(Z) SS2PL 强严格两阶段锁，strong strict two-phase locking，也称为rigorous 2PL。 除了封锁满足两阶段之外，还要求事务提交之前不得释放任何锁。 SS2PL降低了并发度，但带来的好处是 不像S2PL需要跟踪每个事务的锁的阶段，整个事务提交前都是两阶段增长阶段，提交后才是缩减阶段，这样实现起来就很简单了 且保证了提交顺序，CO(commitment ordering)，以后再聊CO 隔离级别 我们再看看如何基于锁并发控制实现不同的隔离级别 隔离级别 读操作 写操作 范围操作 读未提交 S S S 读已提交 S C S 可重复读 C C S 可串行化 S S S S表示操作前加锁，操作后释放锁 C表示操作前加锁，提交后释放锁 通过这些锁的策略，则可以通过锁来实现各种隔离级别。 例如读已提交：写操作在提交后再释放，这样就阻塞读请求了，从而避免读到未提交的数据。 多版本并发控制 multi-version concurrency control (MVCC) 是一个比较宽泛的概念，不仅仅指并发控制。 核心思想是对每个对象的修改都会产生一个新的版本，可以读取任意存在的版本，对所有对象进行版本的管理控制。 这样的好处是修改操作和读历史版本的读操作各自不影响对方，可以并行操作。 对于数据库操作冲突来说，读写冲突（快照）是可以通过MVCC机制来避免的，而写写冲突则可以通过加锁的方式来避免并发修改，从而保证正确性。 设计MVCC的几个主要的考虑方向: 版本存储：version storage 垃圾回收：garbage collection 索引管理：index management 版本存储 数据库中更新一条记录，就会对这条记录产生一个新的版本，通过指针将不同的版本记录链接起来，组成版本链。这样，数据库就可以通过版本链找到不同版本的记录。 版本的组织方式有以下几种 append-only storage追加方式：新版本直接追加写入到同一个表空间中 time-travel storage时间线方式：老版本单独存储在一个表空间中 delta storage增量存储方式：只存储修改部分的数据，而不是整个元组 元组（tuple）：指的是数据库存储引擎中的一条有版本信息的记录。 追加方式 append-only storage 版本链的指向顺序有两种方式 老版本的指针指向新版本记录 oldest-to-newest(O2N) 新版本的指针指向老版本记录 newest-to-oldest(N2O) O2N 生成新版本时，将老版本的指针指向新版本，所以访问这个新记录，需要遍历老记录，这样就会产生读放大，且生成新纪录时需要更新老记录的指针。 N2O 生成新版本时，将新版本的指针指向老版本，所以访问某个版本的记录时，需要从新记录开始遍历到指定版本。 图来源于论文：《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》 时间线方式 time-travel storage 将老版本直接copy到time-travel表，新版本写入主表，新版本指向老版本。 索引指向主表即可，通过主表中最新版本的指针去遍历老版本。 time-travel也是append-only的存储方式，只是新老版本在不同的表空间中。这种方式有利于回收老版本记录，但同时产生了写放大。 image 增量存储方式 delta storage 只存储修改部分的数据，而不是整个tuple。由于不需要copy整个tuple，所以更快速。 而读取的时候需要组装数据，所以更慢。 image 垃圾回收 当所有运行中的事务都不会再读取到一条记录的某个版本时，则这个版本是可回收的。 如所有运行的事务中，最小可见的版本号是11，如果一条记录的最新版本号是6，那么这条记录的小于6的版本都是可以回收的。 一般有两种回收方式 tuple-level元组级别：以元组为粒度进行回收过期记录。 transaction-level事务级别：以事务为粒度回收过期记录。 元组级别的回收 两种回收方式 后台清理的方式background vacuuming：后台线程定期进行清理 合作式回收cooperative cleaning：工作线程来清理 后台清理 Vacuum线程扫描到版本比他TS小的tuple，vacuum的TS应该选择比现在running最小的transactions还小的TS。 TS为自增版本号，每个事务启动时分配一个此TS作为其可见性判断。 合作式回收 工作线程执行事务操作时，扫描到过期不可见的版本记录则直接删除，这样就不再需要单独的后台线程来定期清理了，但如果某条记录一直没有被事务扫描到，则永远无法被清理。 混合式 使用合作式回收，然而定期再用后台清理方式，来避免有的数据没有被读取则无法回收的问题。 事务级别的回收 事务维护一个读集合和写集合，分别存储其读到的记录和创建的记录，当事务结束后，再从两个集合中找出对于运行中事务不再可见的记录，将他们删除掉。 索引管理 索引管理是指数据库中的索引，如何指向实际的主表数据？ 有两种方式 逻辑指针 logical pointer 物理指针 physical pointer image 逻辑指针 索引指向一个"中间指针"即逻辑指针，这个中间指针再指向主表存储的元组的位置(某个页面的某个位置)，如图所示。 这种方式对于写比较友好，例如，如果更新某条记录，则这条记录相关的所有索引都不需要更新，只需要更新"中间指针"指向新的元组的位置即可。 但是存在读放大问题，所有索引访问数据都需要先访问"中间指针"，再跳转到实际数据存储位置。 MySQL InnoDB就是使用逻辑指针的方式，所有索引都指向主键，通过主键再去访问真实的数据。 物理指针 所有索引都指向主表存储的元组的真实位置。 这种方式和逻辑指针刚好相反，对于读比较友好，索引指向元组的实际位置，直接就可以访问到元组，无需通过中间指针进行跳转。 但不利于写，如果更新了某条记录的位置，则相关的索引都需要更新，造成写放大。 PostgreSQL使用这种方式，所以更新记录时成本较高。 隔离级别 MVCC不是单独可以使用的技术，需要配合其他并发控制技术一起来实现不同的隔离级别，如S2PL。 那我们来看看MVCC+S2PL如何实现不同的隔离级别。 事务号 数据有多个版本由不同的版本号区分，这种版本号就是一个事务号，是一个单调递增的数字。 将事务分成两种类型：只读事务和更新事务。 如果是只读事务：则在事务开始时获取一个事务号，读取数据时，就读取比这个事务号小的版本号的数据。这种读取是不用加锁的。 如果是更新事务： 读操作：获取共享锁，读取最新版本的值 写操作：获取排它锁，为写的数据创建一个新版本，版本号为无穷大(版本号类型的最大值，当然实际数据库很少这样实现的，考虑到崩溃恢复和持久化，实际数据库实现的版本控制和可见性判断远比这复杂，属于工程上的优化，目的是一样的)，这样其他事务根据其版本号就无法读取到这条记录，提交时，重新获取事务号，将此写入的数据的版本号改为此事务的事务号+1。 所以MVCC天然就不会读取到未提交数据。 读已提交 每次读都重新获取一次快照，读取最新已提交数据。 可重复读 第一次执行读操作时生成快照，后面的读都以此快照进行读取，这样每次读取的版本都是一样的。 如果是SELECT ... FOR UPDATE或DML语句，需要用排它锁锁住需要读写操作的数据直到数据结束。 可串行化 可串行化用SS2PL来实现。 这样，通过MVCC+S2PL实现了数据库的不同隔离级别。 基于可串行化快照隔离的并发控制 快照 快照snapshot 数据库中数据和状态的某一版本（可以认为只要哪怕有一个数据修改，数据库就会产生一个新版本）。 快照隔离 快照隔离 snapshot isolation snapshot isolation is a guarantee that all reads made in a transaction will see a consistent snapshot of the database (in practice it reads the last committed values that existed at the time it started), and the transaction itself will successfully commit only if no updates it has made conflict with any concurrent updates made since that snapshot. 事务像是被"隔离"起来了，对同一数据的操作互相不影响；快照隔离只是一种技术，而不是隔离级别， 快照隔离（后面同一写成SI）技术是MVCC技术的一种实现，所以使用SI的读操作，读到的同一份快照的数据一定是一样的。 SI读取的是数据的某一快照，所以不会发生读写冲突或写读冲突。 这样就避免了幻读异常，不可重复读，脏读，但是SI无法阻止写偏序，所以SI并不是可串行化的。 由于SI是MVCC的一种实现，所以避免异常的实现和前面的MVCC的一样，但是没有锁的保护，所以会产生写偏斜异常。 12345678database, version 11x=10, y=0，约束是 x + y &gt; 0------------------------------------------T1 T2read(x,v11)=10 read(x,v11)=10 read(y,v11)=0 read(y,v11)=0if (x+y) &gt; 0,then if (x+y) &gt; 0,then write(x,v12)=0 write(y,v13)=0 对于SI来说，T1和T2读取的都是某一个快照v11的值，所以if (x+y)&gt;0就会成立，T1将x改成0，T2将y改成0，这样就造成x+y=0了，违反了约束。在串行调度下，不管T1先执行还是T2先执行，都不会出现这种结果。这种异常被称为写偏序(write skew)。 为了解决write skew问题，所以就出现了可序列化的快照隔离技术serializable snapshot isolation(SSI)。 可序列化的快照隔离技术 serializable snapshot isolation(SSI) 既然write skew是读写冲突的一种类型，那么避免或者检测读写冲突就可以解决write skew问题。 理论基础 三种依赖关系 《Generialized isolation level definitions》定义三种依赖关系 写读依赖(write-depends)：T1--wr--&gt;T2，T1写数据项X的一个版本，T2读取这个版本，意味着T1先于T2执行 写写依赖(read-depends)：T1--ww--&gt;T2，T1写X的一个版本，T2再写一个新版本覆盖T1写的版本，意味着T1需要先于T2执行 读写依赖(anti-depends)：rw-dependency或rw-conflicts，Ti先读了X的一个版本Xi，而Tj修改了X值，产生一个新版本Xj，所以是先读后写，属于wr的反向依赖 image 检测写偏序的理论基础的两篇论文，说明读写冲突行为之间的逻辑关系 《weak consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions》：定义了读写依赖，通过读写依赖表明不可串行化必须是多版本可串行化图(MVSG multivertion serialization graphc)存在两条读写依赖边形成的环。 《Making snapshot isolation serializable》：定义读写依赖的扩展形式，表明写偏序发生时，两条读写依赖的边是相邻的。 DSG direct serialization graph 在并发事务之间，根据事务之间的三种关系，画出一幅有向事务关系图，表明事务操作数据时的前后关系，读写操作对新版本值的依赖关系。 事务T1, T2, T3执行如下 123456789101112T1 T2 T3W(z)W(x)W(y) W(x)C R(x) W(y) C R(y) W(z) C DSG图如下 《marking snapshot isolation serializable》中证明了，如果调度不是可序列化的，那么DSG必然存在由两条连续的rw-dependency边形成的环，且这两边边在两个并发事务中。 Suppose H is a multiversion history produced under Snapshot Isolation that is not serializable. Then there is at least one cycle in the serialization graph DSG(H), and we claim that in every cycle there are three consecutive transactions Ti.1, Ti..2, Ti.3 (where it is possible that Ti.1 and Ti.3 are the same transaction) such that Ti.1 and Ti.2 are concurrent, with an edge Ti.1 --&gt; Ti.2, and Ti.2 and Ti.3 are concurrent with an edge Ti.2 --&gt; Ti.3. any cycle must have two rw-dependency edges that occur consecutively, and further, each of these edges is between two concurrent transactions. dangerous structures 然而要找出DSG中是否有环，成本是相当高的，所以论文定义了存在一种危险结构(dangerous structures)，只要在SDG(static dependency graph)图中存在这种危险结构，就会在DSG图中可能存在环，危险结构是出现环的必要非充分条件。同时得出结论，一个SI调度的SDA不存在这种危险结构，这个调度就是可序列化的SI。 《marking snapshot isolation serializable》对dangerous structure的定义如下 Definition 3.5 (Dangerous Structures). We say that the static dependency graph SDG(A) has a dangerous structure if it contains nodes P, Q and R (some of which may not be distinct) such that: (a) There is a vulnerable anti-dependency edge from R to P (b) There is a vulnerable anti-dependency edge from P to Q (c) Either Q = R or else there is a path in the graph from Q to R; that is, (Q, R) is in the reflexive transitive closure of the edge relationship. 其使用SDG图的概念，为了更容易理解，我们直接看《serializable isolation for snapshot database》，论文对这种结论进行了补充，且给出了SSI相关算法的实现。 将rw分成两种情况 读操作读取的不是最新的值：产生rw依赖。读取的是某快照，所以互相不阻塞。 读操作在写操作之前发生：可能产生rw依赖。读取数据需要加SIREAD锁，写时就会检查是否有SIREAD锁，有则代表有rw依赖。 DSG中(MVSG)，存在一个pivot节点（事务），这个节点有两条边（入边，出边），如果存在这个节点，则存在dangerous structure，则可能导致形成环。只要回滚此节点，则会打破环的形成，就可以确保调度是可串行化的。 这样虽然导致了回滚率增加，但是不需要再找出环，只需找出此pivot节点，从而效率大大提高了。 所以这种危险结构是形成环的必要非充分条件。 如下图，Tpivot存在入边和出边，所以产生了危险结构，就可能形成环(如果图中点虚线存在)，这个时候，我们只需要回滚Tpivot就破坏了这种结构。 image SSI SSI在SI的基础上，增加一些其他判断和操作来实现SSI技术。 数据库需要为每个事务维护两个boolean值， T.inConflict ： 此值指示是否存在rw-dependency从其他事务指向事务T T.outConflict ： 指示是否存在rw-dependency从事务T指向其他事务 如果T.inConflict和T.outConflict都是true，既有入边也有出边，则代表此调度可能是非可串行化的。 Write lock 写锁避免了事务并发执行的写写冲突，遵循SS2PL协议。 SIREAD lock 事务读取了某个版本的数据后，则会在此数据上加上SIREAD锁。 SIREAD锁不阻塞写锁，仅仅是一种标志，代表访问此数据。 事务开始 事务开始时，我们将事务的inConflict和outConflict都设置为false。 123modifified begin(T): existing SI code for begin(T) set T.inConflict = T.outConflict = false 读操作 T不应该读到（在T开始时还没有完成提交的事务）但在T结束前已经提交的事务才会生成新的T读不到的版本。 123456789101112131415161718192021modifed read(T, x): get lock(key=x, owner=T, mode=SIREAD) // 设置x的SIREAD // x有写锁时，则可能产生rw依赖，如果写事务不提交则不会产生冲突 if there is a WRITE lock(wl) on x: set wl.owner.inConflict=true set T.outConflict=true // SSI依旧使用SI算法 existing SI code for read(T, x) // SI算法实现，读此快照数据 // 检查：比当前快照读读到的x值更新的数据版本 for each version(xNew) of x that is newer than what T read // 如果新版本的事务已经提交，则当前事务对其构成rw依赖 // 如果新版本的事务对其他事务也构成rw依赖，那么就是pivot事务 if xNew.creator is committed and xNew.creator.outConflict=true: abort(T) // 两个相邻的rw-dep，回滚T return UNSAFE_ERROR // 否则这是一个rw-dependency关系，此关系是T指向数据x的新版本的创建者事务的 set xNew.creator.inConflict=true set T.outConflict=true 写操作 123456789101112131415modified write(T,x, xNew): get lock(key=x, locker=T, mode=WRITE) // x加WRITE锁 if there is a SIREAD lock(rl) on x // 当有SIREAD and rl.owner is running // 且rl.owner(锁持有者)仍然未提交 or commit(rl.owner) &gt; begin(T) // 或rl.owner提交晚于T开始时间 if rl.onwer is committed and rl.owner.inConflict: abort(T) return UNSAFE_ERROR // 否则，这是一个rw-dependency关系，由创建SIREAD锁的事务指向本事务 set rl.owner.outConflict=true set T.inConflict=true // SI算法实现 existing SI code for write(T, x, xNew) # do not get WRITE lock again 提交 事务提交后他的SIREAD不释放，是因为提交后事务也会影响其他正在运行的事务。 直到所有这些事务都提交后才可以释放。 1234567modified commit(T): if T.inConflict and T.outConflict abort(T) return UNSAFE_ERROR existing SI code for commit(T) release WRITE locks held by T do not release SIREAD locks 所以，可以看出SSI是一种乐观的实现方式，通过事务执行操作时来检测事务之间的依赖关系，如果读写不构成危险结构，则读写可以并发执行，如果构成，则以回滚pivot事务为代价来避免读写冲突。这样就实现了可串行化。 隔离级别 在RC和RR隔离级别下，SSI和SS2PL+MVCC实现几乎一样。但在可串行化下，SSI本质上是使用MVCC+行级锁+SIREAD来实现可串行化的。 SS2PL+MVCC vs SSI MVCC技术避免了读写冲突，让读读，读写（快照读）可以并发的执行，但仅限制于只读事务与其他事务可以并发；而非快照读写冲突是无法避免的，这个时候就需要S2PL来避免冲突，保证事务的可串行化。 所以很多数据库就是通过这种MVCC+S2PL技术来实现其事务的可串行化。例如MySQL的InnoDB。然而这种方式是悲观的，有的读写冲突是不会造成异常的，有的却会引起写偏斜等异常，S2PL无法进行区分，所以就降低了事务的并发执行。 而PostgreSQL则是通过SSI来实现其事务的可串行化执行，SSI是乐观的实现方式，读写操作时进行检测，如果存在危险结构则可能造成异常，回滚其pivot事务即可，这种方式可以允许一定的读写并发执行，所以相对于MVCC+S2PL的方式性能更为优异。 例如，对于下面的两个事务， 123456T1 T2R(x) R(z)W(y) W(x) T1读取了x，而T2修改了x，所以T1 -- rw --&gt; T2，有读写依赖， 而T2读取了z，如果没有其他并发事务修改z，则T2不会与其他事务产生rw依赖，则T2不会pivot事务，对于这种情况，SSI是运行并发执行的，而S2PL由于读写冲突，是不允许并发执行的。所以，从这点看，SSI的并发度更高。 而论文对SI、SS2PL和SSI的并发度测试结果，也证明了SSI的性能远优异于S2PL。 最后 并发控制的实现技术非常多，还有基于时间戳的并发控制，基于有效性检查的并发控制等等，但是大多数RDBMS还是基于上面两种技术，也是相对较为主流的实现方案。 本文主要系统性的讲了并发控制相关的理论，然而数据库在工程上的实现是非常复杂的，如PostgreSQL就对锁做了很多优化，还有SSI等等。 我阅读了很多并发控制和事务相关的论文、书籍和资料，不断思考和总结写出本文，希望能给大家带来帮助。如有错误之处，欢迎指正。 主要阅读参考的资料如下： 《concurrency control and recovery in database systems》 《generalized isolation level definitions》 《making snapshot isolation serializable》 《serializable Isolation for Snapshot Databases》 《数据库事务处理的艺术》 《postgresql技术内幕：事务处理深度探索》 《database system implementation》 《database system concepts》]]></content>
      <categories>
        <category>2022</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>PostgreSQL</tag>
        <tag>Concurrency Control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库路径选择理论与postgreSQL实现]]></title>
    <url>%2F2022%2F02%2F20%2Fdatabase-postgresql-path%2F</url>
    <content type="text"><![CDATA[对要枚举出查询树上所有路径，从中找出一个代价最小的路径，是个NP问题，计算成本是非常高昂的。而当查询树的关系(表、子查询等)较少时，可以使用动态规划算法枚举出所有的路径找出最优解，但是当关系较多时，枚举的计算成本是我们无法接收的，这时就需要使用一些折中方案，如贪心算法、遗传算法。遗传算法借鉴自然界“优胜劣汰”的法则对路径进化，从而能找出相对较优的查询路径...... 前言 数据库将SQL解析成一棵语法树后，就需要将语法树转换成查询树，然而查询树的每一个节点或者节点之间的连接都会有一种或者多种实现算法，而需要从中挑选出最优的节点组合。 路径表示单个表的访问方式(顺序访问、索引访问、点查)、两个表的连接方式(嵌套连接、归并连接、hash连接)以及多个表的连接顺序。 那么优化器需要枚举出所有的路径，从中挑选代价最低的路径来执行。 查询树上有的节点是可以直接进行优化的，如过滤、投影等，在查询树底部先将不需要的数据过滤掉，以减少数据量往查询树上层的传递以及操作的成本，这种优化是显而易见的，是基于经验的，我们称为启发式规则优化。 然而，有的优化是无法直接进行的，例如，A JOIN B，JOIN顺序A JOIN B 或者 B JOIN A都会输出正确的查询树结果，哪种顺序成本最低呢，这就需要基于统计数据来进行估算，找出代价最小的顺序，这种优化我们称之为基于成本的优化。 路径选择理论 因为不同的关系访问方式，ORDER、JOIN方式组合和排列非常多，而JOIN的优化是最为复杂的，所以我们就只讨论数据库是如何挑选出最优的JOIN路径的。 JOIN顺序的枚举 对于OUTER JOIN来说，JOIN顺序是固定的，所以路径数量相对较少(只需要考虑不同JOIN算法组成的路径)；然而对于INNER JOIN来说，表之间的JOIN顺序是可以不同的，这样就可以由不同的JOIN组合、不同的JOIN顺序组成非常多的不同路径。 如A JOIN B JOIN C，路径有： (A⋈B)⋈C ：就有两种排列顺序(A JOIN B) JOIN C 和 C JOIN (A JOIN B) (A JOIN C) JOIN B A JOIN (C JOIN B) 等等 我们将所有join顺序组成的树的形态分为 左深树left-deep tree：((1⋈2)⋈3)⋈4 右深树right-deep tree：1⋈(2⋈(3⋈4)) 紧密树bushy tree: (1⋈2)⋈(3⋈4), ((1⋈2)⋈3)⋈(4⋈5) image image JOIN连接的形状 就是一棵full binary tree 的形状，树形态的数量 是一个卡塔兰数(catalan number)， 所以就有 \[ C_{n} = \frac{(2(n-1))!}{n!(n-1)!} \] 种形态的树。 而每种形态的树，都会有n!种排列， 所以n种关系(表，子查询等)就会有 (2(n − 1))!∕(n − 1)!种不同的JOIN路径。 例如4个表JOIN，组成的树的形态就有5种，而每种形态 4! 种排列，所以就有5 * 4! =120 种不同的JOIN路径。 如果是7个表连接，就有665280种JOIN路径！那么要将全部的JOIN路径枚举出来，是非常耗时的。 JOIN方式的枚举 JOIN一般有几种实现方式：嵌套连接、归并连接、hash连接等，所以不同的JOIN顺序还有这几种JOIN方式。 如：A⋈B就有两种排列顺序A JOIN B 和 B JOIN A 而根据不同的JOIN方式有： A nested-loop join B A merge join B A hash join B 实现方式 通常数据库使用以下几种方法来从JOIN路径中找出成本最优的路径 动态规划：考虑全部子集，从中找出最优子集 贪心算法：考虑部分子集 随机算法：考虑部分子集，常见的有模拟退火算法，随机爬山算法，遗传算法等等 动态规划 遍历出所有的路径，记忆路径子集，避免重复计算，通过最优子问题构建出整个问题最优解。 这种方法能够计算出最优路径，但是如果表的个数变多，则“基于代价的动态规划算法”暴露出连接膨胀的问题。依据前面的计算，7个表就有665280种连接顺序，就算有大量子问题能避免重复计算，但整个计算成本还是相当巨大的。 如果表太多，我们就要取舍，追求次最优或者局部最优解(尽量扩大局部性)，如贪心算法、随机算法 贪心算法 贪心算法求解局部最优解，在局部最优解的基础上再去扩大局部最优解，然而这种方法是具有后效性的，得出的结果不一定是最优的。 随机算法 使用贪心算法，前面子问题的解确定下来后，如果子问题的解导致全局的解不是最优，那么就没有修正的余地了。 而随机算法一般是在局部最优解确定下来后，仍然可以以一定概率进行"变化"，以谋求存在一定概率下能打破子问题后效性。 如：遗传算法中，优胜劣汰的规则，会求出局部最优解，但仍然存在一定概率进行"变异"，这样可以存在多个局部解，最优解的被选择概率更高，就确保了在子问题最优而全局不是最优的情况下，仍然有一定概率选择其他子问题解，以谋求得到相对更优的全局解。 不同的数据库选用的算法不一样，有的选用贪心，或者动态规划，或者随机退火等等，而postgreSQL的实现较为灵活，当表较少时，选择动态规划，否则选择遗传算法，还可以允许用户自己定义路径算法。 postgreSQL实现 postgreSQL的路径生成算法有三种： 动态规划 遗传算法 用户自定义 路径生成由make_one_rel函数完成，首先为每个基本关系生成不同的访问路径， 再将这些关系作为叶子节点生成JOIN路径。 123456789101112RelOptInfo *make_one_rel(PlannerInfo *root, List *joinlist)&#123; ...... // 为每个关系生成不同的访问路径 set_base_rel_pathlists(root); // 生成JOIN路径 rel = make_rel_from_joinlist(root, joinlist); return rel;&#125; 生成不同访问路径 遍历所有的关系，为关系生成不同访问路径 12345678910111213141516171819202122232425262728static void set_base_rel_pathlists(PlannerInfo *root)&#123; for (rti = 1; rti &lt; root-&gt;simple_rel_array_size; rti++) &#123; ...... set_rel_pathlist(root, rel, rti, root-&gt;simple_rte_array[rti]); &#125;&#125;static void set_rel_pathlist(PlannerInfo *root, RelOptInfo *rel, Index rti, RangeTblEntry *rte)&#123; ...... switch (rel-&gt;rtekind) &#123; // 关系 case RTE_RELATION: ...... else // 普通关系 &#123; set_plain_rel_pathlist(root, rel, rte); &#125; ...... &#125; ...... set_cheapest(rel);&#125; 再看看普通关系如何生成路径的 12345678910111213static void set_plain_rel_pathlist(PlannerInfo *root, RelOptInfo *rel, RangeTblEntry *rte)&#123; ...... // 顺序扫描路径 add_path(rel, create_seqscan_path(root, rel, required_outer, 0)); // 是否支持索引扫描，支持则添加到路径中 create_index_paths(root, rel); // 表达式是否可以转成TID扫描， create_tidscan_paths(root, rel);&#125; 生成基本访问路径后，就开始进行JOIN路径计算了。 123456789101112131415161718192021static RelOptInfo *make_rel_from_joinlist(PlannerInfo *root, List *joinlist)&#123; ...... if (levels_needed == 1) &#123; // 只有一个join关系，则直接返回 return (RelOptInfo *) linitial(initial_rels); &#125; else &#123; ...... // 如果有自定义join生成算法则使用 if (join_search_hook) return (*join_search_hook) (root, levels_needed, initial_rels); // 如果开启了遗传算法且join关系大于阈值(默认12)则使用遗传算法 else if (enable_geqo &amp;&amp; levels_needed &gt;= geqo_threshold) return geqo(root, levels_needed, initial_rels); else // 否则，使用动态规划算法 return standard_join_search(root, levels_needed, initial_rels); &#125;&#125; 遗传算法 算法 遗传算法： 对问题进行编码，确定搜索空间的大小 随机初始化群体（主要是给染色体赋初始值，计算每个染色体的适应度的值，并对各个染色体的适应度排序），以及计算好进化次数 循环：进行N次进化 “随机”选出两个染色体，分别是momma和daddy 对于momma和daddy使用“杂交”方式，求出其孩子 求杂交得到的kid的适应度的值 把kid插入群体中（如果kid的适应度比群体中最差的适应度还差，则不插入，否则，替换掉最差的那个且排好序） 生成了最优路径 根据最优的连接路径生成查询执行计划和花费估算 实现 种群Pool 用来存储染色体，所有染色体在种群中进化出最终的较优染色体。 123456typedef struct Pool&#123; Chromosome *data; // 染色体数据 int size; // 染色体数量 int string_length; // 染色体大小，用以连接的表的数目&#125; Pool; 基因Gene 最小进化单位，在这里定义成基本关系，Relid 1typedef int Gene; 染色体Chromosome 由基因组成，且由worth来表示此染色体的适应度，适应度差的更高的概率会被淘汰掉。 12345typedef struct Chromosome&#123; Gene *string; // 基因，是连接表构成的序列即一连串不重复的整数形式的字符串 Cost worth; // 适应度，也就是路径代价&#125; Chromosome; postgreSQL实现了几种杂交算法 基于边重组杂交 (edge recombination crossover) 部分匹配杂交 (partially matched crossover) 循环杂交 (cycle crossover) 位置杂交 (position crossover) 顺序杂交 (order crossover) 由于基于边重组杂交是默认的算法，所以我们这里只讨论此算法。 主流程 代码中只保留了基于边重组的杂交算法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970RelOptInfo *geqo(PlannerInfo *root, int number_of_rels, List *initial_rels)&#123; int generation; Chromosome *momma; Chromosome *daddy; Chromosome *kid; Pool *pool; int pool_size, number_generations; Gene *best_tour; Edge *edge_table; /* list of edges */ ...... // 计算种群大小 pool_size = gimme_pool_size(number_of_rels); // 计算杂交次数（进化次数） number_generations = gimme_number_generations(pool_size); // 分配种群 pool = alloc_pool(root, pool_size, number_of_rels); // 随机初始化群体（主要是给染色体赋初始值，并调用geqo_eval函数为每个染色体求适应度的值， // 调用sort_pool函数对各个染色体的适应度排序）。 random_init_pool(root, pool); // 按照适应度值（cheapestpath）排序群体中的染色体, // 适应度值小表示此路径花费小,应该优先选择 sort_pool(root, pool); // 分配父母染色体 momma = alloc_chromo(root, pool-&gt;string_length); daddy = alloc_chromo(root, pool-&gt;string_length); // 创建边表（边重组） edge_table = alloc_edge_table(root, pool-&gt;string_length); // 开始进化 for (generation = 0; generation &lt; number_generations; generation++) &#123; // 选择:利用线性偏差函数,从中选出父母 geqo_selection(root, momma, daddy, pool, Geqo_selection_bias); // **基于边重组杂交** // 通过父母染色体来初始化边表 gimme_edge_table(root, momma-&gt;string, daddy-&gt;string, pool-&gt;string_length, edge_table); // 分配kid染色体 kid = momma; // 杂交 ：从边表中选择合适的基因组成新的染色体 edge_failures += gimme_tour(root, edge_table, kid-&gt;string, pool-&gt;string_length); // 计算kid的适应度，通过gimme_tree构造最优路径 kid-&gt;worth = geqo_eval(root, kid-&gt;string, pool-&gt;string_length); // 进化： // 将kid插入种群中，如果kid比种群中最差的个体适应度差，则不插入 // 否则使用二分查找到合适位置替换掉适应度比kid差的个体 spread_chromo(root, kid, pool); &#125; // 把最优路径传递给gimme_tree best_tour = (Gene *) pool-&gt;data[0].string; best_rel = gimme_tree(root, best_tour, pool-&gt;string_length); ...... return best_rel;&#125; 确定种群大小 种群越大，理论上就越能找出最优解，但就会导致计算成本的增加，所以要对种群大小进行取舍。 12345678910111213141516171819202122static int gimme_pool_size(int nr_rel)&#123; ...... // 如果指定了群体的大小，指定值大于等于2，则采用指定值为群体大小 if (Geqo_pool_size &gt;= 2) return Geqo_pool_size; // size = 2^(nr_rel + 1) size = pow(2.0, nr_rel + 1.0); // 约束 maxsize = 50 * Geqo_effort; /* 50 to 500 individuals */ if (size &gt; maxsize) return maxsize; minsize = 10 * Geqo_effort; /* 10 to 100 individuals */ if (size &lt; minsize) return minsize; return (int) ceil(size);&#125; 初始化种群 随机初始化种群 123456789101112131415161718void random_init_pool(PlannerInfo *root, Pool *pool)&#123; ...... while (i &lt; pool-&gt;size) &#123; init_tour(root, chromo[i].string, pool-&gt;string_length); pool-&gt;data[i].worth = geqo_eval(root, chromo[i].string, pool-&gt;string_length); if (pool-&gt;data[i].worth &lt; DBL_MAX) i++; else &#123; bad++; if (i == 0 &amp;&amp; bad &gt;= 10000) elog(ERROR, &quot;geqo failed to make a valid plan&quot;); &#125; &#125;&#125; 生成染色体 改进的Fisher-Yates洗牌算法， 利用了编码有序的特点，将初始化染色体和随机交换结合起来一起进行 123456789101112131415161718// 随机生成连接顺序void init_tour(PlannerInfo *root, Gene *tour, int num_gene)&#123; ...... if (num_gene &gt; 0) tour[0] = (Gene) 1; for (i = 1; i &lt; num_gene; i++) &#123; j = geqo_randint(root, i, 0); // 交换i和j的值（i并没有初始化） /* i != j check avoids fetching uninitialized array element */ if (i != j) tour[i] = tour[j]; //将tour[j]的值赋值给tour[i] tour[j] = (Gene) (i + 1); // tour[i]之前的值(初始化)赋值给tour[j] &#125;&#125; 适应度 geqo_eval函数的主要只有两步 通过gimme_tree函数进行多表连接 计算连接结果的适应度值 1234567891011121314151617181920Cost geqo_eval(PlannerInfo *root, Gene *tour, int num_gene)&#123; ...... // 进行多表连接 joinrel = gimme_tree(root, tour, num_gene); if (joinrel) &#123; // 计算连接结果的适应度 Path *best_path = joinrel-&gt;cheapest_total_path; fitness = best_path-&gt;total_cost; &#125; else fitness = DBL_MAX; ...... return fitness;&#125; 选择染色体 由于种群中的染色体已经按照适应度排好序了，对我们来说适应度越低（代价越低）的染色体越好，因此选择操作基于概率分布的随机。 这样在选择父亲染色体和母亲染色体的时候更倾向于选择适应度低的染色体，同时也有机会选择非局部最优染色体，以达到能够一定随机性。 123456789101112131415void geqo_selection(PlannerInfo *root, Chromosome *momma, Chromosome *daddy, Pool *pool, double bias)&#123; int first, second; // 选择两个随机数(基于概率分布的) first = linear_rand(root, pool-&gt;size, bias); second = linear_rand(root, pool-&gt;size, bias); ...... // 在种群中基于随机数选择父母染色体 geqo_copy(root, momma, &amp;pool-&gt;data[first], pool-&gt;string_length); geqo_copy(root, daddy, &amp;pool-&gt;data[second], pool-&gt;string_length);&#125; 线性随机 1234567891011121314151617static int linear_rand(PlannerInfo *root, int pool_size, double bias)&#123; double index; /* index between 0 and pool_size */ double max = (double) pool_size; do &#123; double sqrtval; sqrtval = (bias * bias) - 4.0 * (bias - 1.0) * geqo_rand(root); if (sqrtval &gt; 0.0) sqrtval = sqrt(sqrtval); index = max * (bias - sqrtval) / 2.0 / (bias - 1.0); &#125; while (index &lt; 0.0 || index &gt;= max); return (int) index;&#125; 先生成一个基于[0, 1]之间的随机值，然后根据概率分布函数求出符合概率密度的随机值。 生成基于某种概率分布的随机数，要先计算其概率密度函数，postgreSQL的概率密度函数为 \[ f(x) = bias - 2(bias -1)x, (0&lt;x&lt;1) \] bias默认是2，所以 概率密度函数 f(x) = 2-2x，代表概率的变化率。 通过概率密度函数获得概率分布函数 \[ F(x) = \int f(x)dx = bias \times x - (bias - 1) \times x^2, (0&lt;x&lt;1) \] 通过概率分布函数的逆函数法可以获得符合概率分布的随机数 \[ F^{-1}(x) = \frac{bias - \sqrt{bias^2 - 4(bias - 1)y}}{2(bias - 1)} \] 求得概率分布函数如图中红色曲线， 概率分布函数逆函数如图中蓝线， image 从图中红色曲线可以看出，在X轴上递增，则概率越低（连续型随机变量）。 如：F(x)d(0 ~ 0.2)的概率就比F(x)d(0.4 ~ 0.6)高。 而蓝色曲线是概率分布函数的逆函数，值域(Y轴)为随机变量，在X轴上选定任意的p(0&lt;=p&lt;=1)时，通过逆函数得到的是随机变量的值，也就是获得概率和为p的随机变量的上限。 所以通过逆函数，可以将rand(0, 1) “转换成” 符合概率密度分布的随机值。 测试：将linear_rand代码提取出来，进化10000次，种群选择10，那么10条染色体被选中的概率分别为18.8%，16.9%，15.5%，12.9%，11.3%，9.3%，6.3%，5.1%，2.8%，1.1%，可以看出概率变化是符合概率密度函数 f(x) = 2-2x的。 bias的选择 密度函数中bias越大，则适应度低的染色体被选择概率更高，随着进化次数越多，产生"近亲繁殖"的概率就越高，也就越容易造成贪心算法类似的问题； bias越小，则"优胜劣汰"的速度越慢，需要的进化次数越多，但是，最终找出最优解的可能性也越高。 杂交算法 默认的杂交算法是：基于边重组的杂交。 算法 如2条染色体 染色体1：(A, D, C, B) 染色体2：(C, B, D, A) 边关系 视染色体为循环队列：染色体1中A与B是有边关系的 边关系是双向的：如染色体1中，基因A和D的边关系为(A, D)和(D, A) 我们使用一个边表来存储染色体的基因之间的边关系 A -&gt; (-D, B, C) ： 表示基因A和(D, B, C)有边关系，而(-D)表示有2条这样的边关系，称为共享边 B -&gt; (A, -C, D) C -&gt; (D, -B, A) D -&gt; (-A, C, B) 在交叉过程中，会尽量选择共享边，这样可以达到选择较优基因的目的。 边表 边表用来存储染色体中基因的边关系。 123456typedef struct Edge&#123; Gene edge_list[4]; // 每个基因有4条边(2条染色体，前后左右各2条) int total_edges; // 总的数量 int unused_edges; // 没有选择的边数量&#125; Edge; alloc_edge_table分配表边，为总数=基因+1的边。 根据染色体来填充边表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// tour1和tour2为2条染色体float gimme_edge_table(PlannerInfo *root, Gene *tour1, Gene *tour2, int num_gene, Edge *edge_table)&#123; ...... // index1和index2为2个基因的索引 for (index1 = 0; index1 &lt; num_gene; index1++) &#123; // index2 = (index1 + 1) % num_gene; // 边是双向的，填充1-&gt;2，也填充2-&gt;1 // 填充tour1染色体的边关系 edge_total += gimme_edge(root, tour1[index1], tour1[index2], edge_table); gimme_edge(root, tour1[index2], tour1[index1], edge_table); // 填充tour2染色体的边关系 edge_total += gimme_edge(root, tour2[index1], tour2[index2], edge_table); gimme_edge(root, tour2[index2], tour2[index1], edge_table); &#125; return ((float) (edge_total * 2) / (float) num_gene);&#125;static int gimme_edge(PlannerInfo *root, Gene gene1, Gene gene2, Edge *edge_table)&#123; int i; int edges; int city1 = (int) gene1; int city2 = (int) gene2; // edge_list： city1指向的基因的边，如基因A-&gt;B，A-&gt;C, A-&gt;M，edges存储[B,C,M] // 边数量 edges = edge_table[city1].total_edges; // 如果基因city1-&gt;city2已经存在，说明其他染色体也有此边关系 // 则在city1的edge_list中city2改写成 -city2， // 即[-B, C, M]，表示这是共享边 for (i = 0; i &lt; edges; i++) &#123; if ((Gene) Abs(edge_table[city1].edge_list[i]) == city2) &#123; edge_table[city1].edge_list[i] = 0 - city2; return 0; &#125; &#125; // 添加city1-&gt;city2边关系 edge_table[city1].edge_list[edges] = city2; edge_table[city1].total_edges++; edge_table[city1].unused_edges++; return 1;&#125; 通过边表，生成新的染色体 123456789101112131415161718192021222324252627int gimme_tour(PlannerInfo *root, Edge *edge_table, Gene *new_gene, int num_gene)&#123; int i; int edge_failures = 0; // 随机选择一个基因 new_gene[0] = (Gene) geqo_randint(root, num_gene, 1); for (i = 1; i &lt; num_gene; i++) &#123; // 将边表中，指向new_gene[i-1]的边删除掉 remove_gene(root, new_gene[i - 1], edge_table[(int) new_gene[i - 1]], edge_table); // 如果new_gene[i-1]的edge_list中还有边关系，则从中找一个比较&quot;合适&quot;的边关系 if (edge_table[new_gene[i - 1]].unused_edges &gt; 0) new_gene[i] = gimme_gene(root, edge_table[(int) new_gene[i - 1]], edge_table); else &#123; // 如果new_gene[i-1]的edge_list中国没有边关系，则随机找一个new_gene中没有的基因 edge_failures++; new_gene[i] = edge_failure(root, new_gene, i - 1, edge_table, num_gene); &#125; edge_table[(int) new_gene[i - 1]].unused_edges = -1; &#125; /* for (i=1; i&lt;num_gene; i++) */ return edge_failures;&#125; 从基因的边关系中找出一条“合适的”边关系 优先选择共享边关系：共享边是指某条边在父母中都存在，也说明2个基因组成的这条边是较优的，所以要尽量继承下去。 从邻居基因的边关系中，找出其边关系最少的邻居 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static Gene gimme_gene(PlannerInfo *root, Edge edge, Edge *edge_table)&#123; ...... minimum_edges = 5; // 遍历边关系 for (i = 0; i &lt; edge.unused_edges; i++) &#123; friend = (Gene) edge.edge_list[i]; // 优先选择共享边 if (friend &lt; 0) return (Gene) Abs(friend); // 在所有邻居基因中，找出边关系最少的 if (edge_table[(int) friend].unused_edges &lt; minimum_edges) &#123; minimum_edges = edge_table[(int) friend].unused_edges; minimum_count = 1; &#125; else if (minimum_count == -1) elog(ERROR, &quot;minimum_count not set&quot;); else if (edge_table[(int) friend].unused_edges == minimum_edges) minimum_count++; &#125; rand_decision = geqo_randint(root, minimum_count - 1, 0); // 在&quot;边关系最少的&quot;邻居基因中随机选择一个 for (i = 0; i &lt; edge.unused_edges; i++) &#123; friend = (Gene) edge.edge_list[i]; /* return the chosen candidate point */ if (edge_table[(int) friend].unused_edges == minimum_edges) &#123; minimum_count--; if (minimum_count == rand_decision) return friend; &#125; &#125; elog(ERROR, &quot;neither shared nor minimum number nor random edge found&quot;); return 0; /* to keep the compiler quiet */&#125; 这样，我们就通过边重组的方式，产生出较优的染色体。 生成连接路径 根据指定的染色体，生成最佳路径。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546RelOptInfo *gimme_tree(PlannerInfo *root, Gene *tour, int num_gene)&#123; ...... clumps = NIL; // 遍历所有基因 for (rel_count = 0; rel_count &lt; num_gene; rel_count++) &#123; ...... // 提取关系， Get the next input relation cur_rel_index = (int) tour[rel_count]; cur_rel = (RelOptInfo *) list_nth(private-&gt;initial_rels, cur_rel_index - 1); /* Make it into a single-rel clump */ cur_clump = (Clump *) palloc(sizeof(Clump)); cur_clump-&gt;joinrel = cur_rel; cur_clump-&gt;size = 1; // clumps初始是NIL，不断增加可做连接的连接后的关系。 // merge_clump函数的作用就是把cur_clump和clumps中的每个可连接的关系进行连接， // 连接的结果放于clumps中 /* Merge it into the clumps list, using only desirable joins */ clumps = merge_clump(root, clumps, cur_clump, num_gene, false); &#125; if (list_length(clumps) &gt; 1) &#123; /* Force-join the remaining clumps in some legal order */ List *fclumps; ListCell *lc; fclumps = NIL; foreach(lc, clumps) &#123; Clump *clump = (Clump *) lfirst(lc); fclumps = merge_clump(root, fclumps, clump, num_gene, true); &#125; clumps = fclumps; &#125; /* Did we succeed in forming a single join relation? */ if (list_length(clumps) != 1) return NULL; return ((Clump *) linitial(clumps))-&gt;joinrel;&#125; new_clump与clumps中每项进行连接，如果可以连接（且连接顺序符合染色体）， 则连接并加入到clumps中，如果不可以，则将new_clump加入到clumps中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static List *merge_clump(PlannerInfo *root, List *clumps, Clump *new_clump, int num_gene, bool force)&#123; ListCell *lc; int pos; foreach(lc, clumps) &#123; Clump *old_clump = (Clump *) lfirst(lc); if (force || desirable_join(root, old_clump-&gt;joinrel, new_clump-&gt;joinrel)) &#123; RelOptInfo *joinrel; // 完成关系连接处理 joinrel = make_join_rel(root, old_clump-&gt;joinrel, new_clump-&gt;joinrel); if (joinrel) &#123; generate_partitionwise_join_paths(root, joinrel); if (old_clump-&gt;size + new_clump-&gt;size &lt; num_gene) generate_useful_gather_paths(root, joinrel, false); // 找关系上的最优路径 /* Find and save the cheapest paths for this joinrel */ set_cheapest(joinrel); old_clump-&gt;joinrel = joinrel; old_clump-&gt;size += new_clump-&gt;size; pfree(new_clump); /* Remove old_clump from list */ // 去掉old_clump,保证下步递归调用merge_clump函数时不和自身连接 clumps = foreach_delete_current(clumps, lc); return merge_clump(root, clumps, old_clump, num_gene, force); &#125; &#125; &#125; if (clumps == NIL || new_clump-&gt;size == 1) return lappend(clumps, new_clump); for (pos = 0; pos &lt; list_length(clumps); pos++) &#123; Clump *old_clump = (Clump *) list_nth(clumps, pos); if (new_clump-&gt;size &gt; old_clump-&gt;size) break; &#125; clumps = list_insert_nth(clumps, pos, new_clump); return clumps;&#125; 动态规划 算法 过程 第一步 ： 初始化第1层关系，为基本关系节点，包含基本关系的数据访问路径 第二步 ：生成第2层到第n层关系： 左右深树连接方式：将第2层到第n-1层的每个关系，与第1层中的每个关系连接，生成新的关系，每一个新关系，均求出最优路径 紧密树连接方式：将第k层(2&lt;=k&lt;=n-k)的每个关系，与第other_level层(n-k)中的每个关系连接，生成新的关系放于第n层，且每一个新关系，均求解其最优路径 实现 主流程 123456789101112131415161718192021222324252627282930RelOptInfo *standard_join_search(PlannerInfo *root, int levels_needed, List *initial_rels)&#123; ...... // 第1层 // 每个节点是一个基本关系(包含每个关系的所有访问方式) root-&gt;join_rel_level[1] = initial_rels; // 从第2层开始，求出每层的连接关系 for (lev = 2; lev &lt;= levels_needed; lev++) &#123; // 计算出第k层所有的中间关系 join_search_one_level(root, lev); // 遍历lev上每个中间关系，找出最优的 foreach(lc, root-&gt;join_rel_level[lev]) &#123; rel = (RelOptInfo *) lfirst(lc); ...... // 查找本节点最优路径 set_cheapest(rel); &#125; &#125; // 返回最终路径 rel = (RelOptInfo *) linitial(root-&gt;join_rel_level[levels_needed]); root-&gt;join_rel_level = NULL; return rel;&#125; 计算某层的中间关系 左右深树 PlannerInfo结构上的join_rel_level的用来存储关系，第1层是基本表关系，第2层到第N层是连接关系。 假设A JOIN B JOIN C JOIN D，join_rel_level如下： join_rel_level[1]：(A)，(B)，(C)，(D) join_rel_level[2] ：连接第1层的表(满足JOIN约束)，得到(A⋈B)，(A⋈C)，(A⋈D)，(B⋈C)，(B⋈D)，(C⋈D) (A⋈B) ：路径有(A JOIN B), (B JOIN A) 两种join顺序，A merge-join B, A hash-join B等不同的join方式。 join_rel_level[3]：用第1层和第2层的关系连接得到 左深树： ((A⋈B)⋈C)，((A⋈B)⋈D)，((A⋈C)⋈D)，((B⋈C)⋈D) ,等等 右深树：(A⋈(B⋈C)), (A⋈(B⋈D))等等 join_rel_level[4]：用第3层和第1层的关系进行连接 (((A⋈B)⋈C)⋈D), (A⋈(B⋈(C⋈D))) 等等 最终生成整个问题的最优路径。 紧密树 紧密树是由两个或多个连接关系之间再进行连接产生的关系。 所以，只有大于3层的关系才能产生紧密树，假如A JOIN B JOIN C JOIN D JOIN E 将第2层之间进行连接：((A⋈B)⋈(C⋈D)), ((A⋈D)⋈(B⋈C)), ((D⋈E)⋈(B⋈C)) 等 将第2层与第3层进行连接：((A⋈B)⋈(C⋈(D⋈E))), 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 计算出level的节点void join_search_one_level(PlannerInfo *root, int level)&#123; ...... // 左深树和右深树 // 遍历下一层的所有节点，将其与第1层节点进行JOIN foreach(r, joinrels[level - 1]) &#123; RelOptInfo *old_rel = (RelOptInfo *) lfirst(r); // 符合连接约束 if (old_rel-&gt;joininfo != NIL || old_rel-&gt;has_eclass_joins || has_join_restriction(root, old_rel)) &#123; ...... other_rels_list = joinrels[1]; // 第1层rels other_rels = list_head(other_rels_list); // 第1层的第一个rel或者当level=2时，第1层中r后面的rels，避免重复计算 // 找出所有old_rel 和 other_rels_list的other_rels节点以后所有节点的 的连接 make_rels_by_clause_joins(root, old_rel, other_rels_list, other_rels); &#125; else // 没有join关系，做笛卡尔积 &#123; make_rels_by_clauseless_joins(root, old_rel, joinrels[1]); &#125; &#125; // bushy join，用第k层和level-k层连接，构成第level层关系; 而level在本函数的上层函数， // 是从2到N从小到大动态变化的，这样，逐步对每层能够利用bushy连接方式构造连接树 for (k = 2;; k++) &#123; int other_level = level - k; if (k &gt; other_level) break; // 遍历第k层节点 foreach(r, joinrels[k]) &#123; ...... // 遍历level-k层节点 for_each_cell(r2, other_rels_list, other_rels) &#123; RelOptInfo *new_rel = (RelOptInfo *) lfirst(r2); if (!bms_overlap(old_rel-&gt;relids, new_rel-&gt;relids)) &#123; if (have_relevant_joinclause(root, old_rel, new_rel) || have_join_order_restriction(root, old_rel, new_rel)) &#123; (void) make_join_rel(root, old_rel, new_rel); &#125; &#125; &#125; &#125; &#125; // 如果没有连接则做cartesian-product， if (joinrels[level] == NIL) &#123; ...... &#125;&#125;// old_rel与 other_rels 之间生成连接关系static void make_rels_by_clause_joins(PlannerInfo *root, RelOptInfo *old_rel, List *other_rels_list, ListCell *other_rels)&#123; ListCell *l; for_each_cell(l, other_rels_list, other_rels) &#123; RelOptInfo *other_rel = (RelOptInfo *) lfirst(l); // 两个relis没有相同的relation，且有join关联性，且符合join顺序要求 if (!bms_overlap(old_rel-&gt;relids, other_rel-&gt;relids) &amp;&amp; (have_relevant_joinclause(root, old_rel, other_rel) || have_join_order_restriction(root, old_rel, other_rel))) &#123; (void) make_join_rel(root, old_rel, other_rel); &#125; &#125;&#125; 创建关系 make_join_rel用于创建一个新的连接关系并生成此连接关系的路径。 这个新关系的构成的每种方式就是一个路径，并保存于RelOptInfo的pathlist。 12345678910111213RelOptInfo *make_join_rel(PlannerInfo *root, RelOptInfo *rel1, RelOptInfo *rel2)&#123; ...... // 查找joinrel，如果没有则创建一个 joinrel = build_join_rel(root, joinrelids, rel1, rel2, sjinfo, &amp;restrictlist); ...... // 生成路径 populate_joinrel_with_paths(root, rel1, rel2, joinrel, sjinfo, restrictlist); bms_free(joinrelids); return joinrel;&#125; 1234567891011121314151617181920RelOptInfo *build_join_rel(PlannerInfo *root, Relids joinrelids, RelOptInfo *outer_rel, RelOptInfo *inner_rel, SpecialJoinInfo *sjinfo, List **restrictlist_ptr)&#123; ...... // 先查找此连接关系是否已经存在， joinrel = find_join_rel(root, joinrelids); if (joinrel) &#123; if (restrictlist_ptr) *restrictlist_ptr = build_joinrel_restrictlist(root, joinrel, outer_rel, inner_rel); return joinrel; &#125; // 不存在，则初始化一个 joinrel = makeNode(RelOptInfo); joinrel-&gt;reloptkind = RELOPT_JOINREL; ...... return joinrel;&#125; 生成路径 123456789101112131415161718192021static void populate_joinrel_with_paths(PlannerInfo *root, RelOptInfo *rel1, RelOptInfo *rel2, RelOptInfo *joinrel, SpecialJoinInfo *sjinfo, List *restrictlist)&#123; switch (sjinfo-&gt;jointype) &#123; case JOIN_INNER: ...... // 生成 (rel1 JOIN rel2) and (rel2 JOIN rel1)两种路径 add_paths_to_joinrel(root, joinrel, rel1, rel2, JOIN_INNER, sjinfo, restrictlist); add_paths_to_joinrel(root, joinrel, rel2, rel1, JOIN_INNER, sjinfo, restrictlist); break; case JOIN_LEFT: ...... // 对于left join：生成(rel1 left join rel2) and (rel2 right join rel1)两种路径 add_paths_to_joinrel(root, joinrel, rel1, rel2, JOIN_LEFT, sjinfo, restrictlist); add_paths_to_joinrel(root, joinrel, rel2, rel1, JOIN_RIGHT, sjinfo, restrictlist); break; ...... &#125; ......&#125; 总结 要枚举出查询树上所有路径，从中找出一个代价最小的路径，是个NP问题，计算成本是非常高昂的。而当查询树的关系(表、子查询等)较少时，可以使用动态规划算法枚举出所有的路径找出最优解，但是当关系较多时，枚举的计算成本是我们无法接收的，这时就需要使用一些折中方案，如贪心算法、遗传算法。遗传算法借鉴自然界“优胜劣汰”的法则对路径进化，从而能找出相对较优的查询路径。]]></content>
      <categories>
        <category>2022</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>PostgreSQL</tag>
        <tag>genetic algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes雪崩了]]></title>
    <url>%2F2021%2F10%2F26%2Fkubernetes_case%2F</url>
    <content type="text"><![CDATA[云原生最重要的组件之一，竟然“雪崩”了...... kubernetes作为云原生不可或缺的重要组件，在编排调度系统中一统天下，成为事实上的标准，如果kubernetes出现问题，那将是灾难性的。而我们在容器化推进的过程中，就遇到了很多有意思的故障，今天我们就来分析一个“雪崩”的案例。 问题描述 第一次故障 一个黑色星期三，突然收到大量报警：服务大量5xx；某台kubernetes宿主机器CPU飚高到近80%；内存增速过快等等。 赶紧打开监控，发现一些服务的超时请求很多，内存急剧增高，gc变慢，CPU、goroutines暴涨等等。 粗略分析看不出到底哪个是因哪些是果，于是启动应急预案，赶紧将此机器上的POD迁移到其他机器上，迁移后服务运行正常，流量恢复。 初步解决问题，开始详细分析原因，从监控看实在分析不出原因，也没有内核日志，只有故障4分钟后的一条OOM kill的日志，没有什么价值。 但是从监控上看A服务在此机器上的POD问题尤为严重。 由于当时急于恢复流量，没有保留好现场，以为是某个服务出现什么问题导致的（当时还猜测可能linux某部分隔离性没有做好引发的）。 服务故障部分监控 宿主机 内核OOM日志 第二次故障 3个小时后，又一台机器发生故障，先恢复故障的同时使用perf捕获了调用栈，然后又继续骚操作将POD迁移走，这一次就导致其他3台宿主机出现问题了，这3台宿主机上的服务以及依赖这些服务的服务也相继出现问题。 这时，才意识到是什么达到了临界，才会导致其他机器相继出现问题。于是赶紧将一些服务转移到物理机部署，宿主机立即恢复正常）。 宿主机CPU利用率 top查看到很多服务的CPU使用率都很高 perf record 初步分析 虽然问题初步解决了，但还得继续分析根本原因。 从perf record来看，大多数进程的调用都blocking在__write_lock_failed函数，大部分都是由于__neigh_create调用造成的，于是先google，搜索到相似问题的文章: - https://www.cnblogs.com/tencent-cloud-native/p/14481570.html 但还不能如此草率的下定论，仍然需要进一步分析才能盖棺定论。 内核源码分析 线上机器内核较老，3.10.0-1160.36.2.el7.x86_64版本， 由于有tcp流量也有udp流量，所以两种协议都要进行分析。 系统调用 从系统调用send开始分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354SYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len, unsigned int, flags)&#123; return sys_sendto(fd, buff, len, flags, NULL, 0);&#125;SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len, unsigned int, flags, struct sockaddr __user *, addr, int, addr_len)&#123; ...... // 根据fd找到socket sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed); if (!sock) goto out; // 构造msghdr iov.iov_base = buff; iov.iov_len = len; msg.msg_name = NULL; msg.msg_iov = &amp;iov; msg.msg_iovlen = 1; msg.msg_control = NULL; msg.msg_controllen = 0; msg.msg_namelen = 0; // 发送数据 err = sock_sendmsg(sock, &amp;msg, len); ......&#125;int sock_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)&#123; ...... ret = __sock_sendmsg(&amp;iocb, sock, msg, size); ......&#125;static inline int __sock_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; ..... return err ?: __sock_sendmsg_nosec(iocb, sock, msg, size);&#125;static inline int __sock_sendmsg_nosec(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; ...... // AF_INET 协议族提供的是inet_sendmsg函数来发送数据 return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);&#125; 1234567int inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg, size_t size)&#123; ...... return sk-&gt;sk_prot-&gt;sendmsg(iocb, sk, msg, size); &#125;EXPORT_SYMBOL(inet_sendmsg); inet_sendmsg会调用不同协议的发送函数， - udp协议：udp_sendmsg - tcp协议：tcp_sendmsg 协议层 udp 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)&#123; ...... /* Lockless fast path for the non-corking case. */ if (!corkreq) &#123; // 构造skb skb = ip_make_skb(sk, fl4, getfrag, msg-&gt;msg_iov, ulen, sizeof(struct udphdr), &amp;ipc, &amp;rt, msg-&gt;msg_flags); err = PTR_ERR(skb); if (!IS_ERR_OR_NULL(skb)) err = udp_send_skb(skb, fl4); goto out; &#125; ...... return err;&#125;EXPORT_SYMBOL(udp_sendmsg);// static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)&#123; ...... // 填充UDP头，checksum uh = udp_hdr(skb); uh-&gt;source = inet-&gt;inet_sport; uh-&gt;dest = fl4-&gt;fl4_dport; uh-&gt;len = htons(len); ...... uh-&gt;check = csum_tcpudp_magic(fl4-&gt;saddr, fl4-&gt;daddr, len, sk-&gt;sk_protocol, csum); ...... // 发送 err = ip_send_skb(sock_net(sk), skb); if (err) &#123; if (err == -ENOBUFS &amp;&amp; !inet-&gt;recverr) &#123; UDP_INC_STATS_USER(sock_net(sk), UDP_MIB_SNDBUFERRORS, is_udplite); err = 0; &#125; &#125; else ...... return err; &#125;int ip_send_skb(struct net *net, struct sk_buff *skb)&#123; int err; err = ip_local_out(skb); if (err) &#123; if (err &gt; 0) err = net_xmit_errno(err); if (err) IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS); &#125; return err;&#125; 最终调用到ip_local_out函数 tcp 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t size)&#123; ...... struct tcp_sock *tp = tcp_sk(sk); lock_sock(sk); ...... /* Ok commence sending. */ // 获取⽤户传递过来的数据和标志 iovlen = msg-&gt;msg_iovlen; iov = msg-&gt;msg_iov; // 数据地址 copied = 0; ...... // 遍历用户层的数据块 while (--iovlen &gt;= 0) &#123; size_t seglen = iov-&gt;iov_len; unsigned char __user *from = iov-&gt;iov_base; // 待发送数据块 iov++; ...... while (seglen &gt; 0) &#123; int copy = 0; int max = size_goal; // 获取发送队列，列队最后一个skb，尝试将现在数据加入skb skb = tcp_write_queue_tail(sk); if (tcp_send_head(sk)) &#123; if (skb-&gt;ip_summed == CHECKSUM_NONE) max = mss_now; copy = max - skb-&gt;len; &#125; if (copy &lt;= 0) &#123; // 需要申请新的skb // 申请 skb，并添加到发送队列的尾部 if (!sk_stream_memory_free(sk)) goto wait_for_sndbuf; skb = sk_stream_alloc_skb(sk, select_size(sk, sg), sk-&gt;sk_allocation); if (!skb) goto wait_for_memory; ...... // 把 skb 挂到socket的发送队列上 skb_entail(sk, skb); copy = size_goal; max = size_goal; &#125; /* Try to append data to the end of skb. */ if (copy &gt; seglen) copy = seglen; /* Where to copy to? */ // skb 中有⾜够的空间 if (skb_availroom(skb) &gt; 0) &#123; // copy⽤户空间的数据到内核空间、计算校验和 // user space data ---- copy -----&gt; skb copy = min_t(int, copy, skb_availroom(skb)); err = skb_add_data_nocache(sk, skb, from, copy); if (err) goto do_fault; &#125; else &#123; ...... &#125; ...... if (forced_push(tp)) &#123; tcp_mark_push(tp, skb); // 发送数据 __tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH); &#125; else if (skb == tcp_send_head(sk)) tcp_push_one(sk, mss_now); // 发送 continue; ...... &#125; &#125; ......&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss, int nonagle)&#123; ...... if (tcp_write_xmit(sk, cur_mss, nonagle, 0, sk_gfp_atomic(sk, GFP_ATOMIC))) tcp_check_probe_timer(sk);&#125;static bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle, int push_one, gfp_t gfp)&#123; ...... // 循环获取待发送的skb while ((skb = tcp_send_head(sk))) &#123; ...... // 滑动窗口管理 cwnd_quota = tcp_cwnd_test(tp, skb); ...... // 发送数据 if (unlikely(tcp_transmit_skb(sk, skb, 1, gfp))) break;repair: tcp_event_new_data_sent(sk, skb); tcp_minshall_update(tp, mss_now, skb); sent_pkts += tcp_skb_pcount(skb); if (push_one) break; &#125; if (likely(sent_pkts)) &#123; if (tcp_in_cwnd_reduction(sk)) tp-&gt;prr_out += sent_pkts; /* Send one loss probe per tail loss episode. */ if (push_one != 2) tcp_schedule_loss_probe(sk); tcp_cwnd_validate(sk); return false; &#125; return (push_one == 2) || (!tp-&gt;packets_out &amp;&amp; tcp_send_head(sk));&#125;static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it, gfp_t gfp_mask)&#123; ...... err = icsk-&gt;icsk_af_ops-&gt;queue_xmit(skb, &amp;inet-&gt;cork.fl); if (likely(err &lt;= 0)) return err; return net_xmit_eval(err);&#125; 1234567// 网络层int ip_queue_xmit(struct sk_buff *skb, struct flowi *fl)&#123; ...... res = ip_local_out(skb); return res;&#125; 最终也调用到ip_local_out函数 网络层 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859int ip_local_out(struct sk_buff *skb)&#123; int err; err = __ip_local_out(skb); // 构造ip header，netfilter过滤 if (likely(err == 1)) // 允许数据通过 err = dst_output(skb); return err;&#125;int __ip_local_out(struct sk_buff *skb)&#123; // 设置ip头 struct iphdr *iph = ip_hdr(skb); iph-&gt;tot_len = htons(skb-&gt;len); ip_send_check(iph); // checksum // netfilter的hook, iptables的实现 return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL, skb_dst(skb)-&gt;dev, dst_output);&#125;static inline int dst_output(struct sk_buff *skb)&#123; return skb_dst(skb)-&gt;output(skb); // 调用ip_output&#125;int ip_output(struct sk_buff *skb)&#123; ...... // if !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED) 为真 // 将skb发给netfilter，过滤通过，则调用ip_finish_output return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev, ip_finish_output, !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED));&#125;static int ip_finish_output(struct sk_buff *skb)&#123; // 如果内核启用了 netfilter 和数据包转换（XFRM）， // 则更新 skb 的标志并通过 dst_output 来发送#if defined(CONFIG_NETFILTER) &amp;&amp; defined(CONFIG_XFRM) /* Policy lookup after SNAT yielded a new policy */ if (skb_dst(skb)-&gt;xfrm != NULL) &#123; IPCB(skb)-&gt;flags |= IPSKB_REROUTED; return dst_output(skb); &#125;#endif // 如果数据包的长度大于 MTU 并且分片不会 offload 到设备， // 则会调用 ip_fragment 在发送之前对数据包进行分片 if (skb-&gt;len &gt; ip_skb_dst_mtu(skb) &amp;&amp; !skb_is_gso(skb)) return ip_fragment(skb, ip_finish_output2); else return ip_finish_output2(skb);&#125; 邻居子系统 重点来了！ 调用ip_finish_output2到达邻居子系统。 12345678910111213141516171819202122232425262728static inline int ip_finish_output2(struct sk_buff *skb)&#123; struct dst_entry *dst = skb_dst(skb); struct rtable *rt = (struct rtable *)dst; struct net_device *dev = dst-&gt;dev; // 出口设备 ...... rcu_read_lock_bh(); // 下一跳 nexthop = (__force u32) rt_nexthop(rt, ip_hdr(skb)-&gt;daddr); // 根据下一跳和设备名去arp缓存表查询，如果没有，则创建 neigh = __ipv4_neigh_lookup_noref(dev, nexthop); if (unlikely(!neigh)) neigh = __neigh_create(&amp;arp_tbl, &amp;nexthop, dev, false); if (!IS_ERR(neigh)) &#123; int res = dst_neigh_output(dst, neigh, skb); rcu_read_unlock_bh(); return res; &#125; rcu_read_unlock_bh(); kfree_skb(skb); return -EINVAL;&#125; 下一跳为网关，则返回网关地址。 下一跳为POD的IP地址，所以对于macvlan的网络架构来说，同一宿主机的两个POD之间互相访问，都会产生两条ARP表项。 123456static inline __be32 rt_nexthop(const struct rtable *rt, __be32 daddr)&#123; if (rt-&gt;rt_gateway) return rt-&gt;rt_gateway; return daddr;&#125; 查找ARP表项 __ipv4_neigh_lookup_noref是根据 出口设备“唯一标示”和目标ip 计算一个hash值，然后在 arp hash table中查找是否存在此arp项 所以arp hash table里面存储的条目是全局的 - dev：可以到达neighbour的设备，是dev设备指针的值，而此时在内核态，所以可以看作是设备的唯一标示 - ip：L3层目标地址(下一跳地址) 12345678910111213141516171819202122232425262728293031323334static inline struct neighbour *__ipv4_neigh_lookup_noref(struct net_device *dev, u32 key)&#123; struct neigh_hash_table *nht = rcu_dereference_bh(arp_tbl.nht); struct neighbour *n; u32 hash_val; // 将dev指针传递给hash函数 hash_val = arp_hashfn(key, dev, nht-&gt;hash_rnd[0]) &gt;&gt; (32 - nht-&gt;hash_shift); for (n = rcu_dereference_bh(nht-&gt;hash_buckets[hash_val]); n != NULL; n = rcu_dereference_bh(n-&gt;next)) &#123; if (n-&gt;dev == dev &amp;&amp; *(u32 *)n-&gt;primary_key == key) return n; &#125; return NULL;&#125;static inline u32 arp_hashfn(u32 key, const struct net_device *dev, u32 hash_rnd)&#123; u32 val = key ^ hash32_ptr(dev); // 计算的是ip + dev指针， return val * hash_rnd;&#125;static inline u32 hash32_ptr(const void *ptr)&#123; unsigned long val = (unsigned long)ptr;#if BITS_PER_LONG == 64 val ^= (val &gt;&gt; 32);#endif return (u32)val;&#125; 创建邻居表项 如果ARP缓存表不存在此项，则创建， 创建失败，返回-EINVAL。 这就是问题所在了，没有任何内核日志，返回一个有歧义的错误码，也就导致无法快速定位原因。 还好在linux kernel 5.x内核代码中，会打印报错信息： neighbour: arp_cache: neighbor table overflow! 123456789101112131415struct neighbour *__neigh_create(struct neigh_table *tbl, const void *pkey, struct net_device *dev, bool want_ref)&#123; // 分配neighbour // struct neighbour *n1, *rc, *n = neigh_alloc(tbl, dev); struct neigh_hash_table *nht; if (!n) &#123; rc = ERR_PTR(-ENOBUFS); goto out; &#125; ......out: return rc;&#125; 分配neighbour， 同时如果ARP缓存表超过gc_thresh3，则强制做一次gc。 如果gc没有回收任何过期ARP记录且表项数超过gc_thresh3，则返回NULL，标识分配邻居表项失败 1234567891011121314151617181920212223242526272829// 分配失败，返回NULLstatic struct neighbour *neigh_alloc(struct neigh_table *tbl, struct net_device *dev)&#123; struct neighbour *n = NULL; unsigned long now = jiffies; int entries; entries = atomic_inc_return(&amp;tbl-&gt;entries) - 1; // 如果超过gc_thres3或者 // 超过gc_thresh2且5秒没有刷新(HZ一般为1000) // 则强制gc if (entries &gt;= tbl-&gt;gc_thresh3 || (entries &gt;= tbl-&gt;gc_thresh2 &amp;&amp; time_after(now, tbl-&gt;last_flush + 5 * HZ))) &#123; if (!neigh_forced_gc(tbl) &amp;&amp; entries &gt;= tbl-&gt;gc_thresh3) goto out_entries; // arp表项超过阈值 且回收失败， &#125; n = kzalloc(tbl-&gt;entry_size + dev-&gt;neigh_priv_len, GFP_ATOMIC); ...... out: return n;out_entries: atomic_dec(&amp;tbl-&gt;entries); goto out;&#125; 对ARP表进行垃圾回收，回收期间需要获取写锁。 如果成功回收则返回1，否则返回0 12345678910111213141516171819202122232425262728293031323334353637383940static int neigh_forced_gc(struct neigh_table *tbl)&#123; // 获取 arp 缓存hash table 锁 write_lock_bh(&amp;tbl-&gt;lock); nht = rcu_dereference_protected(tbl-&gt;nht, lockdep_is_held(&amp;tbl-&gt;lock)); for (i = 0; i &lt; (1 &lt;&lt; nht-&gt;hash_shift); i++) &#123; struct neighbour *n; struct neighbour __rcu **np; np = &amp;nht-&gt;hash_buckets[i]; while ((n = rcu_dereference_protected(*np, lockdep_is_held(&amp;tbl-&gt;lock))) != NULL) &#123; /* Neighbour record may be discarded if: * - nobody refers to it. * - it is not permanent */ write_lock(&amp;n-&gt;lock); if (atomic_read(&amp;n-&gt;refcnt) == 1 &amp;&amp; !(n-&gt;nud_state &amp; NUD_PERMANENT)) &#123; rcu_assign_pointer(*np, rcu_dereference_protected(n-&gt;next, lockdep_is_held(&amp;tbl-&gt;lock))); n-&gt;dead = 1; shrunk = 1; write_unlock(&amp;n-&gt;lock); neigh_cleanup_and_release(n); continue; &#125; write_unlock(&amp;n-&gt;lock); np = &amp;n-&gt;next; &#125; &#125; tbl-&gt;last_flush = jiffies; write_unlock_bh(&amp;tbl-&gt;lock); return shrunk;&#125; 123456789101112131415161718192021222324static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n, struct sk_buff *skb)&#123; const struct hh_cache *hh; if (dst-&gt;pending_confirm) &#123; unsigned long now = jiffies; dst-&gt;pending_confirm = 0; /* avoid dirtying neighbour */ if (n-&gt;confirmed != now) n-&gt;confirmed = now; &#125; hh = &amp;n-&gt;hh; // 如果下面三种情况之一，不需要ARP请求 // NUD_PERMANENT : 静态路由 // NUD_NOARP：多播地址，广播地址或者本地环回设备lookback // NUD_REACHABLE：邻居表项状态是reachable if ((n-&gt;nud_state &amp; NUD_CONNECTED) &amp;&amp; hh-&gt;hh_len) return neigh_hh_output(hh, skb); else return n-&gt;output(n, skb); // 非连接状态, neigh_resolve_output&#125; ARP 下面的源码分析其实已经不重要了， neigh_resolve_output会间接调用函数neigh_event_send()， 实现邻居项状态从 NUD_NONE 到 NUD_INCOMPLETTE 状态的改变 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/* Slow and careful. */int neigh_resolve_output(struct neighbour *neigh, struct sk_buff *skb)&#123; ...... // 发送arp请求 if (!neigh_event_send(neigh, skb)) &#123; // 如果neigh已经连接，则发送skb int err; struct net_device *dev = neigh-&gt;dev; unsigned int seq; if (dev-&gt;header_ops-&gt;cache &amp;&amp; !neigh-&gt;hh.hh_len) neigh_hh_init(neigh, dst); // 初始化mac缓存值 do &#123; __skb_pull(skb, skb_network_offset(skb)); seq = read_seqbegin(&amp;neigh-&gt;ha_lock); err = dev_hard_header(skb, dev, ntohs(skb-&gt;protocol), neigh-&gt;ha, NULL, skb-&gt;len); &#125; while (read_seqretry(&amp;neigh-&gt;ha_lock, seq)); if (err &gt;= 0) rc = dev_queue_xmit(skb); // 真正发送数据 else goto out_kfree_skb; &#125; ......&#125;EXPORT_SYMBOL(neigh_resolve_output);static inline int neigh_event_send(struct neighbour *neigh, struct sk_buff *skb)&#123; ...... if (!(neigh-&gt;nud_state&amp;(NUD_CONNECTED|NUD_DELAY|NUD_PROBE))) return __neigh_event_send(neigh, skb); return 0;&#125;int __neigh_event_send(struct neighbour *neigh, struct sk_buff *skb)&#123; write_lock_bh(&amp;neigh-&gt;lock); ...... // 初始化阶段 if (!(neigh-&gt;nud_state &amp; (NUD_STALE | NUD_INCOMPLETE))) &#123; if (neigh-&gt;parms-&gt;mcast_probes + neigh-&gt;parms-&gt;app_probes) &#123; unsigned long next, now = jiffies; atomic_set(&amp;neigh-&gt;probes, neigh-&gt;parms-&gt;ucast_probes); neigh-&gt;nud_state = NUD_INCOMPLETE; // 设置表项尾incomplete neigh-&gt;updated = now; next = now + max(neigh-&gt;parms-&gt;retrans_time, HZ/2); neigh_add_timer(neigh, next); // 定时器：刷新状态后在neigh_update中发送skb immediate_probe = true; &#125; else &#123; ...... &#125; ...... &#125; if (neigh-&gt;nud_state == NUD_INCOMPLETE) &#123; if (skb) &#123; while (neigh-&gt;arp_queue_len_bytes + skb-&gt;truesize &gt; neigh-&gt;parms-&gt;queue_len_bytes) &#123; struct sk_buff *buff; // 丢弃 buff = __skb_dequeue(&amp;neigh-&gt;arp_queue); if (!buff) break; neigh-&gt;arp_queue_len_bytes -= buff-&gt;truesize; kfree_skb(buff); NEIGH_CACHE_STAT_INC(neigh-&gt;tbl, unres_discards); &#125; skb_dst_force(skb); __skb_queue_tail(&amp;neigh-&gt;arp_queue, skb); //报文放入arp_queue队列中，arp_queue 队列用于缓存等待发送的数据包 neigh-&gt;arp_queue_len_bytes += skb-&gt;truesize; &#125; rc = 1; &#125;out_unlock_bh: if (immediate_probe) neigh_probe(neigh); // 进行探测，发送arp else write_unlock(&amp;neigh-&gt;lock); local_bh_enable(); return rc;&#125;EXPORT_SYMBOL(__neigh_event_send);static void neigh_probe(struct neighbour *neigh) __releases(neigh-&gt;lock)&#123; struct sk_buff *skb = skb_peek(&amp;neigh-&gt;arp_queue); // 取出报文 /* keep skb alive even if arp_queue overflows */ if (skb) skb = skb_copy(skb, GFP_ATOMIC); write_unlock(&amp;neigh-&gt;lock); neigh-&gt;ops-&gt;solicit(neigh, skb); // 发送arp请求，arp_solicit atomic_inc(&amp;neigh-&gt;probes); kfree_skb(skb);&#125; ARP响应的处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071static struct packet_type arp_packet_type __read_mostly = &#123; .type = cpu_to_be16(ETH_P_ARP), .func = arp_rcv,&#125;;void __init arp_init(void)&#123; neigh_table_init(&amp;arp_tbl); dev_add_pack(&amp;arp_packet_type); // add packet handler ......&#125;static int arp_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)&#123; ...... return NF_HOOK(NFPROTO_ARP, NF_ARP_IN, skb, dev, NULL, arp_process);&#125;static int arp_process(struct sk_buff *skb) &#123; ...... n = __neigh_lookup(&amp;arp_tbl, &amp;sip, dev, 0); if (n) &#123; // 更新邻居项 neigh_update(n, sha, state, override ? NEIGH_UPDATE_F_OVERRIDE : 0); &#125; ......&#125;int neigh_update(struct neighbour *neigh, const u8 *lladdr, u8 new, u32 flags)&#123; ...... write_lock_bh(&amp;neigh-&gt;lock); ...... if (new &amp; NUD_CONNECTED) neigh_connect(neigh); else neigh_suspect(neigh); if (!(old &amp; NUD_VALID)) &#123; // 源状态不为valid，则发送缓存的skb struct sk_buff *skb; while (neigh-&gt;nud_state &amp; NUD_VALID &amp;&amp; (skb = __skb_dequeue(&amp;neigh-&gt;arp_queue)) != NULL) &#123; struct dst_entry *dst = skb_dst(skb); struct neighbour *n2, *n1 = neigh; write_unlock_bh(&amp;neigh-&gt;lock); rcu_read_lock(); n2 = NULL; if (dst) &#123; n2 = dst_neigh_lookup_skb(dst, skb); if (n2) n1 = n2; &#125; n1-&gt;output(n1, skb); // 调用neigh的output函数，此时已经改成connect函数 if (n2) neigh_release(n2); rcu_read_unlock(); write_lock_bh(&amp;neigh-&gt;lock); &#125; skb_queue_purge(&amp;neigh-&gt;arp_queue); neigh-&gt;arp_queue_len_bytes = 0; &#125; ...... write_unlock_bh(&amp;neigh-&gt;lock); ......&#125; 自旋锁 对邻居表进行修改需要获取写锁，而这个锁的实现为自旋锁。 12345678910111213141516171819202122232425262728293031323334static inline void __raw_write_lock_bh(rwlock_t *lock)&#123; local_bh_disable(); preempt_disable(); rwlock_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);&#125;# define do_raw_write_trylock(rwlock) arch_write_trylock(&amp;(rwlock)-&gt;raw_lock)static inline void arch_write_lock(arch_rwlock_t *rw)&#123; u32 val = __insn_fetchor4(&amp;rw-&gt;lock, __WRITE_LOCK_BIT); if (unlikely(val != 0)) __write_lock_failed(rw, val);&#125;/* * If we failed because there were readers, clear the &quot;writer&quot; bit * so we don&apos;t block additional readers. Otherwise, there was another * writer anyway, so our &quot;fetchor&quot; made no difference. Then wait, * issuing periodic fetchor instructions, till we get the lock. */void __write_lock_failed(arch_rwlock_t *rw, u32 val)&#123; int iterations = 0; do &#123; if (!arch_write_val_locked(val)) val = __insn_fetchand4(&amp;rw-&gt;lock, ~__WRITE_LOCK_BIT); delay_backoff(iterations++); val = __insn_fetchor4(&amp;rw-&gt;lock, __WRITE_LOCK_BIT); &#125; while (val != 0);&#125;EXPORT_SYMBOL(__write_lock_failed); 根本原因 原因已经明确了，由于POD之间通信都会产生2条ARP记录缓存在ARP表中，如果宿主机上POD较多，则ARP表项会很多，而linux对ARP大小是有限制的。 ARP表项相关的内核参数 内核参数 含义 默认值 /proc/sys/net/ipv4/neigh/default/base_reachable_time reachable 状态基础过期时间 30秒 /proc/sys/net/ipv4/neigh/default/gc_stale_time stale 状态过期时间 60秒 /proc/sys/net/ipv4/neigh/default/delay_first_probe_time delay 状态过期到 probe 的时间 5秒 /proc/sys/net/ipv4/neigh/default/gc_interval gc 启动的周期时间 30秒 /proc/sys/net/ipv4/neigh/default/gc_thresh1 最小可保留的表项数量 128 /proc/sys/net/ipv4/neigh/default/gc_thresh2 最多纪录的软限制 512 /proc/sys/net/ipv4/neigh/default/gc_thresh3 最多纪录的硬限制 1024 ARP表垃圾回收机制 表的垃圾回收分为同步和异步： - 同步：neigh_alloc函数被调用时执行 - 异步：gc_interval指定的周期性gc 回收策略 - arp 表项数量 &lt; gc_thresh1：不做处理 - gc_thresh1 =&lt; arp 表项数量 &lt;= gc_thresh2：按照 gc_interval 定期启动ARP表垃圾回收 - gc_thresh2 &lt; arp 表项数量 &lt;= gc_thresh3，5 * HZ（一般5秒）后ARP表垃圾回收 - arp 表项数量 &gt; gc_thresh3：立即启动ARP表垃圾回收 ARP表项状态转换 图来源于《Understanding Linux Network Internals》 如图所示，只有两种状态的ARP记录才能被回收： - stale状态过期后 - failed状态 结论 由于我们kubernetes是基于macvlan网络的，所以会为每个POD分配一个mac地址，这样同一台宿主机的POD之间访问，就会产生2条ARP表项。 所以，当ARP表项超过gc_thresh3，则会触发回收ARP表，当没有可回收的表项时，则会导致neigh_create分配失败，从而导致系统调用失败。 那么上层应用层很可能会进行不断重试，也会有其他流量调用内核发送数据，最终只要数据接收方的ARP记录不在ARP表中，都会导致进行垃圾回收，而垃圾回收需要加写锁后再对ARP表进行遍历，这个遍历虽然耗时不长，但是持写锁时间 * 回收次数 导致持写锁的总体时间很长，这样就导致大量的锁竞争（自旋），导致CPU暴涨，从而引发故障。 验证 先将gc_thresh改小 1234#!/bin/bashecho 6 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh1echo 7 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh2echo 8 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh3 ping下其他POD，让ARP表大小达到阈值 123456789[root@d /]# ip neigh10.170.254.32 dev eth0 lladdr 02:00:0a:aa:fe:20 REACHABLE10.170.254.1 dev eth0 lladdr 2c:21:31:79:96:c0 REACHABLE10.170.254.27 dev eth0 lladdr 02:00:0a:aa:fe:1b REACHABLE10.170.254.24 dev eth0 lladdr 02:00:0a:aa:fe:18 REACHABLE10.170.254.25 dev eth0 lladdr 02:00:0a:aa:fe:19 REACHABLE10.170.254.30 dev eth0 lladdr 02:00:0a:aa:fe:1e REACHABLE10.170.254.28 dev eth0 lladdr 02:00:0a:aa:fe:1c REACHABLE10.170.254.29 dev eth0 lladdr 02:00:0a:aa:fe:1d REACHABLE ping下不在ARP表中的POD，发现ping命令“卡住了” 12[root@d tmp]# ping 10.170.254.10PING 10.170.254.10 (10.170.254.10) 56(84) bytes of data. 再将gc_thresh改大 1echo 80 &gt; /proc/sys/net/ipv4/neigh/default/gc_thresh3 ping命令立即恢复了 12345678[root@dns-8685bbc547-4dmld tmp]# ping 10.170.254.10PING 10.170.254.10 (10.170.254.10) 56(84) bytes of data.64 bytes from 10.170.254.10: icmp_seq=125 ttl=64 time=2004 ms64 bytes from 10.170.254.10: icmp_seq=126 ttl=64 time=1004 ms64 bytes from 10.170.254.10: icmp_seq=127 ttl=64 time=4.04 ms64 bytes from 10.170.254.10: icmp_seq=128 ttl=64 time=0.022 ms64 bytes from 10.170.254.10: icmp_seq=129 ttl=64 time=0.021 ms64 bytes from 10.170.254.10: icmp_seq=130 ttl=64 time=0.020 ms 总结 当将线上kubernetes宿主机的ARP参数都改大后，再也没有出现过此类问题了。 123sysctl -w net.ipv4.neigh.default.gc_thresh3=32768sysctl -w net.ipv4.neigh.default.gc_thresh2=16384sysctl -w net.ipv4.neigh.default.gc_thresh1=8192 虽然问题发生在kubernetes上，但是本质还是由于对底层OS不了解导致的。]]></content>
      <categories>
        <category>2021</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从使用者，DBA，内核开发三个不同角度来分析SQL的性能问题]]></title>
    <url>%2F2021%2F09%2F10%2Fpostgresql_sql_case%2F</url>
    <content type="text"><![CDATA[从使用者，DBA，内核开发三个不同角度来分析SQL的性能问题 ...... 有时候写的SQL有性能问题时往往束手无策，而求助于DBA。 今天，我们从使用者、DBA、内核开发三个不同的角度来分析一个有趣的SQL性能问题的案例， 从浅入深了解postgreSQL的优化器。 问题描述 同事A来问我这个假DBA一条SQL的性能问题： A：两条SQL语句只有limit不一样，而limit 1的执行比limit 10的慢N倍 我：是不是缓存问题，先执行limit 10再执行limit 1试试 A：......，执行了，limit还是很慢 两条SQL生产环境执行情况 limit 10 1select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 10; Execution Time: 1.307 ms limit 1 1select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 1; Execution Time: 144.098 ms 分析 执行计划 既然不是缓存问题，那我们先看看执行计划有什么不一样的 limit 1 1234567891011121314# explain analyze verbose select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 1; QUERY PLAN--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.43..416.25 rows=1 width=73) (actual time=135.213..135.214 rows=1 loops=1) Output: xxx -&gt; Index Scan Backward using user_gift_pkey on yay.user_gift (cost=0.43..368000.44 rows=885 width=73) (actual time=135.212..135.212 rows=1 loops=1) Output: xxx Filter: ((user_gift.user_id = 11695667) AND (user_gift.user_type = &apos;default&apos;::user_type)) Rows Removed by Filter: 330192 Planning Time: 0.102 ms Execution Time: 135.235 ms(8 rows)Time: 135.691 ms limit 10 123456789101112131415161718# explain analyze verbose select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 10; QUERY PLAN---------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=868.20..868.22 rows=10 width=73) (actual time=1.543..1.545 rows=10 loops=1) Output: xxx -&gt; Sort (cost=868.20..870.41 rows=885 width=73) (actual time=1.543..1.543 rows=10 loops=1) Output: xxx Sort Key: user_gift.id DESC Sort Method: top-N heapsort Memory: 27kB -&gt; Index Scan using idx_user_type on yay.user_gift (cost=0.56..849.07 rows=885 width=73) (actual time=0.020..1.366 rows=775 loops=1) Output: xxx Index Cond: (user_gift.user_id = 11695667) Filter: (user_gift.user_type = &apos;default&apos;::user_type) Planning Time: 0.079 ms Execution Time: 1.564 ms(12 rows)Time: 1.871 ms 可以看到，两个SQL执行计划不一样： limit 1语句 ：使用主键进行倒序扫描， Index Scan Backward using user_gift_pkey on yay.user_gift limit 10语句 ：使用(user_id, user_type)复合索引直接查找用户数据，Index Scan using idx_user_type on yay.user_gift 为什么执行计划不一样？ total cost 其实postgreSQL的执行计划并没有“问题”，因为limit 1的total cost Limit (cost=0.43..416.25 rows=1 width=73) 是416，run cost是416-0.43=415.57。而limit 10的total cost Limit (cost=868.20..868.22 rows=10 width=73)是868.22。 如果使用Index Scan Backward using user_gift_pkey的方式估算，那么limit 1成本是415, limit 2是415*2=830, limit 3 是 1245，大于868，所以当limit 3的时候会使用Index Scan using idx_user_type扫索引的计划。 验证 123456789101112131415161718192021# explain select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 2; QUERY PLAN------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.43..831.95 rows=2 width=73) -&gt; Index Scan Backward using user_gift_pkey on user_gift (cost=0.43..367528.67 rows=884 width=73) Filter: ((user_id = 11695667) AND (user_type = &apos;default&apos;::user_type))(3 rows) Time: 0.341 ms# explain select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 3; QUERY PLAN---------------------------------------------------------------------------------------------------------- Limit (cost=866.19..866.20 rows=3 width=73) -&gt; Sort (cost=866.19..868.40 rows=884 width=73) Sort Key: id DESC -&gt; Index Scan using idx_user_type on user_gift (cost=0.56..854.76 rows=884 width=73) Index Cond: (user_id = 11695667) Filter: (user_type = &apos;default&apos;::user_type)(6 rows) Time: 0.352 ms 结果显示： 当limit 2时，执行计划是Index Scan Backward using user_gift_pkey 当limit 3时，就改变计划了，Index Scan using idx_user_type on user_gift 实际执行时间 limit 1时成本估算的是416.25，比limit 10的868.22还是要快的。 但是实际limit 1执行cost是135.691 ms，而limit 10执行cost是1.871 ms，比limit 10慢了70倍！！！ 我们重新执行下explain，加上buffers选项 12345678910111213141516# explain (analyze, buffers, verbose) select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 1; QUERY PLAN--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.43..416.29 rows=1 width=73) (actual time=451.542..451.544 rows=1 loops=1) Output: xxx Buffers: shared hit=214402 read=5280 dirtied=2302 I/O Timings: read=205.027 -&gt; Index Scan Backward using user_gift_pkey on yay.user_gift (cost=0.43..368032.94 rows=885 width=73) (actual time=451.540..451.540 rows=1 loops=1) Output: xxx Filter: ((user_gift.user_id = 11695667) AND (user_gift.user_type = &apos;default&apos;::user_type)) Rows Removed by Filter: 333462 Buffers: shared hit=214402 read=5280 dirtied=2302 I/O Timings: read=205.027 Planning Time: 1.106 ms Execution Time: 451.594 ms(12 rows) 12345678910111213141516171819202122# explain (analyze, buffers, verbose) select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id desc limit 3; QUERY PLAN----------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=860.51..860.52 rows=3 width=73) (actual time=14.633..14.634 rows=3 loops=1) Output: xxx Buffers: shared hit=467 read=321 I/O Timings: read=10.112 -&gt; Sort (cost=860.51..862.72 rows=885 width=73) (actual time=14.632..14.632 rows=3 loops=1) Output: xxx Sort Key: user_gift.id DESC Sort Method: top-N heapsort Memory: 25kB Buffers: shared hit=467 read=321 I/O Timings: read=10.112 -&gt; Index Scan using idx_user_type on yay.user_gift (cost=0.56..849.07 rows=885 width=73) (actual time=0.192..14.424 rows=775 loops=1) Output: xxx Index Cond: (user_gift.user_id = 11695667) Filter: (user_gift.user_type = &apos;default&apos;::user_type) Buffers: shared hit=464 read=321 I/O Timings: read=10.112 Planning Time: 0.111 ms Execution Time: 14.658 ms(18 rows) 可以看出： limit 1时的IO成本I/O Timings: read=205.027，Rows Removed by Filter: 333462显示过滤了333462行记录 limit 3时IO成本I/O Timings: read=10.112， 从上面输出Buffers: shared hit=214402 read=5280 dirtied=2302可以看出limit 1的计划从磁盘读取了5280个blocks(pages)才找到符合where条件的记录。 为什么要读取这么多数据呢？我们来看看统计信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344schemaname | yaytablename | user_giftattname | idinherited | fnull_frac | 0avg_width | 8n_distinct | -1most_common_vals | most_common_freqs | histogram_bounds | &#123;93,9817,19893,28177,.......&#125;correlation | 0.788011most_common_elems | most_common_elem_freqs | elem_count_histogram | schemaname | yaytablename | user_giftattname | user_idinherited | fnull_frac | 0avg_width | 4n_distinct | -0.175761most_common_vals | &#123;11576819,10299480,14020501,.......,11695667,......&#125;most_common_freqs | &#123;0.000353333,0.000326667,0.000246667,......,9.33333e-05,......&#125;histogram_bounds | &#123;3,10002181,10005599,10009672,......,11693300,11698290,......&#125;correlation | 0.53375most_common_elems | most_common_elem_freqs | elem_count_histogram | schemaname | yaytablename | user_giftattname | user_typeinherited | fnull_frac | 0avg_width | 4n_distinct | 3most_common_vals | &#123;default, invalid, deleted&#125;most_common_freqs | &#123;0.997923,0.00194,0.000136667&#125;histogram_bounds | correlation | 0.99763most_common_elems | most_common_elem_freqs | elem_count_histogram | 从统计信息里可以看出： user_id字段的most_common_vals中有11695667(user_id)的值，则可以直接通过其对应的most_common_freqs来得到其selectivity是9.33333e-05； user_type字段为default对应的selectivity是0.997923。 所以where user_id=11695667 and user_type='default'的selectivity是0.0000933333*0.997923 = 0.0000931394467359。 那么可以估算出满足where条件的用户数是0.0000931394467359 * 9499740(总用户数) = 884.8，和执行计划(cost=0.43..367528.67 rows=884 width=73)的884行一样。 而优化器的估算是基于数据分布均匀这个假设的： 从user_gift_pkey(主键id)扫描的话：只要扫描9499740/884=10746行就能找到满足条件的记录，且无须进行排序(order by id desc) 从idx_user_type索引扫描的话：虽然能很快找到此用户的数据，但是需要给884行进行排序，扫描+排序的cost比从主键扫描要高。 那么数据分布真的均匀吗？继续查看数据的实际分布： 表最大的page=128709 12345# select max(ctid) from user_gift; max------------- (128709,29)(1 row) user id=11695667的最大page=124329 12345# select max(ctid), min(ctid) from user_gift where user_id=11695667; max | min-------------+----------- (124329,22) | (3951,64)(1 row) 表本身的pages和tuples数量 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;user_gift&apos;; relpages | reltuples----------+------------- 128875 | 9.49974e+06(1 row) 每个page存储的记录数：9.49974e+06 tuples / 128875 pages = 73.713 tuples/page。 计算：表(main table)的B+tree的最大page是128709，而实际用户11695667的最大page是124329，128709 - 124329 = 4380，需要扫描4380个page才能找到符合where条件的记录所在的page，所以过滤的rows是4380 pages * 73.713 tuples/page ≈ 322862。 实际limit 1时扫描了5280个pages(包含了主键索引的pages)，过滤了333462万行记录，和估算的基本一样： 123Rows Removed by Filter: 333462 Buffers: shared hit=214402 read=5280 dirtied=2302 I/O Timings: read=205.027 所以，此用户数据分布倾斜了： 优化器假设数据分布均匀，只需要扫描10746个记录 而实际需要扫描322862个记录 那么扫描5280个pages要多久？ 需要读取的数据量：5280pages * 8KB/page = 41.2MB的数据。 123456789[root]$ fio -name iops -rw=randread -bs=8k -runtime=10 -iodepth=1 -filename /dev/sdb -ioengine mmap -buffered=1...Run status group 0 (all jobs): READ: bw=965KiB/s (988kB/s), 965KiB/s-965KiB/s (988kB/s-988kB/s), io=9656KiB (9888kB), run=10005-10005msec[root]$ fio -name iops -rw=read -bs=8k -runtime=10 -iodepth=1 -filename /dev/sdb -ioengine mmap -direct=1...Run status group 0 (all jobs): READ: bw=513MiB/s (538MB/s), 513MiB/s-513MiB/s (538MB/s-538MB/s), io=5132MiB (5381MB), run=10001-10001msec 从fio结果可以看出，此数据库机器磁盘的顺序读取速度约为 500MB/s，如果数据都是顺序的，那么扫描40MB数据需要约80ms， 如果数据都是随机的，那么需要40秒。不是所有的数据都是顺序访问的，而且测试的是非线上机器，没有其他IO进程在运行。 到这里问题基本定位清楚了： &gt; postgreSQL的优化器认为数据分布是均匀的，只需要倒序扫描很快就找到符合条件的记录，而实际上此用户的数据分布在表的前端，就导致了实际执行start-up time如此慢了。 从内核视角来分析 我们从postgreSQL内核的角度来继续分析几个问题： 优化器如何估算cost 优化器如何统计actual time 表的信息 表结构 123456789101112131415# \d user_gift; Table &quot;yay.user_gift&quot; Column | Type | Collation | Nullable | Default--------------+--------------------------+-----------+----------+------------------------------------------------ id | bigint | | not null | nextval(&apos;user_gift_id_seq&apos;::regclass) user_id | integer | | not null | ug_name | character varying(100) | | not null | expired_time | timestamp with time zone | | | now() created_time | timestamp with time zone | | not null | now() updated_time | timestamp with time zone | | not null | now() user_type | user_type | | not null | &apos;default&apos;::user_typeIndexes: &quot;user_gift_pkey&quot; PRIMARY KEY, btree (id) &quot;idx_user_type&quot; btree (user_id, ug_name) &quot;user_gift_ug_name_idx&quot; btree (ug_name) 主键索引 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;user_gift_pkey&apos;; relpages | reltuples----------+------------- 40035 | 9.49974e+06(1 row) user_id 索引 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;idx_user_type&apos;; relpages | reltuples----------+------------- 113572 | 9.49974e+06(1 row) 表本身的pages是128875 12345# SELECT relpages, reltuples FROM pg_class WHERE relname = &apos;user_gift&apos;; relpages | reltuples----------+------------- 128875 | 9.49974e+06(1 row) user id=11695667的数据775行 1234567891011=# select count(1) from user_gift where user_id=11695667; count------- 775(1 row)=# select count(1) from user_gift where user_id=11695667 and user_type = &apos;default&apos; ; count------- 775(1 row) 树高度 1234567891011121314# 主键高度# select * from bt_metap(&apos;user_gift_pkey&apos;); magic | version | root | level | fastroot | fastlevel | oldest_xact | last_cleanup_num_tuples--------+---------+------+-------+----------+-----------+-------------+------------------------- 340322 | 3 | 412 | 2 | 412 | 2 | 0 | 9.31928e+06(1 row)// idx_user_type 高度# select * from bt_metap(&apos;idx_user_type&apos;); magic | version | root | level | fastroot | fastlevel | oldest_xact | last_cleanup_num_tuples--------+---------+-------+-------+----------+-----------+-------------+------------------------- 340322 | 3 | 15094 | 3 | 15094 | 3 | 0 | 9.49974e+06(1 row) 估算cost start-up cost postgreSQL对于每种索引的成本估算是不一样的，我们看看B+tree的start-up成本是如何估算的： 12345678910111213141516171819// selfuncs.cvoidbtcostestimate(PlannerInfo *root, IndexPath *path, double loop_count, Cost *indexStartupCost, Cost *indexTotalCost, Selectivity *indexSelectivity, double *indexCorrelation, double *indexPages)&#123; ...... descentCost = ceil(log(index-&gt;tuples) / log(2.0)) * cpu_operator_cost; costs.indexStartupCost += descentCost; ...... // This cost is somewhat arbitrarily set at 50x cpu_operator_cost per page touched descentCost = (index-&gt;tree_height + 1) * 50.0 * cpu_operator_cost; costs.indexStartupCost += descentCost; ......&#125; 其实start-up cost估算很简单，只考虑从B+tree的root page遍历到leaf page，且将这个page读入第一个tuple(记录)的cost。 start-up估算公式如下： \[ \left \{ ceil({\log_2 (N_{index,tuple})}) + (Height_{index} + 1) \times 50 \right \}\ \times cpu\_operator\_cost \] N(index,tuple) ：索引tuples(记录)数量 Height(index) ： 索引B+tree的高度 cpu_operator_cost : 默认值0.0025 使用user_gift_pkey计划的start-up cost 从上面表信息中可以看出： N(index,tuple) ：9.49974e+06， Height(index) ： 2 所以 \[ \left \{ ceil({\log_2 (9499740)}) + (2 + 1) \times 50 \right \}\ \times cpu\_operator\_cost = 173 \times 0.0025 = 0.435 \] 和postgreSQL估算的start-up cost=0.43 一样。 使用idx_user_type计划的start-up cost N(index,tuple) ：9.49974e+06， Height(index) ： 3 \[ \left \{ ceil({\log_2 (9499740)}) + (3 + 1) \times 50 \right \}\ \times cpu\_operator\_cost = 223 \times 0.0025 = 0.5575 \] 和postgreSQL估算的start-up cost=0.56 一样。 run cost run cost的估算是比较复杂的，判断的条件非常多，无法用一个固定的公式计算出来，所以这里只是简单描述下，有兴趣的可以看postgreSQL源码src/backend/optimizer/path/costsize.c的cost_index函数，针对这个案例，一般情况下可以根据此链接的脚本进行来模拟计算cost。 run cost \[ run\_cost = 索引成本 + 主表成本 \] 索引成本 \[ 索引成本 = 随机读取索引相关pages的成本 + 操作相关tuples的成本 \] 主表成本 \[ 主表成本 = max\_io\_cost + index\_correlation ^ 2 \times (min\_io\_cost - max\_io\_cost) \] ndex_correlation : 索引相关性。索引的顺序与主表数据排列顺序的关联性，用来描述通过索引扫描数据时，回表的顺序读的概率 max io cost（最坏情况下IO成本） 所有pages都是随机读取 \[ max\_io\_cost = pages\_fetched \times random\_page\_cost \] min_io_cost（最优情况下IO成本） 第一个page是随机读取，后面pages都是顺序读取 \[ min\_io\_cost = 1 \times random\_page\_cost + (pages\_fetched - 1) \times seq\_page\_cost \] actual start-up time vs estimated start-up cost 刚刚的分析中有一个疑问被忽略了：estimated start-up cost是开始执行计划到从表中读到的第一个tuple的cost(cost is an arbitrary unit)；而actual start-up time则是开始执行计划到从表中读取到第一个符合where条件的tuple的时间。这是为什么呢？ SQL处理流程：postgreSQL将SQL转化成AST，然后进行优化，再将AST转成执行器(executor)来实现具体的操作。不进行优化的执行器是这样的： 123456789101112131415161718┌──────────────┐│ projection │└──────┬───────┘ │ │┌──────▼──────┐│ limit │└──────┬──────┘ │ │┌──────▼──────┐│ selection │└──────┬──────┘ │ │┌──────▼──────┐│ index scan │└─────────────┘ 简化的执行流程如下： index scan executor：扫描到一个tuple，就返回给selection executor selection executor：对tuple进行过滤，如果符合条件则返回给limit executor，如果不符合则继续调用index scan executor limit executor：当达到limit限制则将数据返回给projection executor projection executor：过滤掉非select列的数据 那么如果进行优化，一般会将selection executor和projection executor合并到index scan executor中执行，以减少数据在executor之间的传递。 1234567891011┌─────────────┐│ limit │└──────┬──────┘ │ │┌──────▼──────┐│ index scan ││ ││ + selection ││ + projection│└─────────────┘ 优化后的执行流程： index scan executor：扫描到tuple，然后进行selection过滤，如果符合条件就进行projection再返回给limit，如果不符合条件，则继续扫描 limit executor：当达到limit限制则将数据返回 而通过下面代码可以看出，postgreSQL对于执行时间的统计是基于executor的， 123456789101112// src/backend/executor/execProcnode.cstatic TupleTableSlot *ExecProcNodeInstr(PlanState *node)&#123; TupleTableSlot *result; InstrStartNode(node-&gt;instrument); result = node-&gt;ExecProcNodeReal(node); // 统计执行指标 InstrStopNode(node-&gt;instrument, TupIsNull(result) ? 0.0 : 1.0); return result;&#125; 所以actual time的start-up是从启动executor直到扫描到符合where语句的第一条结果为止。 再看看实际的函数调用栈，user_id=xxx的过滤已经下沉到index scan executor里面了。 1234567891011121314151617181920---&gt; int4eq(FunctionCallInfo fcinfo) (/home/ken/cpp/postgres/src/backend/utils/adt/int.c:379) ExecInterpExpr(ExprState * state, ExprContext * econtext, _Bool * isnull) (/home/ken/cpp/postgres/src/backend/executor/execExprInterp.c:704) ExecInterpExprStillValid(ExprState * state, ExprContext * econtext, _Bool * isNull) (/home/ken/cpp/postgres/src/backend/executor/execExprInterp.c:1807) ExecEvalExprSwitchContext(ExprState * state, ExprContext * econtext, _Bool * isNull) (/home/ken/cpp/postgres/src/include/executor/executor.h:322)---&gt; ExecQual(ExprState * state, ExprContext * econtext) (/home/ken/cpp/postgres/src/include/executor/executor.h:391) ExecScan(ScanState * node, ExecScanAccessMtd accessMtd, ExecScanRecheckMtd recheckMtd) (/home/ken/cpp/postgres/src/backend/executor/execScan.c:227)---&gt; ExecIndexScan(PlanState * pstate) (/home/ken/cpp/postgres/src/backend/executor/nodeIndexscan.c:537) ExecProcNodeInstr(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:466) ExecProcNodeFirst(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:450) ExecProcNode(PlanState * node) (/home/ken/cpp/postgres/src/include/executor/executor.h:248)---&gt; ExecLimit(PlanState * pstate) (/home/ken/cpp/postgres/src/backend/executor/nodeLimit.c:96) ExecProcNodeInstr(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:466) ExecProcNodeFirst(PlanState * node) (/home/ken/cpp/postgres/src/backend/executor/execProcnode.c:450) ExecProcNode(PlanState * node) (/home/ken/cpp/postgres/src/include/executor/executor.h:248) ExecutePlan(EState * estate, PlanState * planstate, _Bool use_parallel_mode, CmdType operation, _Bool sendTuples, uint64 numberTuples, ScanDirection direction, DestReceiver * dest, _Bool execute_once) (/home/ken/cpp/postgres/src/backend/executor/execMain.c:1632) standard_ExecutorRun(QueryDesc * queryDesc, ScanDirection direction, uint64 count, _Bool execute_once) (/home/ken/cpp/postgres/src/backend/executor/execMain.c:350) ExecutorRun(QueryDesc * queryDesc, ScanDirection direction, uint64 count, _Bool execute_once) (/home/ken/cpp/postgres/src/backend/executor/execMain.c:294) ExplainOnePlan(PlannedStmt * plannedstmt, IntoClause * into, ExplainState * es, const char * queryString, ParamListInfo params, QueryEnvironment * queryEnv, const instr_time * planduration, const BufferUsage * bufusage) (/home/ken/cpp/postgres/src/backend/commands/explain.c:571) ExplainOneQuery(Query * query, int cursorOptions, IntoClause * into, ExplainState * es, const char * queryString, ParamListInfo params, QueryEnvironment * queryEnv) (/home/ken/cpp/postgres/src/backend/commands/explain.c:404) ExplainQuery(ParseState * pstate, ExplainStmt * stmt, ParamListInfo params, DestReceiver * dest) (/home/ken/cpp/postgres/src/backend/commands/explain.c:275) 下面代码是scan的实现，其中的ExecQual(qual, econtext)是对tuple进行过滤，因为selection已经合并到scan中了。 123456789101112131415161718192021222324252627TupleTableSlot *ExecScan(ScanState *node, ExecScanAccessMtd accessMtd, ExecScanRecheckMtd recheckMtd)&#123; ...... for (;;) &#123; TupleTableSlot *slot; slot = ExecScanFetch(node, accessMtd, recheckMtd); ...... econtext-&gt;ecxt_scantuple = slot; // Note : selection判断 if (qual == NULL || ExecQual(qual, econtext)) &#123; if (projInfo) &#123; return ExecProject(projInfo); &#125; else &#123; return slot; &#125; &#125; else InstrCountFiltered1(node, 1); &#125;&#125; 解决方案 禁用走主键扫描 既然计划走的是user_gift_pkey倒序扫描，那么我们可以手动避免优化器使用这个索引。 1# explain analyze verbose select xxx from user_gift where user_id=11695667 and user_type = &apos;default&apos; order by id+0 desc limit 1; 将order by id改成order by id+0，由于id+0是个表达式所以优化器就就不会使用user_gift_pkey这个索引了。 这个方案不适合所有场景，如果数据分布均匀的话则某些情况下使用user_gift_pkey扫描更加合理。 增加(user_id, id)索引 1create index idx_user_id on user_gift(user_id, id); 通过增加where条件列和排序键的复合索引，来避免走主键扫描。 写在最后 从排除缓存因素，分析查询计划，定位数据分布倾斜，到调试内核源码来进一步确定原因，最终成功解决性能问题。通过这个有趣的SQL优化经历，相信能给大家带来收获。]]></content>
      <categories>
        <category>2019</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>PostgreSQL</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[译文]TreadMarks: 基于工作站网络的共享内存计算]]></title>
    <url>%2F2020%2F04%2F01%2FTreadMarks%2F</url>
    <content type="text"><![CDATA[TreadMarks: 基于工作站网络的共享内存计算 以前学MIT6.824时看过TreadMarks相关论文，这篇论文当时只翻译了一半。最近无意中看到这篇未完成的翻译，google了下发现仍然没有人翻译过这篇论文，于是借个周末时间翻译完了。 论文: TreadMarks: Shared Memory Computing on Networks of Workstations Christiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Ra jamony, Weimin Yu and Willy Zwaenepoel Department of Computer Science Rice University 摘要（Abstract） TreadMarks通过为应用程序提供共享的内存抽象来支持基于工作站网络上的并行计算。共享内存有助于从顺序程序转换成并行程序，因为大多数数据结构都无需做更改。 只需要添加同步操作。 我们会讨论在TreadMarks中使用的用于提供有效共享内存的技术，并讨论了两个主要应用程序的经验，即混合整数编程和遗传连锁分析。 1 引言（ Introduction） 高速网络和快速提高的微处理器性能使工作站的网络成为越来越有吸引力的并行计算工具。通过仅仅依靠廉价硬件和软件，工作站网络可以以相对较低的成本提供并行处理。可以将工作站网络的多处理器实现成处理器组[17]，这是许多专用于提供计算周期的处理器。或者，它也可以由一组动态变化的机器组成，在这些机器上利用空闲周期执行长时间运行的计算[14]。在后一种情况下，（硬件）成本基本上为零，因为许多组织机构已经拥有广泛的工作站网络。在性能方面，处理器速度，网络带宽和延迟的改进使联网的工作站能够为越来越多的应用程序提供接近或超过超级计算机的性能。我们的立场不是让更紧密耦合的设计在这种松耦合的多处理器设计面前显得过时。特别是，更低延迟和更高带宽的这些紧耦合的设计能有效地执行对同步和通信要求更加严格的应用程序。但是，我们认为网络技术和处理器性能的进步将极大地扩展可以在工作站网络上有效执行的应用程序的种类。 在论文中，我们会讨论使用TreadMarks分布式共享内存（DSM）系统在工作站网络上进行并行计算的经验。DSM允许进程假定全局共享的虚拟内存，即使它们运行的节点并没有在物理上共享内存[13]。图1举例说明了一个DSM系统，该系统由N个联网的工作站组成，每个工作站都有自己的内存，并通过网络连接起来。DSM软件提供了全局共享内存的抽象，每个处理器都可以访问任何数据项，而程序员不必担心数据在哪里或如何获取。相反，在基于工作站网络的“本地”编程模型中，程序员必须确定处理器何时需要进行通信，与谁进行通信以及要发送什么数据。对于具有复杂数据结构和复杂并行化策略的程序，这可能成为艰巨的任务。在DSM系统上，程序员可以专注于算法开发，而不是管理分区的数据集和数据传输。除了易于编程之外，DSM还提供了与（硬件）共享内存多处理器相同的编程环境，从而允许在两种环境之间进行移植。最后，DSM考虑到无缝集成在网络环境中共享内存的多处理器工作站。 本文描述的实际系统TreadMarks [7]，在Unix工作站上以用户级别运行。不需要进行内核修改或使用特权，并且使用标准Unix编译器和链接器。实现DSM系统的挑战在于确保共享内存抽象不会导致大量通信。TreadMarks中使用了多种技术来应对这一挑战，包括lazy release consistency(懒惰更新释放一致性)[6]和multiple writer protocols（多写协议）[3]。本文首先会描述TreadMarks提供的应用程序编程接口（第2节），接下来，我们会讨论实现方面的挑战。（第3节）以及用于应对这些挑战的技术（第4和第5节）。我们在第6节中会简要的描述TreadMarks的实现，并通过讨论我们在混合整数编程和遗传连锁分析（第7节）这两个大型应用程序中的经验来证明其效率。最后，我们在第8节中讨论相关工作，并在第9节中提供一些结论，以及进一步的工作的方向。 arch1.png 图１ 分布式共享内存 2 共享内存编程（Shared Memory Programming） 2.1 应用程序编程接口（Application Programming Interface） TreadMarks API简洁但功能强大（有关C语言接口，见图2）。它提供了用于进程创建和销毁，同步以及共享内存分配的功能。我们专注于同步原语。 共享内存会引起数据竞争。当不同进程以程序员不希望的方式交错对共享变量访问时，就会发生数据竞争。例如，假设一个进程写入共享记录的多个字段，而另一个进程尝试读取这些相同的字段。第二个进程很有可能读取到新写入的值，但也有可能在第一个进程写入新值前读取到旧值。通常不希望发生这种情况。 为了避免这种情况，共享内存并行程序包含同步操作。 在这种特殊情况下，将使用锁来允许每个进程对记录的独占访问。 TreadMarks提供了两种同步原语：barrier(屏障)和exclusive locks(排他锁)。进程通过调用 Tmk_barrier() 来等待屏障。屏障是全局的：调用进程会被暂停，直到系统中的所有进程到达同一屏障为止。进程调用Tmk_lock_acquire来获取锁，然后调用Tmk_lock_release来释放锁。当另一个进程持有该锁时，任何进程都无法获取该锁。同步原语的这种特定选择跟TreadMarks的设计没有任何关系；以后可能会增加其他原语。 我们通过两个简单的应用程序演示了这些以及其他TreadMarks原语的用法。 2.2 两个简单的示例（Two Simple Illustrations） 图3和图4说明了TreadMarks API在Jacobi迭代和解决旅行商问题（TSP）中的使用。 我们很清楚这些示例代码的过于简单。 但在这里仅用于演示目的。 实际应用将在第7节中讨论。 Jacobi是一种求解偏微分方程的方法。我们的示例遍历二维数组。在每次迭代期间，每个矩阵元素都会更新为其最近元素的平均值（上方，下方，左侧和右侧）。Jacobi使用临时数组存储每次迭代期间计算的新值，以避免元素的旧值在其被临近元素使用之前被覆盖。在并行版本中，为所有处理器分配的数量大致相等的行。 边界上的行由两个相邻进程共享。 图3中的TreadMarks版本使用两个数组：一个网格和一个临时数组。网格的保存在共享内存中，而临时的是每个进程专有的。网格由进程0分配和初始化。Jacobi中的同步是通过屏障实现的。 Tmk_barrier(0) 确保在开始计算之前，进程0的初始化对所有进程可见。Tmk_barrier(1) 确保所有进程在上一次迭代中完成读取值之前，不会有进程会覆盖网格中的任何值。Tmk barrier(2) 可防止当前迭代计算中的所有网格的值被设置前，任何进程都不可以开始下一次迭代。 旅行商问题（TSP）是查找从指定城市开始，经过地图上所有其他城市恰好一次并返回开始城市的最短路径。使用了一种简单的分支定界算法。每次局部旅行一次就扩展一个城市。如果局部旅行的长度加上路径剩余部分的下限大于当前最短旅行路径的话，则不会再进一步进行局部旅行了，因为他不会产生比当前最短路径更短的路径了。 该程序维护一个局部旅行的列队，保持最短的局部旅行在列队最前面。它不断往列队中添加可能最优的局部旅行，直到从列队的开头开始找到一个比阈值长的路径为止。然后程序将它从列队中移除，并尝试剩余程序的所有排列。最终，它将比较当前路径和包含此路径的最短路径，且如果有必要的话就更新当前最短路径。列队和当前最短路径是共享的，通过锁来保护对他们的访问。 123456789101112131415161718192021222324/* the maximum number of parallel processes supported by TreadMarks */#define TMK_NPROCS/* the actual number of parallel processes after Tmk_startup */extern unsigned Tmk_nprocs;/* the process id, an integer in the range 0 ... Tmk_nprocs - 1 */extern unsigned Tmk_proc_id;/* the number of lock synchronization objects provided by TreadMarks */#define TMK_NLOCKS/* the number of barrier synchronization objects provided by TreadMarks */#define TMK_NBARRIERS/* Initialize TreadMarks and start the remote processes */void Tmk_startup(argc, argv) int argc; char **argv;/* Terminate the calling process. Other processes are unaffected. */void Tmk_exit(status) int status;/* Block the calling process until every other process arrives at the barrier. */void Tmk_barrier(id) unsigned id;/* Block the calling process until it acquires the specified lock. */void Tmk_lock_acquire(id) unsigned id;/* Release the specified lock. */void Tmk_lock_release(id) unsigned id;/* Allocate the specified number of bytes of shared memory */char *Tmk_malloc(size) unsigned size;/* Free shared memory allocated by Tmk_malloc. */void Tmk_free(ptr) char *ptr; 图 2 TreadMarks C语言接口 1234567891011121314151617181920212223242526272829#define M 1024#define N 1024float **grid;float scratch[M][N];Tmk_startup();if (Tmk_proc_id == 0)&#123; grid = Tmk_malloc(M * N * sizeof(float)); initialize grid;&#125;Tmk_barrier(0);length = M / Tmk_nprocs;begin = length * Tmk_proc_id;end = length * (Tmk_proc_id + 1);for (number of iterations)&#123; for (i = begin; i &lt; end; i++) for (j = 0; j &lt; N; j++) scratch[i][j] = (grid[i - 1][j] + grid[i + 1][j] + grid[i][j - 1] + grid[i][j + 1]) / 4; Tmk_barrier(1); for (i = begin; i &lt; end; i++) for (j = 0; j &lt; N; j++) grid[i][j] = scratch[i][j]; Tmk_barrier(2);&#125; 图 3 The TreadMarks Jacobi 程序 实现的挑战（Implementation Challenges） 提供内存一致性是DSM系统的核心：DSM软件必须以一种提供全局共享内存的方式在处理器之间移动数据。在李的原始IVY系统中[13]，虚拟内存硬件用于维护内存一致性。每个处理器的本地（物理）存储器形成全局虚拟地址空间的高速缓存（请参见图5）。如果一个页面不在本地处理器的内存中，则会产生缺页错误。DSM软件将该页面的最新副本从其远程位置导入本地内存，然后重新启动该过程。例如，图5显示了处理器1发生缺页错误，然后从处理器3的本地内存中复制需要页面的副本。IVY进一步将读取错误与写入错误区分开。发生读取错误时，页面将所有副本以只读方式进行复制，而发生写入错误时，所有现有副本都将无效，并且写的处理器保留唯一副本。 1234567891011121314151617181920212223242526272829queue_type *Queue;int *Shortest_length;int queue_lock, tour_lock;main()&#123; Tmk_startup(); queue_lock = 0; tour_lock = 1; if (Tmk_proc_id == 0) Queue = Tmk_malloc(sizeof(queue_type)); Shortest_length = Tmk_malloc(sizeof(int)); initialize Heap and Shortest_length Tmk_barrier(0); Tmk_lock_acquire(queue_lock); Exit if queue is empty; Keep adding to queue until a long, promising tour appears at the head; Path = Delete the tour from the head; Tmk_lock_release(queue_lock); length = recursively try all cities not on Path, find the shortest tour length Tmk_lock_acquire(tour_lock); if (length &lt; *Shortest_length) *Shortest_length = length; Tmk_lock_release(tour_lock);&#125; 图4 The TreadMarks TSP 程序 尽管很简单，但DSM的IVY实现可能产生大量通信。而在工作站网络上通信的成本很高。发送消息可能涉及到切入操作系统内核，中断，上下文切换以及可能经过多层网络协议的处理。因此，必须保持较少的消息的数量。 我们使用图3和图4的示例来说明IVY中的一些通信问题。 第一个问题与IVY的一致性模型有关，通常称为顺序一致性（sequential consistency） [9]。粗略地说，顺序一致性要求对共享内存的写入对于其他处理器“立即”变为可见。这就是为什么IVY在写入共享内存之前发送无效消息的原因。在许多情况下，顺序一致性提供了过强的保证。 例如，思考下图4中TSP中最佳巡回的更新及其长度。在IVY中，两个共享内存更新将导致发送无效信息到所有其他缓存了这些变量的页面的处理器。 但是，其实只会在有相应锁保护的临界区内访问这些变量，因此仅将无效发送给获取锁的下一个处理器就足够了，并且仅在获取锁时才发送。 第二个问题涉及到潜在的伪共享（false sharing）的问题。当位于同一个页面的两个或两个以上不相关的数据对象由不同的处理器并发写入时，就会导致伪共享，从而导致该页在处理器之间像乒乓球一样来回移动。由于一致性单元很大（虚拟内存页），所以伪共享是一个潜在的严重问题。图6展示了Jacobi中网格数组可能的页面布局。 当两个处理器都更新其网格数组的一部分时，它们正在同时写入同一页面，并导致该页面在处理器间来回移动多次。 ivh1.png 图５ IVH DSM系统操作 4 懒惰更新释放一致性（Lazy Release Consistency） 4.1 释放一致性（Release Consistency） 释放一致性[5]是一个宽松的内存一致性模型。它不能始终保证一致性。 对共享内存更新的传播可能会延迟一段时间。这样，释放一致性的实现可以将多个更新消息合并为单个消息，从而减少消息的数量。但是，通过在某些同步操作中强制执行一致性，至少对于正确同步的程序而言，对程序员屏蔽了潜在的不一致性。我们都在直观层面上讨论这些问题。关于更详细的说明，读者可以参考Gharachorloo 等人的论文 释放一致性的直觉如下。并行程序不应出现数据竞争，因为这会导致不确定的结果。因此，必须充分的使用同步来防止数据竞争。更具体地说，在对共享内存的两个冲突性访问之间必须存在同步（如果两个访问相同的内存位置，并且其中至少一个是写操作，则这两个访问是冲突性的）。由于存在这种同步，因此DSM系统可以延迟将更新从一个进程发送到另一个进程，直到发生这样的同步事件为止，因为只有执行了同步操作，第二个进程才会访问数据。对于没有数据竞争的程序，顺序一致性和释放一致性的结果是相同的。 我们将通过第2节中的Jacobi和TSP示例来说明这个原理。在Jacobi中，执行barrier 1 后才进行写共享内存操作，此时将新计算的值从临时数组复制到网格数组中。执行barrier 2时计算阶段才结束。执行barrier 2是为了避免在所有新值被写入网格数组前进程就开始下一次迭代。不管是什么存储模型，都必须需要barrier来确保正确性。但是，它的存在使我们可以推迟网格数组元素新值的传播，直到遇到更低的barrier。 在TSP中，任务队列是主要的共享数据结构。处理器从队列中获取任务并对其进行处理，也会创建新的任务。 这些新创建的任务将插入队列。任务队列结构的更新需要对共享内存进行一系列的写入，例如其大小，位于队列开头的任务等。原子地对任务队列数据结构进行访问是为了保证程序的正常运行。一次仅允许一个处理器访问任务队列数据结构。这些保证通过在操作前加锁，操作后解锁来实现。因此只有获得锁的进程才能访问数据结构，也无须立即将更新传播给其他进程。这些处理器需要首先获得锁。因此在获得锁的时候传播数据结构的更新就可以了。 这两个示例说明了释放一致性的基本原理。在共享内存的并行程序中引入了同步，以防止进程在同步操作完成之前查看某些内存。由此得出结论，在同步操作完成之前，不必传播那些内存操作的值。相反，顺序一致性会立即更新每个远程内存。 释放一致性需要少得多的消息，并导致更少的消息延迟到达。 程序员不必对此太在意。 如果程序没有数据竞争，则它的行为就好像在普通的共享内存系统上执行一样，其条件是：所有同步都必须使用TreadMarks提供的原语来完成。 否则，TreadMarks无法知道何时应该让共享内存保持一致性。 false_sharing1.png 图６ Jacobi的伪共享例子 4.2 懒惰更新释放一致性（Lazy Release Consistency） TreadMarks使用懒惰更新释放一致性算法[6]来实现释放一致性。粗略地说，懒惰更新释放一致性在获取锁时确保一致性，相反的，Munin早期实现的释放一致性在释放锁时实施一致性。懒惰更新释放一致性发送的消息更少。在锁释放时，Munin的释放锁的处理器发送缓存了修改数据的所有其他处理器。比较起来，在懒惰更新释放一致性中，一致性消息仅在锁的最后一个释放者和新获取者之间传播。 懒惰更新释放一致性（lazy release consistency）比急迫更新释放一致性（eager release consistency）更复杂一些。在释放后，Munin可以忘记释放的处理器在释放之前所做的所有修改。懒惰更新释放一致性不是这种情况，因为第三个处理器以后可能会获取锁并需要查看修改。在实践中，我们的经验表明，对于工作站网络而言，发送消息的成本很高，减少消息数量所带来的收益超过了更复杂的实现的成本。 5 多写协议（Multiple-Writer Protocols） 大多数硬件缓存和DSM系统使用单写程序协议。这些协议允许多个读同时访问同一页面，但是在执行任何修改之前，只允许一个写对页面进行修改。单写程序协议易于实现，因为给定页面的所有副本始终都是相同的，并且始终可以通过从当前具有有效副本的任何其他处理器中获取页面的副本来解决缺页错误。不幸的是，这种简单性通常以牺牲消息流量为代价的。 在写页面之前，所有其他副本都必须无效。如果其页面已失效的处理器仍在访问该页面的数据，这些失效则可能导致随后的访问丢失。 伪共享会由于无关访问之间的干扰而导致单写程序协议的性能更差。DSM系统通常比硬件系统遭遇更多的伪共享，因为它们以虚拟内存页面而不是高速缓存行的粒度跟踪数据访问。 顾名思义，多写协议允许多个写同时修改同一页面，并延后发送保证一致性的消息，尤其是直到同步发生为止 TreadMarks使用虚拟内存硬件来检测对共享内存页面的访问和修改。图7显示了如何使用保护故障（protection faults）来创建差异（diffs）。有效页面最初被写保护。 发生写操作时，TreadMarks会创建虚拟内存页的一个副本，或一个twin（双胞胎），并将其保存在系统空间中。当需要将修改发送到另一个处理器时，则将页面的当前副本与twin逐字比较，并将变化的字节保存到diff数据结构中。一旦创建diff后，twin将被丢弃。 除了处理器第一次访问页面外，其页面副本仅通过应用diff进行更新。 不再需要页面的新完整副本。 使用diff的主要好处是它们可用于实现多写协议，由于diff通常比页面小得多，因此它们还可显着降低总体带宽需求。 diff1.png 图７ DiffCreation ６ TreadMarks系统（The TreadMarks System） TreadMarks完全以Unix上用户库的方式实现的。不需要修改Unix内核，因为现代Unix提供了在用户级别实现TreadMarks所需的所有通信和内存管理功能。用C，C++或者FORTRAN编写的程序可以用这些语言标准的编译器来编译和连接TreadMarks库。该系统是相对便携式的。 当前，它可以在SPARC，DECStation，DEC / Alpha，IBM RS-6000，IBM SP-1和SGI平台以及以太网和ATM网络上运行。 在本节中，我们简要描述如何实现TreadMarks的通信和内存管理。 TreadMarks使用Berkeley套接字接口实现了机器间通信。取决于底层的网络硬件，例如以太网或ATM，TreadMarks使用UDP / IP或AAL3 / 4作为消息传输协议。默认情况下，除非计算机通过ATM LAN连接，否则TreadMarks使用UDP / IP。AAL3 / 4是ATM标准规定的面向连接的最大努力交付协议（best-efforts delivery protocol）。 由于UDP / IP和AAL3 / 4都不保证可靠的传递，因此TreadMarks使用轻量，特定操作的用户级协议来确保消息到达。 TreadMarks发送的每个消息要么是请求消息要么是响应消息。TreadMarks发送的消息可能是显式调用TreadMarks库产生的或者是缺页错误产生的。一旦机器发送了请求消息，它将阻塞直到有一个请求消息或者预期的响应消息到达。为了将请求处理的延迟最小化，TreadMarks使用SIGIO信号的处理程序来处理请求。消息到达用于接收请求消息的任何套接字都会产生SIGIO信号。由于AAL3 / 4是面向连接的协议，因此与其他每台机器都有一个相对应的套接字。为了确定哪个套接字处理请求，处理程序将执行select系统调用。处理程序收到请求消息后，将执行指定的操作，发送响应消息，然后返回到中断的进程。 为了实现一致性协议，TreadMarks使用mprotect系统调用来控制对共享页面的访问。尝试在共享页面上执行受限访问都会产生SIGSEGV信号。SIGSEGV信号处理程序检查本地数据结构以确定页面的状态。如果本地副本是只读的，则该处理程序从空闲页面池中分配一个页面，并执行bcopy来创建一个twin。最后，处理程序将访问权限升级到原来的页面并返回。如果本地页面无效，则处理程序执行一个过程（procedure）通过从最少的远程计算机集中获取对共享内存的必要更新。 7 应用（Applications） 使用TreadMarks已实现了许多应用程序，并且一些基准测试的性能已在之前进行了报道[7]。在这里，我们描述最近使用TreadMarks实现的两个大型应用程序的经验。在本文作者的帮助下，顺序执行代码的作者将混合整数编程（mixed integer programming）和遗传连锁分析（genetic linkage analysis）这些应用程序从现有的顺序代码进行了并行处理。虽然很难量化所涉及的工作量，但已经证明为获得有效的并行代码而进行的修改的工作量相对很小，这将在本节的其余部分中演示。 7.1 混合整数编程（Mixed Integer Programming） 混合整数编程（MIP）是线性编程（LP）的一个版本。在LP中，在由一组线性不等式描述的区域中优化了目标函数。在MIP中，部分或全部变量被约束为只能用整数值（有时仅是0或1）。 图8显示了一个精确的数学公式，图9显示了一个简单的二维实例。 mip3.png 图 8 混合整数编程问题(MIP) mip_two1.png ​ 图 9 MIP二维实例 branch_and_bound_mip2.png 图10 解决MIP问题的分支界定法 Lee等人实现了用于解决MIP问题的TreadMarks代码 [11]。该代码使用分支剪切法。首先将MIP问题简化为相应的LP问题。通常，这个LP问题的解决方案将为某些约束为整数的变量产生非整数值。下一步是选择这些变量中的一个，并分支出两个新的LP问题，一个问题具有 \(x_{i} = \left \lfloor x_{i} \right \rfloor\) 的附加约束，另一个具有 \(x_{i} = \left \lceil x_{i} \right \rceil\) 的附加约束（见图10）。慢慢地，该算法会生成此类分支的树。找到解决方案后，此解决方案便会在解决方案上建立界限。分支中的LP问题的解决方案产生的结果低于此边界则无需再进入下一步了。为了加快此过程，该算法使用一种称为plunging的技术，本质上是对树进行深度优先搜索，以找到整数解并尽快建立边界。最后一种感兴趣的改进的算法是cutting planes的使用。 这些是添加到LP问题的附加约束，以加强对整数问题的描述。 该代码用于解决MIPLIB库中的所有51个问题。该库包括航空人员调度，网络流量，工厂位置，机队调度等方面的代表性示例。图11显示了在8台运行Ultrix 4.3且通过100Mbps Fore ATM交换机连接的DecStation-5000 / 240机器的网络上获得的加速，这些问题的顺序运行时间超过2,000秒。对于大多数问题，加速几乎是线性的。 一个问题表现为超线性加速，这是因为并行代码命中了前期运行的解决方案，从而裁剪掉了大多数分枝定界树。对于另一个问题，几乎没有加速，因为在预处理步骤之后不久就找到了解决方案，此时还没有进行并行化。除了MIPLIB库中的问题外，该代码还可用于解决以前无法解决的多商品流问题[2]。 该问题在具有8个处理器的IBM SP-上花费的CPU时间为30天，并且还表现出接近线性的加速。 miplib2.png 图11 MIPLIB库的结果 7.2 遗传连锁（Genetic Linkage） 遗传连锁分析是一种统计技术，使用家族谱系信息来绘制人类基因图谱并在人类基因组中定位疾病基因。生物学和遗传学的最新进展已提供了大量可用的遗传材料，使计算成为进一步研究的瓶颈。 在Mendelian经典的继承理论中，孩子的染色体接收父母一方每个染色体的一条链。考虑重组时，情况会有所不同。如果发生重组，则孩子的染色体链将包含父母染色体的每一条链中的每一条（见图12）。连锁分析的目的是推导我们寻找的基因与已知位置的基因之间发生重组的概率。 从这些概率可以计算出基因在染色体上的大概位置。 ILINK [4]是广泛使用的遗传连锁分析程序的一个并行版本，它是LINKAGE [10]程序包的一部分。ILINK将称为谱系的家谱作为输入，并增加了有关该家族成员的一些遗传信息。它计算重组概率θ的最大似然估计。从最上层来看，ILINK由一个优化θ的循环组成。在优化循环的每次迭代中，程序遍历整个谱系，一次遍历一个核心家庭，在已知家庭成员的遗传信息的情况下，计算当前θ的可能性。对于核心家庭的每个成员，该算法都会更新一系列条件概率，这些条件概率表示个体具有特定遗传特征的概率，其条件取决于θ和已经遍历的部分家谱。 通过以均衡负载的方式在可用处理器之间分配每个核心家庭的迭代空间，可以并行化上述算法。负载均衡是必不可少的，且依赖于阵列元素中表示的遗传信息的知识。另一个方式是拆分树，它未能产生良好的加速效果，因为大多数计算都发生在树的一小部分上（通常，这些节点靠近树的根部，他们表示已知遗传信息很少的已故者）。另外，无法并行评估不同的θ值，因为优化程序会从一个θ值顺序移至下一个θ值。 图13显示了使用ILINK为各种数据集获得的加速。数据集来自对真实的疾病基因定位的研究。对于运行时间较长的数据集，可以实现良好的加速。 对于最小的数据集BAD，由于通信与计算的比率变大，因此加速要少得多。 dna1.png 图12 DNA重组 8 相关工作（Related Work） 本节的目标不是进行广泛的并行编程研究，而是以一个独特的示例说明替代性方案，并将最终的系统与TreadMarks进行比较。 消息传递（PVM）。 当前，消息传递是分布式存储系统的主要编程范例。便携式虚拟机（PVM）[15]是一种流行的消息传递软件。它允许将异构的计算机网络视为单个并发计算引擎。尽管PVM中的编程比底层机器的本机消息传递范例中的编程容易得多，并且可移植性强，但是应用程序程序员仍然需要编写代码来显式交换消息。TreadMarks的目标是减轻程序员的负担。 对于具有复杂数据结构和复杂并行化策略的程序，我们认为这是一个主要优势。 隐式并行（HPF）。 如HPF [8]中所示，隐式并行性依赖于用户提供的数据分布，编译器随后使用这些数据分布来生成消息传递代码。这种方法适用于数据并行程序，例如Jacobi。 动态并行性的程序，例如TSP，ILINK或MIP，很难在HPF框架中表达。 面向对象的并行计算（Orca）。Orca [16]和其他面向对象的并行计算系统一样没有为程序员提供共享的存储空间，而是支持对象的共享空间，每个对象都可以通过适当同步的方法进行访问。除了从编程的角度来看的优点之外，这种方法还允许编译器推断出某些优化方法，这些优化方法可用于减少通信量。缺点是，顺序程序中“自然”的对象通常不是正确的并行化对象，需要更多的更改才能获得有效的并行程序 硬件共享内存实现（DASH）。 另一种方法是在硬件中实现共享内存，对少量处理器使用侦听总线协议，或对大量处理器使用基于目录的协议（例如[12]）。我们使用这种方法来共享编程模型，但是我们的实现避免了昂贵的缓存控制器硬件。 另一方面，硬件实现可以有效地支持具有更细粒度并行性的应用程序。 条目一致性（Entry Consistency）（中途）。 条目一致性是另一种宽松的内存模型[1]。它要求所有共享数据都与同步对象相关联。当获取同步对象时，只传输与该同步对象关联的修改后的数据，从而进一步减少通信数据量。但是，条目一致性内存模型比释放一致性弱，这可能会使编程更加困难。 ilink.png 图13 ILINK结果 9 总结和下一步工作（Conclusions and Further Work） 我们的经验表明，使用适当的实现技术，分布式共享内存可以为工作站网络上的并行计算提供有效的平台。多数应用程序移植到TreadMarks分布式共享内存系统上，几乎没有困难，并且性能良好。在下一步的工作中，我们打算尝试其他实际应用，包括地震建模代码。我们还在开发各种工具，以进一步减轻编程负担并提高性能。特别是，我们正在研究使用诸如支持预取的编译器，以及使用性能监视工具来消除不必要的同步。 引用（References） [1] B.N. Bershad, M.J. Zekauskas, and W.A. Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, pages 528-537, February 1993. [2] D. Bienstock and O. Gumluk. Computational experience with a difficult mixed-integer multi-commodity flow problem. To appear in Mathematical Programming, 1994. [3] J.B. Carter, J.K. Bennett, and W. Zwaenepoel. Implementation and performance of Munin. In Proceedings of the 13th ACM Symposium on Operating Systems Principles, pages 152-164, October 1991. [4] S. Dwarkadas, A.A. Schäffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. Parallelization of general linkage analysis problems. Human Heredity, 44:127-141, 1994. [5] K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. Memory consistency and event ordering in scalable shared-memory multiprocessors. In Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 15-26, May 1990. [6] P. Keleher, A. L. Cox, and W. Zwaenepoel. Lazy release consistency for software distributed shared memory. In Proceedings of the 19th Annual International Symposium on Computer Architecture, pages 13-21, May 1992. [7] P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: Distributed shared memory on standard workstations and operating systems. In Proceedings of the 1994 Winter Usenix Conference, pages 115-131, January 1994. [8] C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. The High Performance Fortran Handbook. The MIT Press, 1994. [9] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess programs. IEEE Transactions on Computers, C-28(9):690{691, September 1979. [10] G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. Strategies for multilocus linkage analysis in humans. Proc. Natl. Acad. Sci. USA, 81:3443-3446, June 1984. [11] E. Lee, R. Bixby, W. Cook, and A.L. Cox. Parallelism in mixed integer programming. Submitted for publication, 1994. [12] D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. The directory-based cache coherence protocol for the DASH multiprocessor. In Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 148-159, May 1990. [13] K. Li and P. Hudak. Memory coherence in shared virtual memory systems. ACM Transactions on Computer Systems, 7(4):321-359, November 1989. [14] M. Litzkow, M. Livny, and M. Mutka. Condor - a hunter of idle workstations. In Proceedings of the 8th International Conference on Distributed Computing Systems, pages 104-111, June 1988. [15] V. Sunderam. PVM: A framework for parallel distributed computing. Concurrency:Practice and Experience, 2(4):315-339, December 1990. [16] A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal. Parallel programming using shared ob jects and broadcasting. IEEE Computer, 25(8):10-20, August 1992. [17] A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, J. Jansen, and G. van Rossum. Experiences with the Amoeba distributed operating system. Communications of the ACM, 33(12):46-63, December 1990.]]></content>
      <categories>
        <category>2020</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[percolator的理解与开源实现分析]]></title>
    <url>%2F2019%2F02%2F17%2Fpercolator%2F</url>
    <content type="text"><![CDATA[对percolator的理解与tidb实现 论文 论文地址 https://research.google/pubs/pub36726/ 概述 设计percolator的目的 为大数据集群进行增量处理更新的系统，主要用于google网页搜索索引服务。使用基于Percolator的增量处理系统代替原有的批处理索引系统后，Google在处理同样数据量的文档时，将文档的平均搜索延时降低了50%。 基于bigtable单行事务实现跨行事务 Percolator 在 Bigtable 之上实现的，以client library 的方式实现。 Percolator 利用 Bigtable 的单行事务能力，依靠client的协议和一个全局的授时服务器 TSO 以及两阶段提交协议来实现了跨机器的多行事务。 MVCC与snapshot isolation Percolator依据全局时间戳和MVCC实现 Snapshot Isolation 隔离级别的并发控制协议。 percolator特点 - 事务: 跨行、跨表的、基于快照隔离的ACID事务 - 观察者(observers)：一种类似触发器的通知机制 设计 隔离等级 通过MVCC来实现SI(Snapshop isolation)隔离等级。 Percolator 使用Bigtable的时间戳记维度实现了数据的多版本化。优点如下： - 读操作：可以读取任何指定时间戳版本的记录 - 写操作，能很好的应对写写冲突：若两个事务操作同一记录，只有一个会提交成功 I存在write skew(写偏斜) 锁机制 Because it is built as a client library accessing Bigtable, rather than controlling access to storage itself, Percolator faces a different set of challenges implementing distributed transactions than traditional PDBMSs. Other parallel databases integrate locking into the system component that manages access to the disk: since each node already mediates access to data on the disk it can grant locks on requests and deny accesses that violate locking requirements. Percolator锁的管理必须满足以下条件： - 能应对机器故障：若一个锁在两阶段提交时消失，系统可能将两个有冲突的事务都提交 - 高吞吐量：上千台机器会同时请求获取锁 - 低延时 锁服务要实现： - 多副本 ： survive failure - distributed and balanced ： handle load - 写入持久化存储系统 时间戳 Timestamp Oracle(不是Oracle数据库)： TSO通过统一中心授权可以保证按照递增的方式分配逻辑时钟,任何事件申请的时钟都不会重复，能够保证事务版本号的单调递增，确保分布式事务的时序。 所以TSO是一个分配严格的单调递增时间戳的服务器。 优化 因为每个事务都需要调用oracle两次，所以这个服务必须有很好的可伸缩性。 Oracle会定期分配一个范围的时间戳，然后将范围中的最大值写入持久化，Oracle在内存中原子递增来快速分配时间戳，查询时不涉及磁盘I/O。如果oracle重启，将以存储中的最大值作为开始值。 worker会维持一个长连接RPC到oracle，低频率的、批量的获取时间戳。 性能 Oracle中单台机器每秒向外分配接近两百万的时间戳。 关于批量获取时间戳 批量获取时间戳并不会造成乱序问题，因为就算事务A先获取时间戳T1，事务B后获取时间戳T2，T1&lt;T2，那么分布式系统中，也无法保证事务A先执行，事务B后执行。 如果事务B先执行，那么事务A势必能发现写冲突从而rollback。 单点 为了保证单调递增的特性，所以很多TSO的开源实现都存在单点问题。如tidb的TSO。 而且，一般TSO也存在跨数据中心高延迟的问题。 其他时序方案 Logic Clock: dynamoDB True Time : spanner Hybrid Logic Clock : cockroachDB，没有单点问题，但是为了解决时钟误差而无法避免的时延问题。 数据存储 percolator定义了5个列 Column Use c:lock An uncommitted transaction is writing this cell; contains the location of primary lock c:write Committed data present; stores the Bigtable timestamp of the data c:data Stores the data itself c:notify Hint: observers may need to run c:ack O Observer “O” has run ; stores start timestamp of successful last run Lock 事务的锁，key value映射 1(key,start_ts) ==&gt; (primary_key,lock_type) key：数据的key start_ts：事务开始时间 primary：该锁的primary的引用。事务从待修改的keys中选择一个作为primary,其余的则作为secondary，secondary的primary_key指向primary的key，事务的上锁和解锁都由primary key决定。 Write 已提交的数据对应的时间戳。key value映射 1(key,commit_ts) ==&gt; (start_ts) key：数据的key commit_ts：事务的提交时间 start_ts：事务的开始时间（此数据在data中的时间戳版本） Data 存储数据的列，key value映射 1(key,start_ts) ==&gt; (value) key：对应的主键 start_ts：事务的开始时间 value：除主键外的数据列 Notify notify列仅仅是一个hint值（可能是个bool值），表示是否需要触发通知。 Ack ack列是一个简单的时间戳值，表示最近执行通知的观察者的开始时间。 案例 以银行转账为案例 Bob 向 Joe 转账7元。 事务开始时间：start timestamp =7 ，提交时间：commit timestamp=8。 percolator-demo1.png Bob有10元：查询column write获取最新时间戳版本的数据(data@5),然后从column data里面获取时间戳为5的数据($10），Joe($2) percolator-demo2.png stat timestamp=7 作为当前事务的开始时间戳，将Bob选为此事务的primary key，再写入column:lock对Bob上锁，同时将column:data列更新为7:$3。 percolator-demo3.png start timestamp=7作为锁定Joe账户的时间戳，更新其column:data为$9，其锁是secondary指向primary percolator-demo4.png 当前时间戳commit timestamp=8作为事务提交时间戳：删除primary所在的lock，在write列中写入commit_ts：data@7 percolator-demo5.png 在所有secondary中写入column:write且清理column:lock，事务完成。 流程 伪代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106class Transaction &#123; struct Write&#123; Row row; Column: col; string value;&#125;; vector&lt;Write&gt; writes_; int start_ts_; Transaction():start_ts_(oracle.GetTimestamp()) &#123;&#125; // 往事务中添加一行数据 void Set(Write w) &#123;writes_.push_back(w);&#125; bool Get(Row row, Column c, string* value) &#123; while(true) &#123; // 开启bigtable事务 bigtable::Txn = bigtable::StartRowTransaction(row); // Check for locks that signal concurrent writes. // 检查[0, start_ts]内是否有锁，有则等待再重试 if (T.Read(row, c+&quot;locks&quot;, [0, start_ts_])) &#123; // There is a pending lock; try to clean it and wait BackoffAndMaybeCleanupLock(row, c); continue; &#125; &#125; // Find the latest write below our start_timestamp. // 读取column:wriet的[0, start_ts]的最新提交记录版本 latest_write = T.Read(row, c+&quot;write&quot;, [0, start_ts_]); if(!latest_write.found()) return false; // no data // 根据此时间戳版本去column:data读取数据 int data_ts = latest_write.start_timestamp(); *value = T.Read(row, c+&quot;data&quot;, [data_ts, data_ts]); return true; &#125; // prewrite : 尝试上锁+写数据，如果锁冲突则返回失败 bool Prewrite(Write w, Write primary) &#123; Column c = w.col; // bigtable 事务 bigtable::Txn T = bigtable::StartRowTransaction(w.row); // 检查写写冲突：从column:write获取最新数据，如果commit_ts&gt;=事务开始时间，则冲突 if (T.Read(w.row, c+&quot;write&quot;, [start_ts_, max])) return false; // 检查column:lock，如果存在已存在，则锁冲突 if (T.Read(w.row, c+&quot;lock&quot;, [0, max])) return false; // 往column:lock写入锁，若为secondary，则指向primary T.Write(w.row, c+&quot;lock&quot;, start_ts_, &#123;primary.row, primary.col&#125;) // 往column:data写入(key, start_ts = value) T.Write(w.row, c+&quot;data&quot;, start_ts_, w.value); // 提交bigtable事务 return T.Commit(); &#125; // 此commit整合 2PC的prewrite和commit， // 对外caller而言不需要关注是2PC提交，直接commit就好 bool Commit() &#123; // ***** Prewrite *****// // 2PC 第一阶段：prewrite // 第一行为primary Write primary = write_[0]; // 其余为secondary vector&lt;Write&gt; secondaries(write_.begin() + 1, write_.end()); if (!Prewrite(primary, primary)) return false; for (Write w : secondaries) if (!Prewrite(w, primary)) return false; // ***** Commit *****// // 2PC第二阶段：commit // 从oracle获取commit timestamp int commit_ts = oracle.GetTimestamp(); // Commit primary first. Write p = primary; // primary : 先开始bigtable的事务 bigtable::Txn T = bigtable::StartRowTransaction(p.row); // primary : 如果primiary的锁不存在，则abort if (!T.Read(p.row, p.col+&quot;lock&quot;, [start_ts_, start_ts_])) return false; // aborted while working // primary : 在column:write 列写入开始时间（Pointer to data written at start_ts_） T.Write(p.row, p.col+&quot;write&quot;, commit_ts, start_ts_); // primary : 移除column:lock 列 T.Erase(p.row, p.col+&quot;lock&quot;, commit_ts); // primary : 提交bigtable 事务 if(!T.Commit()) return false; // Second phase: write our write records for secondary cells. // 遍历所有secondary，写入column:write数据同时删除column:lock for (Write w:secondaries) &#123; bigtable::write(w.row, w.col+&quot;write&quot;, commit_ts, start_ts_); bigtable::Erase(w.row, w.col+&quot;lock&quot;, commit_ts); &#125; return true; &#125;&#125;; 事务 - 第一阶段 - 获取时间戳T1, - 写入column:data和锁：时间戳都为T1 - 第二阶段 - 获取时间戳T2 - 写入column:write：key:commit_ts:start_ts (key:T2:T1) - 删除锁 读取 - 获取时间戳Tx - 从column:write 读取key[0, Tx]的最大时间戳数据(获取到事务写入的commit_ts=T2) - 从T2中提取出start_ts为T1 - 从column:data中读取 key:T1的数据 所以读取到的数据是commit时间戳的数据。 清理锁 若客户端在Commit一个事务时，出现了异常，Prepare时产生的锁会被留下。为避免将新事务挂住，Percolator必须清理这些锁。 Percolator用lazy方式来处理未处理的锁：当事务在执行时，发现其他事务造成的锁未处理掉，事务将决定其他事务是否失败，以及清理其他事务的那些锁。 当客户端在执行两阶段提交的commit阶段crash时，事务会留下一个提交点commit point(至少已经写入一条write记录)，但可能会留下一些lock未被处理掉 - 如果priarmy lock 已被write所替代：意味着该事务已被提交，事务需要roll forword，也就是对所有涉及到的、未完成提交的数据，用write记录替代标准的锁standed lock。 - 如果primary lock存在：事务将roll back(因为总是最先提交primary，所以primary未被提交时，可以安全地执行回滚) 这些都是基于bigtable的事务中的。 清理操作在primary锁上是同步的，所以清理alive客户端持有的锁是安全的；然而回滚会强迫事务取消，这会严重影响性能。所以，一个事务将不会清理一个锁除非它猜测这个锁属于一个僵死的worker。 Percolator使用简单的机制来确定另一个事务的活跃度。运行中的worker会写一个token到Chubby锁服务来指示他们属于本系统，token会被其他worker视为一个代表活跃度的信号（退出时token会被自动删除）。有些worker是活跃的，但不在运行中，为了处理这种情况，我们附加的写入一个wall time到锁中；一个锁的wall time如果太老，即使token有效也会被清理。有些操作运行很长时间才会提交，针对这种情况，在整个提交过程中worker会周期的更新wall time。 通知 用户对感兴趣的列编写观察者function注册到percolator，当列发生改变时，percolator通知percolator的worker运行用户function。 通知与写操作不是原子的 通知类似于数据库中的触发器，然而不同的是，通知在其他事务(worker)中执行，所以写操作与观察者执行不是原子的，且观察者的执行会有时效性问题。 这和传统关系型数据库的ACID的C有一定的差别，我猜: 所以这里不叫trigger而是通知的原因 通知与观察者无限循环 编写观察者时，用户要自己考虑通知与观察者进入无限循环的情况(通知-&gt;观察者-&gt;通知-&gt;观察者.....)。 通知的"丢失" 一个列的多次更改只会触发一次通知，所以通知和操作系统的中断一样会存在“丢失”的问题。 实现通知机制 为了实现通知机制，Percolator需要高效找到被观察的脏cell。 Percolator在Bigtable维护一个“notify”列(notify列为一个独立的Bigtable locality group)，表示此cell是否为脏。当事务修改被观察的cell时，则设置cell的notify。worker对notify列执行一个分布式扫描来找到脏cell。找到notify则触发观察者并且等到观察者事务提交成功后，会删除对应的notify cell。 tidb的实现 percolator定义了5个列：data, write, lock, ack, notify。 tidb定义了其中3个：data, write, lock。所以tidb没有实现notify功能(tidb不需要增量处理能力)。 tidb定义了3个rocksdb的column family： - CF_DEFAULT：对应percolator的data列 - CF_LOCK：对应percolator的lock列 - CF_WRITE：对应percolator的write列 CF_DEFAULT 1(key, start_ts) ==&gt; value CF_LOCK 1key ==&gt; lock_info 同一时刻一个key最多只有一个锁，所以，tidb的锁没有start_ts。 CF_WRITE 1(key, commit_ts) ==&gt; write_info tidb 1 client 向 tidb 发起开启事务 begin 2 tidb 向 pd 获取 tso 作为当前事务的 start_ts 3 client 向 tidb 执行以下请求： 读操作，从 tikv 读取版本 start_ts 对应具体数据. 写操作，写入 memory 中。 4 client 向 tidb 发起 commit 提交事务请求 5 tidb 开始两阶段提交。 6 tidb 按照 region 对需要写的数据进行分组。 7 tidb 开始 prewrite 操作：向所有涉及改动的 region 并发执行 prewrite 请求。若其中某个prewrite 失败，根据错误类型决定处理方式： KeyIsLock：尝试 Resolve Lock 后，若成功，则重试当前 region 的 prewrite[步骤7]。否则，重新获取 tso 作为 start_ts 启动 2pc 提交（步骤5）。 WriteConfict 有其它事务在写当前 key, abort事务 其它错误，向 client 返回失败。 8 commit : tidb 向 pd 获取 tso 作为当前事务的 commit_ts。 9 tidb 开始 commit:tidb 向 primary 所在 region 发起 commit。 若 commit primary 失败，则先执行 rollback keys,然后根据错误判断是否重试: LockNotExist abort事务 其它错误，向 client 返回失败。 10 tidb 向 tikv 异步并发向剩余 region 发起 commit。 11 tidb 向 client 返回事务提交成功信息。 所有涉及重新获取 tso 重启事务的两阶段提交的地方，会先检查当前事务是否可以满足重试条件：只有单条语句组成的事务才可以重新获取tso作为start_ts。 tikv Prewrite 伪代码 - -&gt; 代表rpc调用, 例如tidb-&gt;tikv.Prewrite tidb调用tikv的Prewrite接口 - . 代表进程内调用, 例如memory.Put往内存模型写数据 1234567891011121314151617181920212223242526272829303132333435363738start_ts = tidb-&gt;pd.GetTso() // get start_ts// tidb调用tikv prewrite接口 tidb-&gt;tikv.Prewrite(start_ts, data_list)&#123; // tikv prewrite实现 keyIsLockedArray = [] // prewrite each key with start_ts in memory，中间出现失败，则整个prewrite失败 for key in data_list &#123; // check write conflict: 通过raft获取key的数据 record = raft.Get(WriteColumn, key, start_ts) if record.commit_ts &gt;= start_ts &#123; return error(write conflict, END) &#125; // check lock lock = raft.Get(LockColumn, key) if lock != null &amp;&amp; lock.ts != start_ts // lock已存在且为其他tx的锁 &#123; keyIsLockedArray.append(key) continue &#125; // 往内存中的 lock 列写入 lock(start_ts,key) 为当前key加锁, // 若当前key被选为 primary, 则标记为 primary, // 若为secondary,则标明指向primary的信息。 memory.Put(LockColumn, key, start_ts, (primary|secondary), ttl, short_value) memory.Put(DataColumn, key, start_ts, long_value) &#125; if len(keyIsLockedArray) &gt; 0 &#123; return error(keyIsLockedArray) &#125; // 将此事务在内存模型中写入的数据 持久化到raft中 raft.Commit(memory_data) return ok&#125; Commit 伪代码 1234567891011121314151617181920212223242526272829// tidb调用rikv的Commit接口，进行2PC的Commit阶段tidb-&gt;tikv.Commit(keys, start_ts, commit_ts)&#123; for key in keys // do commit &#123; lock = raft.Get(LockColumn, key) // lock存在且匹配，则提交 if lock != null &amp;&amp; lock.ts == start_ts &#123; memory.Put(WriteColumn, key, commit_ts, start_ts) memory.Del(LockColumn, key, start_ts) &#125; // lock does not exist or tx dismatch else if lock == null || lock.ts != start_ts &#123; record = raft.Get(WriteColumn, key, start_ts, commit_ts) if record != null &amp;&amp; record.write_type == (PUT|DELETE|Lock) &#123; continue; // already commited &#125; else if record == null || record.write_type == RollBack &#123; return error(tx conflict, lock not exist) &#125; &#125; &#125; // commit to raft raft.Save(memory.data) return ok&#125; Rollback 当事务在两阶段提交过程中失败时， tidb 会向当前事务涉及到的所有 tikv 发起回滚操作。 伪代码 123456789101112131415161718192021222324252627282930313233343536373839404142// tidb调用tikv的Rollback接口tidb-&gt;tikv.Rollback(keys)&#123; // Rollback接口实现 // 检查合法性 for key in keys &#123; // 检查当前key的锁 lock=memory.GetLockColumn(start_ts, key) if lock != null and lock.ts = start_ts &#123; // 如果锁还存在且是之前的锁，则删除锁，写入的数据 // 且在WriteColumn写入rollback记录防止后面commit请求的到来 memory.Del(DataColumn, key, start_ts) memory.Put(WriteColumn, key, start_ts, rollback) meomry.Del(LockColumn, key, start_ts) continue &#125; // 检查提交情况 record = raft.Get(WriteColumn, key, start_ts) if record != null &#123; if record.status == (PUT|DELETE) &#123; return error(transaction is already commited.) &#125; else if record.status == RollBack &#123; continue; // already rollbacked &#125; &#125; else &#123; // record is null // 提交纪录不存在，说明当前 key 尚未被 prewrite 过， // 为预防 prewrite 在rollback之后过来(可能网络原因)， // 在这里留下 (key,start_ts,rollback)记录 memory.Put(WriteColumn, key, start_ts, rollback) continue &#125; // persist raft-&gt;Save(memory.data) &#125; return ok&#125; Resolve Lock 若客户端在Commit一个事务时，出现了异常，Prepare 时产生的锁会被留下。为避免将新事务hang住，Percolator必须清理这些锁。 Percolator用lazy方式处理这些锁：当事务A在执行时，发现事务B造成的锁冲突，事务A将决定事务B是否失败，以及清理事务B的那些锁。 tidb 在执行 prewrite, get 过程中，若遇到锁，在锁超时的情况下，会向 tikv 发起清锁操作。 1234567891011121314151617181920// tidb调用tikv的ResolveLock接口tidb-&gt;tikv.ResolveLock(start_ts, commit_ts)&#123; // 找出所有 lock.ts==start_ts 的锁并执行清锁操作 locks = raft.Scan(LockColumn, lock.ts = start_ts) for (lock in locks) &#123; // commit_ts存在，则说明已提交 if (commit_ts != null) &#123; // 对已上锁的key进行提交 memory.Commit(lock.key, commit_ts) &#125; else &#123; memory.Rollback(lock.key) &#125; &#125; raft-&gt;Save(memory.data) return ok&#125; Get 12345678910111213141516171819202122232425262728293031323334// tidb调用tikv的Get接口tidb-&gt;tikv.Get(key, start_ts)&#123; // check lock: 如果锁存在且在此之前加的锁，则返回锁冲突 lock = raft-&gt;Get(LockColumn, key) if (lock != null &amp;&amp; lock.ts &lt;= start_ts) return error(isLocked); version = start_ts - 1; RAFTGET: // get writeColumn： 获取小于start_ts的最新提交记录 data = raft-&gt;Get(WriteColumn, key, version) if (data != null) &#123; if (data.writeType == PUT) &#123; if (data.isShortValue) return data.shortValue; // long value return raft-&gt;Get(DataColumn, key, start_ts) &#125; else if (data.writeType == DELETE) &#123; // 没有出现过该值，或该值最近已被删除，返回tidb空 return ok(None)； &#125; else if (data.writeType == &quot;LOCK | ROLLBACK&quot;) &#123; // version=commit_ts-1, 继续查找下一个最近版本 version=commit_ts-1 goto RAFTGET &#125; &#125; else &#123; return ok(None) &#125;&#125; GC TiDB 的事务的实现采用了MVCC机制，当新写入的数据覆盖旧的数据时，旧的数据不会被替换掉，而是与新写入的数据同时保留，并以时间戳来区分版本。 GC 的任务便是清理不再需要的旧数据。 一个 TiDB 集群中会有一个 TiDB 实例被选举为 GC leader，GC 的运行由 GC leader 来控制。 GC 会被定期触发。每次 GC 时，首先，TiDB 会计算一个称为 safe point 的时间戳，接下来 TiDB 会在保证 safe point 之后的快照全部拥有正确数据的前提下，删除更早的过期数据。 每一轮 GC 分为以下三个步骤： - Resolve Locks：该阶段会对所有 Region 扫描 safe point 之前的锁，并清理这些锁 - Delete Ranges：该阶段快速地删除由于 DROP TABLE/DROP INDEX 等操作产生的整区间的废弃数据 - Do GC：该阶段每个 TiKV 节点将会各自扫描该节点上的数据，并对每一个 key 删除其不再需要的旧版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344// tidb 向 tikv 发起 GC操作，要求清理 safe-point 版本之前的所有无意义版本// tidb调用tikv的GC接口tidb-&gt;tikv.Gc(savePoint)&#123; startKey=null; for &#123; // scan (startKey, maxKey] keys = raft.Scan(WriteColumn, startKey, batchSize) if (batch == null) return ok; // 对每个key进行gc操作 for (key in keys) &#123; bool remove_older = false; // 清理write中commit_ts &lt;= startPoint的每个item // item: 一个key的所有write记录 for (item in key) &#123; if (remove_older is true) &#123; memory.Del(WriteColumn, item); memory.Del(DataColumn, item); &#125; // check if is ordest version to save if (writeType == PUT) &#123; // 保留该提交，清理所有该提交之前的数据 memory.Set(memoryOrder, true); &#125; else if (writeType == DELETE) &#123; // 清理所有safe-point 之前的数据 remove_older = true; memory.Del(WriteColumn, item); &#125; else if (writeType == ROLLBACK | LOCK) &#123; // 清理所有小于 safe-point 的 Rollback 和 Lock memory.Del(WriteColumn, item); &#125; &#125; &#125; startKey = keys[keys.length() - 1]; &#125;&#125; 优化 Parallel Prewrite tikv分批的并发进行prewrite，不会像percolator要先prewrite primary，再去prewrite secondary。 如果事务冲突，导致rollback，在tikv的rollback实现中，其会留下rollback记录，这样就会导致事务的prewrite失败，而不会产生副作用。 Short Value 对于percolator，先读取column:write 列，提取到key的start_ts，再去column:data列读取key数据本身。 这样会造成两次读取，tidb的优化是，如果数据本身很小，那么就直接存储在colulmn:write中，只需读取一次即可。 Point Read Without Timestamp 为了减少一次RPC调用和减轻TSO压力，对于单点读，并不需要获取timestamp。 因为单点读不存在跨行一致性问题(读取多行数据时，必须是同一个版本的数据)，所以直接可以读取最新的数据即可。 Calculated Commit Timestamp 如果不通过TSO获取commit_ts，则会减少一次RPC交互从而降低事务的时延。 然而，为了实现SI的RR特性(repeatable read)，所以commit_ts需要确保其他事务多次读取的值是一样的。那么commit_ts就和其他事务的读取有相关性。 下面公式可以计算出一个commit_ts 1max&#123;start_ts, max_read_ts_of_written_keys&#125; &lt; commit_ts &lt;= now 由于不可能记录每个key的最大的读取时间，但是可以记录每个region的最大读取时间，所以公式转换为： 1commit_ts = max&#123;start_ts, region_1_max_read_ts, region_2_max_read_ts, ...&#125; + 1 region_x_max_read_ts : 事务涉及到的key的region。 Single Region 1PC 对于事务只涉及到一个Region，那么其实是没有必要走2PC流程的。直接提交事务即可。]]></content>
      <categories>
        <category>2019</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
        <tag>percolator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对领域驱动设计的理解与实践]]></title>
    <url>%2F2018%2F08%2F19%2Fddd%2F</url>
    <content type="text"><![CDATA[对领域驱动设计的理解与实践 更新于：2020-06-14 什么是DDD 领域驱动设计（Domain-Driven-Design）是一种针对大型复杂系统的领域建模与分析方法论。 2003 年，Eric Evans 发布《Domain-Driven Design: Tackling Complexity in the Heart of Software》（领域驱动设计：软件核心复杂性应对之道），其中定义了DDD。 DDD改变了传统软件开发针对数据库进行的建模方法；DDD先对业务领域进行分析，建立领域模型，根据领域模型驱动代码设计。合理运用面向对象的高内聚低耦合设计要素，降低整个系统的业务复杂性，并使得系统具有更好的扩展性，更好的应对多变的业务需求。 领域 Domain 一个领域就是一个问题域，只要是同一个领域，那问题域就相同。 只要确定了系统所属的领域，那么这个系统的核心业务，即要解决的问题以及问题的边界就基本确定了。 举例 陌生人社交领域，包含有聊天，用户推荐，朋友圈等核心环节。 只要是这个领域，一般都会有这些相同的核心业务，因为他们要解决问题的本质是一样的，就是交友。 每一个领域，都有一个对应的领域模型，领域模型能够很好的帮我们解决复杂的业务问题。 驱动设计 在DDD中，以领域(domain)为边界，分析领域的核心问题，再设计对应的领域模型，最后通过领域模型驱动代码设计的实现。这样设计的系统才有合理的分层与解耦，对以后业务的迭代开发，代码的维护才更加容易。 然而很多互联网公司，为了追求快速的上线，都是模型都没有想清楚就开始写代码，这就导致了后续代码维护困难，无法扩展。修改bug同时又引入新的bug，反反复复，进入恶性循环。 当然，这跟梳理清楚领域模型需要一定时间，这与初创型的互联网公司需求快速上线有点相悖，但是，这点时间的投入是非常值得的。因为可以避免了系统上线后不久又得重构的问题。 概念总结 领域就是问题域 模型驱动的思想：通过建立领域模型来解决领域中的核心问题 领域建模的目标：针对我们在领域中核心问题，而不是整个领域中的所有问题 领域模型设计：设计时应考虑一定的抽象性、通用性，以及复用价值 代码实现：通过领域模型驱动代码的实现，确保代码让领域模型落地，代码最终能解决问题 为什么需要DDD 系统复杂性 耦合 随着产品不断的迭代，业务逻辑变得越来越复杂，系统也越来越庞大。模块彼此互相关联、耦合。导致增加或修改一个功能变得异常艰难，同时功能间的界限也变得模糊，职责不再清晰。这个时候就需要进行重构，拆分。 虽然架构本身是随着业务进行不断演进的；但是，如果架构初始设计不体现出业务的模型，那么新需求就无法体现在现有架构上，导致不断腐化，不断重构。 内聚 贫血模型 Anemic Domain Object domain object仅用作数据载体，而没有行为和动作的领域对象。 指领域对象里只有get和set方法，没有相关领域对象的业务逻辑。业务逻辑放在业务层。 充血模型 Rich Domain Object 将业务逻辑和对象存储放在domain object里面，业务层只是简单进行小部分业务的封装及其他domain的编排。 面向对象设计，符合单一职责设计。 贫血 vs 充血 贫血模型的domain object很轻量，这导致业务层的复杂，domain object相关的业务逻辑散布在各个业务层，造成业务逻辑的冗余以及原本domain object的定义就变得相对模糊，这就是贫血症引起的失忆症。 而采用领域开发的方式，将数据和行为封装在一起，与业务对象相映射；领域对象职责清晰，将相关业务聚合到领域对象内部。 微服务 DDD 的本质是一种软件设计方法论，而微服务架构是具体的实现方式。微服务架构并没有定义对复杂系统进行分解的具体方法论，而 DDD 正好就是解决方案。 微服务架构强调从业务维度来分治系统的复杂度，而DDD也是同样的着重业务视角。 DDD能带来什么 建立通用语言： 围绕领域模型建立的一种语言，团队所有成员都使用这种语言进行沟通和活动 驱动代码设计：领域建立模型，模型指导设计，设计产出代码 解决核心问题：模型的设计中心就是核心域，就是解决核心的问题 DDD建模 战略设计 战略设计就是从宏观角度对领域进行建模。划分出业务的边界，组织架构，系统架构。 DDD中，对系统的划分是基于领域的，也是基于业务的。 通用语言Ubiquitous Language 通用语言是指确定统一的领域术语，提高开发人员与领域专家之间的沟通效率。 一旦确定了统一语言，无论是与领域专家的讨论，还是最终的实现代码，都可以通过使用相同的术语，清晰准确地定义领域知识。 当确认整个团队统一的语言后，就可以开始进行领域建模。 领域和子域 领域Domain 一个领域本质上可以理解为就是一个问题域。只要我们确定了系统所属的领域，那这个系统的核心业务，即要解决的关键问题、问题的范围边界就基本确定了。 举例 - 社交领域：关键问题是用户推荐，聊天 - 电商领域：关键问题是购物，订单，物流 子域Subdomain 如果一个领域过于复杂，涉及到的领域概念、业务规则、交互流程太多，导致没办法直接针对这个大的领域进行领域建模。这时就需要将领域进行拆分，本质上就是把大问题拆分为小问题，把一个大的领域划分为了多个小的领域（子域），那最关键的就是要理清每个子域的边界 子域可以根据自身重要性和功能属性划分为三类子域： - 核心域：公司核心产品和业务的领域 - 支撑子域：不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域 - 通用子域：被多个子域使用的通用功能子域 每个领域的划分都不一样。对相同领域公司而言，其核心，支撑，通用的子域也可能有不一样的地方，但大体上基本都是一样的。 举例 社交领域的划分 - 核心域：用户推荐，聊天 - 支撑子域：客服，反垃圾 - 通用子域：消息推送 限界上下文Bounded Context 限界上下文 限界指划分边界，上下文对应一个聚合，限界上下文可以理解为业务的边界。 一个子域对应一个或多个限界上下文。如果对应多个上下文，则可以考虑子域是否要再进行细粒度的拆分。 限界上下文的目的是为了更加明确领域模型的职责和范围 划分限界上下文 三个原则： - 概念相同，含义不同(通用语言)：如果一个模型在一个上下文里面有歧义，那有歧义的地方就是边界所在，应该把它们拆到不同的限界上下文中。 - 外部系统：有时候系统需要同外部系统交互，这时可以把与外部系统交互的那部分拆分出去以实现更好的扩展性。这样一旦外部系统发生了变化，就不会影响到我们的核心业务逻辑。 - 组织扩展：尽量不要两个团队一起在一个限界上下文里面开发，因为这样可能会存在沟通不顺畅、集成困难等问题。 组织架构 康威定律 任何组织在设计一套系统时，所交付的设计方案在结构上都与该组织的沟通结构保持一致。 团队结构就是组织结构，限界上下文就是系统的业务结构。所以，团队结构应该尽量和限界上下文保持一致。 举例 社交领域中，订单子域对应订单上下文 上下文映射 从宏观上看每个上下文之间的关系，可以更好理解各个上下文之间的依赖关系。 梳理清楚上下文之间的关系是为了： - 任务更好拆分，一个开发人员可以全身心的投入到相关的一个单独的上下文中 - 沟通更加顺畅，一个上下文可以明确自己对其他上下文的依赖关系，从而使得团队内开发直接更好的对接 - 每个团队在它的上下文中能够更加明确自己领域内的概念，因为上下文是领域的解系统 举例 聊天上下文依赖消息推送，推广上下文也依赖消息推送 战术建模 战术建模是从微观角度对上下文进行建模。 梳理清楚聚合根，实体，值对象，领域服务，领域事件，资源库等。 实体Entity 当一个对象可以由标识进行区分时，这种对象称为实体 和数据库中的实体是不同的，这里的实体是从业务角度进行划分的。 实体： - 具有唯一标识 - 持久化 - 可变 举例 社交中的用户即为实体，可以通过用户唯一的id进行区分。 值对象value object 当一个对象用于对事物进行描述而没有唯一标识时，它被称作值对象。 在实践中，需要保证值对象创建后就不能被修改，即不允许外部再修改其属性。 例如：年龄，聊天表情符号（ 😛: 吐舌 (U+1F61B)） 习惯了使用数据库的数据建模后，很容易将所有对象看作实体 聚合根Aggregate Root 聚合是一组相关对象的集合，作为一个整体被外界访问，聚合根是这个聚合的根节点。 聚合由根实体，值对象和实体组成。(聚合根里面有多少个实体，由领域建模决定) 外部对象需要访问聚合内的实体时，只能通过聚合根进行访问，而不能直接访问 举例 一个订单是一个聚合根，订单购买的商品是实体，收货地址是值对象。 领域服务Domain Service 领域服务 一些既不是实体，也不是值对象的范畴的领域行为或操作，可以放到领域服务中。用来处理业务逻辑，协调领域对象来完成相关业务。 例如，有些业务逻辑不适合放到领域对象中，或实体之间的业务协调，这些业务逻辑都可以放到领域服务中。 特征 - 与领域相关的操作如执行一个业务操作过程，但它又并不适合放入实体与值对象中 - 操作是无状态的 - 对领域对象进行转换，或以多个领域对象作为输入进行计算，结果产生一个值对象 当采用微服务架构风格，一切领域逻辑的对外暴露均需要通过领域服务来进行。 如原本由聚合根暴露的业务逻辑也需要依托于领域服务。 举例 必须通过订单领域服务来创建和访问订单 领域事件 领域事件是对领域内发生的活动进行的建模。捕获一些有价值的领域活动事件。 作用 - 解耦：可以通过发布订阅模式，发布领域事件 - 一致性：通过领域事件来达到最终一致性 - 事件溯源 举例 发送聊天消息，这属于一个领域事件；撤回消息，也属于一个领域事件。 推送服务订阅消息事件，然后将消息推送给用户端。这样就解耦了消息服务与推送服务之间的强依赖关系。 资源库Repository 资源库用于保存和获取聚合对象。 领域模型 vs 数据模型 资源库介于领域模型(业务模型)和数据模型(数据库)之间，主要用于聚合对象的持久化和检索。 资源库隔离了领域模型和数据模型，以便上层只需要关注于领域模型而不需要考虑如何进行持久化。 分层架构 把一系列相同的对象进行分类放在同一层，然后根据他们之间的依赖关系再确定上下层次关系。 在实际决策时，我们需要知道各层的职责、意义以及相应的场景； 落实到代码层面时，我们还需要知道各层所包含的具体内容、各层的一些常见的具体策略/模式、层次之间的交互/依赖关系。 DDD经典分层架构 ddd_layers.png 用户接口层（interfaces）：处理显示和用户请求，以及一些基本的参数检查，不包括业务逻辑 应用层（application）：主要协调领域对象的操作；处理持久化事务、发送消息、安全认证等 领域层（domain）：处理核心业务逻辑，不包括技术实现细节。领域层是业务软件的核心 基础设施层（infrastructure）：处理纯技术细节，为其他层提供技术支撑，也可用于封装调用的外部系统细节。例如：持久化的实现，消息中间件的实现，工具类，rpc等 个人理解：这种分层，既可以在一个单体应用中，也可以是微服务的形式。DDD分层并不一定要按微服务的服务粒度进行分层。 如果一个业务逻辑非常简单的子域，则可以将几层都放进一个单体应用中，在应用中进行分层。如果业务较为复杂，则可以按服务进行拆分，每层都有自己对应的服务。 其他架构 对称性架构 洋葱架构 整洁架构 CQRS架构 DDD工程实践 以一个简化的社交领域的例子来实践DDD。 核心概念 用户（User）： 一个账户，并以用户id识别 关系（Relationship）：用户之间的关系 动态（Feed）： 用户发布文字，图片，视频，评论等内容 会话（Conversation）：用户之间的聊天会话 领域设计 战略建模 领域就是社交领域，核心问题和绝大部分社交系统一样。 子域 - 核心域：聊天，动态 - 支撑子域：反作弊，推广 - 通用子域：用户，关系，消息推送 上下文 消息上下文 会话上下文 动态上下文 推送上下文 用户上下文 战术建模 以会话上下文为例子来进行战术建模 会话上下文 会话：聚合根 用户：实体 用户：实体 消息列表：实体 发送人：实体 接收人：实体 消息内容：值对象 消息在会话上下文属于实体，在消息上下文属于聚合根。 结构 以会话子域为例 架构分层 interfaces 接口层 RESTful RPC application 应用层 Conversation Message domain_service 领域服务层 model Conversation Message repository Conversation Message infrastructure 基础设施层 store ConversationRepository MessageRepository message SendMessage utils 领域 1234567891011121314151617package domain// 聚合根type Conversation struct &#123; ID int User1 User User2 User Messages list.List&#125;// 实体type Message struct &#123; ID int From User // 实体 To User Body Content // 值对象&#125; 用户接口层 1234567891011121314151617181920212223242526type ChatInferface struct &#123; // 应用层 app app.ChatApplication&#125;func (c *ChatInferface) Route() &#123; c.route(&quot;POST&quot;, &quot;/api/message&quot;, c.SendMessage) c.route(&quot;PATCH&quot;, &quot;/api/message&quot;, c.RecallMessage)&#125;// POST /api/messagefunc (c *ChatInferface) SendMessage(ctx *Context) &#123; if !c.validateRequest(ctx) &#123; return &#125; message := c.parseMessage(ctx) app.SendMessage(message)&#125;func (c *ChatInferface) RecallMessage(ctx *Context) &#123; if !c.validateRequest(ctx) &#123; return &#125; messageID := c.parseMessage(ctx) app.RecallMessage(messageID)&#125; 应用层 1234567891011121314type ChatApplication struct &#123; user service.UserService chat service.ChatService // 这里领域事件由应用层发布 // publisher EventPublisher lbs LBSFacade&#125;func (c *ChatApplication) SendMessage(msg *Message) &#123; if !c.user.CheckUser(msg.UserID) &#123; return &#125; c.chat.SendMessage(msg)&#125; 领域服务层 12345678910111213141516type ChatService struct &#123; // 领域事件 publisher MessageEventPublisher repo MessageRepository&#125;func (c *ChatService SendMessage(msg *Message) &#123; // 业务逻辑 ... // 领域资源持久化 c.repo.Save(msg) // 发布领域事件 c.publisher.Publish(msg)&#125; 基础设施层 123456789101112131415161718package infrastructuretype MessageRepository struct &#123; db MessageDatabase cache MessageCache&#125;func (m *MessageRepository) Save(msg *Message) &#123; db.Save(m.ToPO(msg))&#125;func (m MessageRepository) Get(msgID int) *Message &#123; msg := m.cache.Get(msgID) if msg != nil &#123; return m.FromPO(msg) &#125; return m.FromPO(m.db.Get(msgID))&#125; 总结 在设计和实现一个系统的时候，这个系统所要处理问题的领域专家和开发人员以一套统一语言进行协作，共同完成该领域模型的构建，在这个过程中，业务架构和系统架构等问题都得到了解决，之后将领域模型中关于系统架构的主体映射为实现代码，完成系统的实现落地。]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>DDD</tag>
        <tag>领域驱动设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grpc]]></title>
    <url>%2F2018%2F06%2F02%2Fgrpc%2F</url>
    <content type="text"><![CDATA[grpc 特性、原理、实践、生态 gRPC 概述 gRPC是一个由google设计开发基于HTTP/2协议和Protobuf序列化协议的的高性能、多语言、通用的开源 RPC 框架。 跨语言、跨平台 插件化 ： 负载均衡，tracing，健康检查，认证等等 编码压缩 ： 节省带宽 多路复用 ： 降低的 TCP 链接次数 使用场景 低延迟、高扩展的分布式系统 与云服务通信 设计一个需要准确，高效且与语言无关的新协议 分层设计，以实现扩展，例如：身份验证，负载平衡，日志记录和监控等 特性 基于HTTP/2 HTTP/2 提供了 链接多路复用、双向流、服务器推送、请求优先级、首部压缩等机制。 gRPC 协议使用了HTTP2 现有的语义，请求和响应的数据使用HTTP Body 发送，其他的控制信息则用Header 表示。 IDL使用ProtoBuffer gRPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议（类似于XML、JSON）。 ProtoBuf能够将数据进行序列化，并广泛应用在数据存储、通信协议等方面。 压缩和传输效率高，向后兼容，语法简单，表达力强。 多语言支持 gRPC支持多种语言，并能够基于语言自动生成客户端和服务端。 目前支持： C#, C++, Dart, Go, Java, Node, Objective-C, PHP, Python, Ruby 等。 详见官网 HTTP/2 HTTP/2 HTTP/1.x 是超文本传输协议第1版，可读性好，但效率不高。 而HTTP/2 是超文本传输协议第2版，是一个二进制协议。 HTTP/1 和 HTTP/2 的基本语义并没有改变，如方法语义（GET/PUST/PUT/DELETE），状态码（200/404/500等），Range Request，Cacheing，Authentication、URL路径。 HTTP/2通用术语： - Stream： 流，一个双向流，一条连接可以有多个 streams。 - Message： 逻辑上面的 request，response。 - Frame：帧，HTTP/2 数据传输的最小单位。每个 Frame 都属于一个特定的 stream。一个 message 可能由多个 frame 组成。 HTTP/2 流、帧 HTTP/2连接上传输的每个帧(frame)都关联到一个流，一个连接上可以同时有多个流， 同一个流的帧按序传输，不同流的帧交错混合传输， 客户端、服务端双方都可以建立流，流也可以被任意一方关闭。 客户端发起的流使用奇数流ID，服务端发起的使用偶数。 Frame结构 : 123456789+-----------------------------------------------+| Length (24) |+---------------+---------------+---------------+| Type (8) | Flags (8) |+-+-------------+---------------+-------------------------------+|R| Stream Identifier (31) |+=+=============================================================+| Frame Payload (0...) ...+---------------------------------------------------------------+ Length ： 也就是 Frame 的长度 Type ：Frame 的类型，有 DATA，HEADERS，SETTINGS 等 Flags ：帧标志位，8个比特表示可以容纳8个不同的标志：stream是否结束(END_STREAM)，header是否结束(END_HEADERS)，priority等等 R：保留位 Stream Identifier：标识frame所属的 stream，如果为 0，则表示这个 frame 属于整条连接(如SETTINGS帧) Frame Payload：帧内容 帧类型 - HEADERS 类似于HTTP/1的 Headers - DATA 类似于HTTP/1的 Body - CONTINUATION 头部太大，分多个帧传输（一个HEADERS+若干CONTINUATION） - SETTINGS 连接设置 - WINDOW_UPDATE 流量控制 - PUSH_PROMISE 服务端推送 - PRIORITY 流优先级更改 - PING 心跳或计算RTT - RST_STREAM 马上中止一个流 - GOAWAY 关闭连接并且发送错误信息 HTTP/2 特性 新的二进制格式（Binary Format） HTTP/1 的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同。 基于这种考虑HTTP/2的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（MultiPlexing） HTTP/1 的request是阻塞的，如果想并发发送多个request，必须使用多个 TCP connection。这样会消耗更多资源，且浏览器为了控制资源，会对单个域名有TCP connection请求限制。 HTTP/2 一个TCP connection可以有多个streams(最大数量由参数SETTINGS_MAX_CONCURRENT_STREAMS控制)， 多个streams 并行发送不同的请求的frames。 可以在SETTINGS帧中设置SETTINGS_MAX_CONCURRENT_STREAMS。 而此值是针对一端而言的，客户端可以告知服务器最大的streams并发数，服务端也可以告知客户端。 如果一条链接上 ID 分配完了， server 则会给 client 发送一个 GOAWAY frame 强制让 client 新建一条连接。 header压缩 HTTP/1 是使用文本协议，而且header每次都要重复发送，浪费了带宽也导致资源加载过慢。 HTTP/2 采取了压缩和缓存来避免重复发送和带宽问题： - 对消息头采用HPACK 进行压缩传输来节省消息头占用的网络的流量。 - 对这些headers采取了压缩策略来减少重复headers的请求数 - HTTP/2在客户端和服务器端使用 headlist 来存储之前发送过的 header，对于相同的header，不再通过每次请求和响应发送； HPACK: Header Compression for HTTP/2 服务端推送 server push功能 : 在无需客户端请求资源的情况下，服务端会直接推送客户端可能需要的资源到客户端。 当服务器想用Server Push推送资源时，会先向客户端发送PUSH_PROMISE帧。 推送的响应必须与客户端的某个请求相关联，因此服务器会在客户端请求的流上发送PUSH_PROMISE帧。 优先级排序 设置优先级的目的是为了告诉对端在并发的多个流之间如何分配资源的行为，同时当发送容量有限时，可以使用优先级来选择用于发送帧的流。 客户端可以通过 HEADERS 帧的 PRIORITY 信息指定一个新建立流的优先级，也可以发送 PRIORITY 帧调整流优先级。 参考官网 Flow Control HTTP/2 支持流控，receiver 端可以对某些stream进行流控也可以针对整个connection流控。 而TCP层只能针对整个connection进行流控。 特性 ： - Flow control 是由方向的 : Receiver 可以选择给 stream 或者整个连接设置接收端的 window size。 - Flow control 是基于信任的 : Receiver 只是会给 sender 建议 连接和 stream 的 flow control window size。 - Flow control 无法禁止 - Flow control 是基于WINDOW_UPDATE帧的 - Flow control 是 hop-by-hop的，而不是 end-to-end 的。例如，用nginx做proxy，则flow control作用于nginx到server和client到nginx这两个connection。 Connection 和 stream 的初始 flow-control window 大小都是 65535。 Connection 的初始窗口大小不能改变，但 stream 的可以(所有stream)，通过发送 SETTINGS 帧，携带 SETTINGS_INITIAL_WINDOW_SIZE，这个值即为新的 stream flow-control window 初始大小。 增加flow control window size能加快数据传输，但同时会消耗更多资源。 主动重置链接 HTTP/1 的body的length的被送给客户端后，服务端就无法中断请求了，只能断开整个TCP connection，但这样导致的代价就是需要重新通过三次握手建立一个新的TCP连接。 HTTP/2 引入了一个 RST_STREAM frame 来让客户端在已有的连接中发送重置请求，从而中断或者放弃响应。当浏览器进行页面跳转或者用户取消下载时，它可以防止建立新连接，避免浪费所有带宽。 HTTP/2 站点demo HTTP/1 和 HTTP/2 加载速度比较： https://http2.akamai.com/demo 访问http2站点 ： https://http2.golang.org/ ProtoBuf ProtoBuf Google Protocol Buffer 是一种轻便高效的结构化数据存储格式，可以用于结构化数据序列化。适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。 描述简单，对开发人员友好 跨平台、跨语言，不依赖于具体运行平台和编程语言 高效自动化解析和生成 压缩比例高 可扩展、兼容性好 gRPC与protobuf gRPC使用 protobuf 作为IDL来定义数据结构和服务。 可以定义数据结构，也可以定义rpc 接口。 然后用proto编译器生成对应语言的框架代码。 定义数据结构 ： 生成对象的 序列化 代码 定义rpc接口 ： 生成 gRPC服务端、客户端响应的代码 protobuf 基本数据类型 https://developers.google.com/protocol-buffers/docs/proto#scalar 数据结构定义 user.proto 1234567891011121314151617181920212223242526272829syntax = &quot;proto3&quot;;import &quot;google/protobuf/any.proto&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; google.protobuf.Any details = 11;&#125; package package声明符，用来防止不同的消息类型有命名冲突。 生成的代码将会包含再package(go等语言)或者命名空间(c++, java等)中。 option go_package = "protos_golang/user"; $LANGUAGE_package 是指定生成的代码的import path和package。 import 要导入其他.proto文件的定义，在文件中添加一个导入声明。 使用导入proto的类型 package名字.结构名 来使用导入proto的类型。 如上面common.Flag 分配字段编号 每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的。 为了保证向后兼容，一旦开始使用就不要再改变。 文件版本申明 syntax = "proto2"; 指定使用proto2语法 syntax = "proto3"; 指定为proto3语法 标识符修饰符 required 和 optional 是proto2的语法，proto3已经不支持。 proto3中所有的字段都是optional的。具体原因见 required : 必须字段。 optional ：可选字段。 repeated ：数组类型字段。 reserved ：保留字段。指出这些字段编号已经删除，不要再重用这些编号了。因为如果这些编号被重新定义成其他类型，那么对于旧版本的protobuf数据，会导致解码错误。 枚举 与数据结构中 enum 类似。字段编号从0开始。 对于protobuf兼容性问题，第一个枚举值应该考虑用unknown这种字段：因为如果在.proto枚举中增加了一个值，而protobuf解析的一方如果没有升级新版本的.proto，则无法解析出此枚举值，而直接使用第一个枚举值，为了避免这种情况，所以最好一般将第一个枚举值设置为unknown。 oneof oneof与数据结构联合体(UNION)类似，一次最多只有一个字段有效。 map map 类型则可以用来表示键值对。 key_type 可以是任何 int 或者 string 类型，float、double 和 bytes除外 any Any类型包括: - bytes : 被序列化为bytes类型的任意消息 - URL : 全局标识符 使用import google/protobuf/any.proto来导入any类型 any可以用来替换proto2中的extension 嵌套类型 可以在消息类型中定义其他消息类型 服务定义 12345678910111213141516syntax = &quot;proto2&quot;;import &quot;user.proto&quot;;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; user.User user = 1;&#125; 如果在 .proto 文件中定义了 RPC 服务接口， 编译器将使用生成服务接口代码和 stubs。 import "user.proto"; 导入user结构定义的proto文件。 插件 protoc编译器通过插件机制实现对不同语言的支持。 protoc会先查找是否有内置的语言插件，如果没有，则会去查找系统中是否存在protoc-gen-$LANGUAGE 的插件。 例如： 如果指定--go_out参数，那么protoc会查询是否有内置的go插件，如果没有则继续查询系统中是否存在protoc-gen-go的可执行程序，再通过插件来生成相关的语言代码。 插件运行流程： - protoc 启动 protoc-gen-xx - 将CodeGeneratorRequest的protobuf二进制传入到 protoc-gen-xx的标准输入 - protoc-gen-xx读取标准输入再反序列化成CodeGeneratorRequest - 遍历CodeGeneratorRequest的FileDescriptorProto数组，其描述了proto文件的语法树 - 将FileDescriptorProto编译成语言源码 - 生成CodeGeneratorResponse对象输出到标准输出 - protoc根据protoc-gen-xx的标准输出再生成源码文件 plugin.proto定义了CodeGeneratorRequest 和 CodeGeneratorResponse，是protoc与插件交互的对象。 descriptor.proto描述的是一个.proto文件的语法树 插件的plugins 插件本身的也是支持以内部plugins形式进行扩展的。 例如：生成go grpc的命令中： 1protoc --go_out=plugins=grpc:. pb/user.proto grpc就是 proto-gen-go的plugin。 代码 Name()返回grpc命名就是plugin的名字，就是上面plugins=grpc 1234// Name returns the name of this plugin, &quot;grpc&quot;.func (g *grpc) Name() string &#123; return &quot;grpc&quot;&#125; gRPC 原理 概念 image gRPC 定义服务，服务包含远程调用的方法。 在服务器端，服务器实现rpc接口并运行一个gRPC服务器来处理客户端请求。 在客户端，客户端有一个"存根stub"，提供与服务器相同签名的方法，来处理客户端请求的编码、解码等，再将请求转发到服务器端，这样客户端调用rpc方法就像调用本地函数一样。 实现 gRPC把HTTP2的steam identifier当作请求ID，每一次请求都发起一个新的stream。 请求的方法、响应的状态码等都放在HEADER frame中。 而请求内容和响应内容由protobuf序列化后使用DATA frame中。 请求 Request主要由 Request-Headers 和 Data 以及 EOS (END_STREAM)组成。 如下图： grpc_request_stream_decoded_wireshark.png Request-Headers Request-Headers 由 HEADERS 和 CONTINUATION frames 组成。 如果Flags有设置标志位END_HEADERS则代表Request-Headers结束。 Request-Headers 主要有 Call-Definition 以及 Custom-Metadata : - Call-Definition : 包括 Method, Scheme, Path, TE, Authority, Timeout, Content-Type ,Message-Type, Message-Encoding, Message-Accept-Encoding, User-Agent - Custom-Metadata : 应用层自定义的任意 key-value，key 不要使用gRPC保留的key前缀字符 grpc- 。 Data 请求体，由一个或多个 Data frame组成。 如果Flags有设置标志位END_STREAM则代表Data结束，请求结束。 request格式大致如下 1234567891011121314151617# request-headers HEADERS (flags = END_HEADERS):method = POST:scheme = http:path = /user.UserService/GetUserInfo:authority = localhost:50000grpc-timeout = 999127ucontent-type = grpc-go/1.20.0-dev## 自定义metadataservice : test_clienttraceid : xxxx# dataDATA (flags = END_STREAM)&lt;Length-Prefixed Message&gt; 响应 Response 主要由 Response-Headers 和 Data 以及 Trailers 组成。 如果遇到了错误，也可以直接返回 Trailers-Only。 如下图： grpc_response_stream_decoded_wireshark.png Response-Headers Response-Headers 包含 : HTTP-Status, Message-Encoding, Message-Accept-Encoding, Content-Type, Custom-Metadata等。 Data 响应体，由一个或多个 Data frame组成。 如果Flags有设置标志位END_STREAM则代表Data结束。 Trailers Trailers-Only 包含 HTTP-Status, Content-Type, Trailers等。 Trailers 包含 Status, Status-Message, Custom-Metadata等。 Trailers作用主要是给响应包含一些额外的动态生成的信息。 如：消息body发送后，再发送一些信息 如数字签名，后处理状态等 格式大致如下： 123456789101112131415161718192021# response-headersHEADERS (flags = END_HEADERS):status = status: 200 content-type = application/grpc## 自定义metadataservice: server_testspanid: xxxx# dataDATA&lt;Length-Prefixed Message&gt;# headersHEADERS (flags = END_STREAM, END_HEADERS)grpc-status: 0## trailers 自定义metadatatimestamp: 1560656283730441829 Status code HTTP状态码对应的gRPC状态码 gRPC通信方式 gRPC有四种通信方式: 1、 unary RPC 一般的rpc调用，客户端发送一个请求对象，然后等待服务端返回一个响应对象 123# 获取用户信息# protorpc GetUserInfo (UserRequest) returns (UserResponse) &#123;&#125; 2、 Server-side streaming RPC 服务端流式rpc 客户端发起一个请求到服务端，服务端返回一段连续的数据流响应。 123# 获取一个用户的所有地理位置历史记录# protorpc UserLocationsStream(UserRequest) returns (stream LocationsResponse) &#123;&#125; 3、 Client-side streaming RPC 客户端流式rpc 客户端将一段连续的数据流发送到服务端，服务端返回一个响应。 123# 客户端将所有数据备份到服务端# protorpc BackupStream(stream BackupRequest) returns (BackupResponse) &#123;&#125; 4、 Bidirectional streaming RPC 双向流式rpc 客户端将连续的数据流发送到服务端，服务端返回交互的数据流。 123# 在线聊天# protorpc LiveChat(stream Message) returns (stream Message) &#123;&#125; 配置 waitForReady 发送请求时，如果connection没有ready，则会一直等待connection ready 或直到超时(达到deadline)。 也常称为fail fast。 timeout 请求超时时间。 如果超时，则会中止请求且返回DEADLINE_EXCEEDED 错误。 maxRequestMessageBytes 请求体的最大payload size(没有压缩的)。 如果客户端请求大于此值的请求会返回RESOURCE_EXHAUSTED错误。 maxResponseMessageBytes 响应体的最大payload size(没有压缩的)。 如果服务端响应大于此值，响应将发送失败。且客户端会得到RESOURCE_EXHAUSTED错误。 gRPC 实践 实践部分以go语言进行demo 环境 安装protoc mac 1brew install protobuf linux 123456PROTOC_ZIP=protoc-3.5.1-linux-x86_64.zipcurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/$PROTOC_ZIPsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protocsudo unzip -o $PROTOC_ZIP -d /usr/local include/*rm -f $PROTOC_ZIP golang的protobuffers插件 1go get -u github.com/golang/protobuf/&#123;protoc-gen-go,proto&#125; Coding 定义proto文件 123456789101112131415161718192021222324252627282930313233343536373839404142syntax = &quot;proto3&quot;;import &quot;google/protobuf/any.proto&quot;;//package user;option go_package = &quot;protos_golang/user&quot;;message User &#123; int32 id = 1; string name = 2; uint32 age = 3; enum Flag &#123; NORMAL = 0; VIP = 1; SVIP = 2; &#125; repeated int32 friends_ids = 5; reserved 6, 7, 8; message Command &#123; int32 id = 1; oneof cmd_value &#123; string name = 2; int32 age = 3; &#125; &#125; Command cmd = 9; map&lt;int32, string&gt; tags = 10; google.protobuf.Any details = 11;&#125;service UserService &#123;// rpc interface rpc GetUserInfo(UserRequest) returns (UserResponse) &#123;&#125;&#125;message UserRequest &#123; uint32 id = 1;&#125;message UserResponse &#123; User user = 1;&#125; 生成代码 生成代码的导入路径和包名 123## protos_golang ： 生成代码的路径## user : golang package 名option go_package = &quot;protos_golang/user&quot;; 目标代码 如果包含rpc接口：则需要指定插件plugins=grpc --go_out=. ： 生成的代码在当前目录; 也可以指定其他目录，如:--go_out=/tmp 代码路径 ： 如果.pb中指定了go_package : 代码路径是 ./$go_package/user.pb.go 如果.pb中没有指定go_package : 则代码路径是 ./pb/user.pb.go 1234protoc --go_out=plugins=grpc:. pb/user.proto# 如果没有rpc定义protoc --go_out=. pb/user.proto 服务端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mainimport ( &quot;context&quot; &quot;log&quot; &quot;net&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( port = &quot;:50000&quot;)// grpc servertype server struct&#123;&#125;// 实现gRPC接口func (s *server) GetUserInfo(ctx context.Context, in *pb.UserRequest) (*pb.UserResponse, error) &#123; return &amp;pb.UserResponse&#123; User: &amp;pb.User&#123; Name: &quot;test_user&quot;, &#125;, &#125;, nil&#125;// 拦截器，简单打印下日志func LogUnaryInterceptorMiddleware() grpc.UnaryServerInterceptor &#123; return func(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (r interface&#123;&#125;, err error) &#123; r, err = handler(ctx, req) fmt.Printf(&quot;fullMethod(%s), errCode(%v)\n&quot;, info.FullMethod, err) return r, err &#125;&#125;func main() &#123; lis, err := net.Listen(&quot;tcp&quot;, port) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; // 拦截器 options := grpc.UnaryInterceptor(LogUnaryInterceptorMiddleware()) s := grpc.NewServer(options) // 注册服务器实现 pb.RegisterUserServiceServer(s, &amp;server&#123;&#125;) // 注册服务端反射 reflection.Register(s) // 启动服务器 if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 客户端 123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;context&quot; &quot;log&quot; &quot;time&quot; &quot;google.golang.org/grpc&quot; pb &quot;testgrpc/protos_golang/user&quot;)const ( address = &quot;localhost:50000&quot;)func main() &#123; conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := pb.NewUserServiceClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.GetUserInfo(ctx, &amp;pb.UserRequest&#123;&#125;) if err != nil &#123; log.Fatalf(&quot;fatal: %v&quot;, err) &#125; log.Printf(&quot;response: %s&quot;, r)&#125; 调试 为了方便调试服务端，所以服务端需要支持reflection功能。 1reflection.Register(grpcServer) 两款比较著名的调试工具： - [grpc_cli](https://github.com/grpc/grpc/blob/master/doc/command_line_tool.md : 官方的 - grpcurl : go的，安装简单 列出服务端注册的service 如果没有配置好公钥和私钥文件，也没有忽略证书的验证过程，则需要加-plaintext 123$ grpcurl -plaintext localhost:50000 list grpc.reflection.v1alpha.ServerReflectionuser.UserService 列出服务的接口 12$ grpcurl -plaintext localhost:50000 list user.UserServiceuser.UserService.GetUserInfo 获取接口的签名 123$ grpcurl -plaintext localhost:50000 describe user.UserService.GetUserInfouser.UserService.GetUserInfo is a method:rpc GetUserInfo ( .user.UserRequest ) returns ( .user.UserResponse ); 获取类型信息 12345$ grpcurl -plaintext localhost:50000 describe .user.UserRequestuser.UserRequest is a message:message UserRequest &#123; uint32 id = 1;&#125; 调试接口 请求体以json的形式描述类型。 123456$ grpcurl -plaintext -d &apos;&#123;&quot;id&quot;:1&#125;&apos; localhost:50000 user.UserService.GetUserInfo&#123; &quot;user&quot;: &#123; &quot;name&quot;: &quot;test_user&quot; &#125;&#125; go gRPC 生态 服务组件 上下文信息传递 rpc客户端将上下文信息传递给服务端。 链路调用信息，服务信息，认证信息等等。 官方实现 服务器反射 服务端反射协议， 可以用途于: - 服务端调试 : grpcurl 工具就是用reflection协议来进行服务端调试的。可以list出服务端的接口定义，以及命令行构造请求进行调试。 - 运行时构造gRPC请求 ：客户端可以运行时根据反射的接口定义构造请求。 官方实现 负载均衡 客户端负载均衡器 官方实现 认证 gRPC主要的两种认证方式： - 基于SSL/TLS认证方式 - Token认证方式 两种方式可以同时应用 官方实现 实现了几种认证方式： - alts - google - oauth - 自定义认证方式 go-grpc-middleware的实现 健康检查 服务端提供一个Check接口返回其状态信息。 客户端调用此接口获取到服务健康状态，是否可以继续提供服务。 官方实现 keepalive 定期发送HTTP/2.0 pings帧来检测 connection 是否存活，如果断开则进行重新连接。 与健康检查区别在于keepalive是检查connection而健康检查是检查服务是否可用。 官方实现 naming 命名解析。 通过服务命名来获取服务相关的信息来达到服务发现目的。 与balancer结合使用来实现进程内负载均衡与服务发现。 官方实现 限流 限制流量来保护服务端以防止服务过载。 可以在客户端，balancer，服务端 进行限流。 go-grpc-middleware实现服务端限流 recovery 将服务内部的错误转换成gRPC错误码。 go-grpc-middleware实现 ： recover go的panic， 并转换成gRPC错误。 重试 客户端对于返回某些gRPC错误码的请求进行重试。 go-grpc-middleware tracing 在链路上下文携带tracing信息，以及将信息以opentracing的规范发送给分布式链路分析服务。 tracing信息包含traceid,spanid,请求时间,错误信息,日志等等。 如：通过设置客户端spanid为服务端spanid的parent_spanid，这样就能知道是客户端调用了服务端rpc请求。 go-grpc-middleware实现opentracing的middleware open-tracing 微服务框架、组件 go-kit : 微服务组件 micro : 微服务框架 go-chassis : 华为开发的go微服务框架 go-grpc-middleware : 服务端和客户端的一些中间件，认证、日志、分布式追踪跟重试等 grpc-gateway ：一个 protoc 的插件，可以将 gRPC 接口转换为对外暴露 RESTful API 的工具，同时还能生成 swagger 文档 gRPC 与 负载均衡 进程内LB(Balancing-aware Client) 需要实现： - 服务注册 - 健康检查 - 服务发现 - 负载均衡 缺点： - 开发成本：要实现上述功能 - 维护成本：不同语言栈的sdk维护与升级 官方已经提供接口来实现进程内的负载均衡。同时结合服务发现，健康检查一起使用。 集中式LB(Proxy Model) proxy 实现服务发现，健康检查，负载均衡等等。 还方便做限流等控制和其他统一控制策略。 缺点： - 单点问题 - 多一层性能开销 - 不方便调试 Nginx Nginx(1.13.10已经支持gRPC) 1234567891011121314151617181920212223upstream grpcservers &#123; server localhost:50000; server localhost:50001;&#125;server &#123; listen 9000 http2; # router location /user.UserService &#123; grpc_pass grpc://grpcservers; error_page 502 = /error502grpc; &#125; # 将默认错误页面更改成gRPC状态码 location = /error502grpc &#123; internal; default_type application/grpc; add_header grpc-status 14; add_header grpc-message &quot;unavailable&quot;; return 204; &#125;&#125; nginx gRPC module 独立LB进程(External Load Balancing Service) 在主机上部署独立的LB进程，来实现服务发现，健康检查，负载均衡等功能。 不用对于不同语言维护不同sdk版本； 常常用于微服务service mesh。 缺点： - 单点问题：但是只影响本机 - 不方便调试 常用的组件： - Istio - Envoy gRPC 生态环境 组件 grpc 只是实现了 RPC 核心功能，缺少很多微服务的特性（服务注册发现、监控、治理、管理等），而基于 HTTP/2 相对来说比较容易进行扩展。 grpc-ecosystem 上有一些比较优秀的外围组件来完善gRPC的生态体系 awesome-grpc 收集了一些优秀的gRPC项目 grpc 文档与交流 文档 官网文档 : https://grpc.io/docs/ github 上 grpc 仓库下的 doc ： https://github.com/grpc/grpc/tree/master/doc 博客 : https://grpc.io/blog/ 交流 https://grpc.io/community/ 交流的方式有： - 邮件列表 - Gitter - Reddit - Meetup Group 参考 grpc.io developers.google.com gRPC github doc http2 specs or github http2 spec]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用之限流]]></title>
    <url>%2F2018%2F04%2F18%2Frate_limit%2F</url>
    <content type="text"><![CDATA[限流的概念，算法，分布式限流以及微服务架构下限流的难点 限流概念 目的 通过对并发/请求进行限速来保护系统，防止系统过载。 做到有损服务，而不是不服务。 负载过高时，优先保护核心服务或业务 限流方式 限流的方式有很多： - QPS：限制每秒的请求数 - 并发数：避免开启过多线程导致资源耗尽 - 连接数：限制TCP连接数 以下指的限流都是指QPS的限制，不涉及并发数和连接数。 限流规则 限流策略是根据业务特征来设置的，可以设置静态策略，也可以动态变化的。 规则包含有： - 限流算法 - 参数，限流阈值 - 相应策略等(处理方式等) 静态规则 限流阈值 服务端提前做好整个服务端链路的性能压测，先了解业务特征，需要请求API的QPS，估算好整个服务集群的水位及下游水位(下游服务和相关存储等中间件)。服务端集群的限流阈值建立在压测的数据基础上。 压测需要得到的相关指标 - 服务：对于计算型服务，需要考虑服务本身计算带来的系统资源消耗 - 缓存及数据库：需要访问存储的服务，需要考虑数据库的负载 - 消息队列：虽然一般消息队列可以用来作为削峰用途，但是有的消息队列是作为其他业务事务等用途的，也需要考虑其负载情况 - 其他：下游链路中涉及到的各种服务或者中间件，都应该考虑到其负载 需要考虑： - 不同API的不同阈值 - 整个链路涉及到的服务阈值：如果某个请求链路需要请求10个服务，则阈值计算需要考虑这10个服务能够提供给此服务的流量配额 - 不同的限流策略有不同的限流阈值 - 理论上每次API实现的业务逻辑的改变，以及增加、减少API都要进行重新压测，相对比较麻烦 - 阈值的估算应该考虑整个服务的安全水位，阈值定义过高则容易产生过载，过低则浪费资源 动态规则 动态规则的两种实现： - 自适应：服务根据RT，负载，QPS等等指标动态变更限流规则 - 外部通知：通过调用服务API来通知服务更新或服务自己轮询规则服务器 限流阈值 根据响应时间或在监控系统以及服务治理中心之上根据服务及下游的负载来动态计算阈值 服务端限流 vs 客户端限流 vs 网关限流 绝大多数情况下限流都是发生在服务端的，因为很多情况下客户端的数量是不确定的。但有时候为了防止单个客户端过度使用服务，那么此处可以在客户端来完成，当然在服务端也可以同时进行。 一般都推荐在服务端做限流 服务端限流 服务在处理请求前，应该对请求进行限流计算，防止系统过载。 同时也要考虑到为不同的业务的客户端提供不同的限流策略，不能因为某个业务的问题达到达到限流阈值而造成其他业务无法请求服务。 服务端限流优点 - 更好控制整个服务的负载情况：服务端的限流阈值不会因为客户端数量增加或减少而改变 - 方便对不同上游服务进行不同阈值的限流策略：可以对不同的调用者进行不同的限流配额，也可以给不同业务打上不同的tag再根据tag来限流。 缺点 - 如果服务端只针对QPS限流，而不考虑连接数：服务在建连过程中也会产生一些资源消耗，而这些压力往往可能会成为瓶颈。特别是短连接，不断的建链过程会产生大量的资源消耗 - 如果服务端也针对连接数进行限制：则不好对不同链路或服务进行配额区分。容易造成某个业务或服务的连接过多而导致其他服务也被限制 客户端限流 客户端调用下游服务时，以每个服务集群的限流配额对下游服务进行过载保护。 优点 - 达到阈值不会请求服务端，避免服务端产生额外的资源消耗，如建立连接 缺点 - 客户端的数量的增加或减少需要重新计算每个客户端的限流阈值 - 客户端限流可能出现bug，或者客户端负载均衡产生倾斜导致限流失效 - 服务不同API不同限流阈值：下游服务较多，而每个服务的不同API有不同限流配额，则客户端的限流较为复杂 网关限流 请求通过网关来请求服务端，在网关中对不同服务及不同的API进行限流。 优点 - 能很好的保护整个集群的负载压力，服务端数量增加或减少，则网关进行相应的阈值调整即可 - 对不同的上游业务的服务设置不同的限流配额和不同的限流策略 缺点 - 需要网关资源 - 网关本身高可用性 限流粒度 限流的粒度可以分为： - 服务：对服务所有API进行统一的限流策略 - API：每个API会有不同的请求链路，则相应会有不同的限流策略(阈值等) - API参数：很多时候我们希望能够对某个热点数据中访问频次最高的 Top K 数据进行限制。例如：秒杀，大促等场景，不要因为某个商品的频繁访问引起的限流导致其他商品无法访问。 服务粒度 一个服务提供一个统一的限流的策略。 优点是非常简单，但很容易造成限流失效，无法保护服务本身及下游。 如：服务提供两种API，都是访问数据，两种API的查询语句并不一致，API1 查询非常复杂，数据库安全水位只能提供10/s的TPS，而对于API2，数据库可以提供1000/s的TPS，这种情况下，如果按照服务粒度进行限流，则只能提供10/s QPS的限流阈值。所以是非常不合理的。 API粒度 不同的API进行不同的限流策略，这种方式相对复杂些，但是更为合理，也能很好的保护服务。 要考虑几种情况： - 增加或减少API，则限流策略要做相应的调整 - API实现的改变：请求处理实现变化则可能需要重新对限流阈值进行调整，避免因为增加一些业务逻辑而导致服务本身或者下游服务过载。 大多数情况下，都应该进行API粒度的限流，这样才能更好的保护服务本身及服务的下游服务和中间件，达到更好的限流效果。 限流的处理方式 如果达到了流量限制的阈值，一般处理方式有： - 直接返回错误码：可以返回资源耗尽或者busy等状态码，或者带backoff的重试 - 等待及重试：要注意不要超过请求超时时间 限流算法 固定窗口算法Fixed window 通过维护一个单位时间内的计数值，每当一个请求通过时，就将计数值加1，当计数值超过阈值时，就进入限流处理流程。如果单位时间已经结束，则将计数器清零，开启下一轮的计数。 fixed_window1.png 这种方式的缺点是：窗口是固定的，会存在两个窗口边界突发流量问题。当然，取决于窗口的大小，如果足够小，则这种问题是可以忽略的 如下图： 时间窗口为1s，1s的限制阈值是4，如果一个恶意用户在1s的最后的最后500毫秒发送3个请求，在下一秒的前500毫秒发送3个请求，那么实际在1秒内发送了6个请求，就超过了流量的限制。 fixedwindow2.png 滑动窗口算法Sliding window 将时间窗口在进行细化，分为N个小窗口，窗口以小窗口为最小滑动单位，这样就可以避免在两个时间窗口之间产生毛刺现象。（当然在小窗口之间仍然会产生毛刺） fixedwindow2.png 令牌桶算法Token Bucket 令牌桶算法是网络流量整形（Traffic Shaping）和速率限制（Rate Limiting）中最常使用的一种算法。 有3个阶段 - 令牌的产生：以r/s的速率将token放入bucket中，如果bucket中token数已到达bucket的容量b，则丢弃token - 获取令牌：在请求处理前从bucket中获取(移除)一个令牌，只有获取到令牌才进行处理 - 无法获取令牌：当bucket中令牌不够，请求进入限流处理流程（阻塞等待或者直接返回失败，等待应该计算好超时时间） 【图来源于dev.to】 token bucket特点 - 最常见的单机限流算法，开源组件很多，使用也简单 - token bucket可以应对短时间的突发流量。例如：bucket容量为10，速率为2/s，而请求QPS为2/s，那么bucket中有8个tokens可以用来应对突发流量。 漏桶算法Leaky Bucket 图左边a：一个桶，不断的倒水进来，桶底下有个洞，按照固定的速率把水漏走，如果水进来的速度比漏走的快，桶可能就会满了，水就溢出了 图右边b：类似于左边a，将请求放入桶中，以固定的速率从桶中拿出请求进行处理，当桶满了，请求将被阻塞或直接拒绝服务 【图来源于dev.to】 leaky bucket特点 - 不能很好应付突发的流量：例如，桶容量为10，请求处理速率为2/s；请求放入桶的速率为2/s，那么可以容纳8个突发请求放入桶中，其他等待或者丢弃。 - 桶可以是普通队列形式，也可以是优先队列：优先队列是很有必要的 - 放入桶中的请求应该计算好其超时时间，如果请求放入桶中到请求被处理，已经超过请求的超时间，则是没有意义的，反而阻塞了其他请求。 和令牌桶区别是：令牌桶是固定速率往桶里放令牌，请求来了取走令牌，桶空了，请求阻塞；漏桶是请求来了往桶里放请求，以固定速率取走请求，桶满了，请求阻塞。令牌桶允许突发流量，而漏桶是不允许的。 动态限流算法 前面的算法都是以恒定速率进行限流的(令牌桶以固定速率将令牌放入桶中，固定和滑动窗口限制阈值都是固定的)，这种最大的问题就是每次业务逻辑变更，都要重新进行压力测试以计算出最新的阈值。 在TCP拥塞控制算法中，流量发送速率是跟网络环境相关的。那其实服务的限流也可以根据服务处理请求的时延或负载等指标来进行动态调整。 预热期慢启动 类似于TCP的慢启动，定义一个启动时的限制阈值，在定义的预热时间周期内逐步提升限制阈值，直到周期结束达到定义的正常值。 这非常适合于服务本身或其依赖的存储等需要进行预热的场景。 可以参考 guava 例如： 以令牌桶为例，开始时限制阈值为4/s，预热时间为3/s，正常限制阈值为7/s，那么就在3秒内，将限制阈值每秒增加1最终达到7/s的阈值。 warmup.png 根据响应时间 这种方式根据请求响应时间来实时调整限流的规则，相对较为合理。 请求响应快则平滑调整增加阈值，响应慢则减少阈值，以及定义一个最大安全阈值。 关键在于如何量化快与慢： - 根据压测得到服务的安全水位，估算出最大阈值。 - 启动时设置一个保守的阈值 - 与前一个时间窗口内响应时间比较，比之前快则可以调高阈值。 那么随着响应时间的变化，阈值也在不断的变化中，阈值的范围在[1, max_value]之间调整。 基于监控系统 如果服务是消耗CPU资源的计算型或者消耗IO资源的存储型等，则基于监控系统更为合理。 根据CPU，load，内存，IO等系统指标和请求响应时间等业务指标综合考虑，随着监控指标的变化动态改变限流规则，这种限流相对较为复杂，根据自己业务设计适合自己的计算方式。 如果是IO型，则IO指标的权重相对要高一些，如果是计算型，则CPU和Load要权重高一些。 分布式限流 单节点限流最大的问题是当服务节点动态添加或减少后，每个服务的限流配额也要跟随动态改变。 如果服务节点增加了，而原来节点限流配额没有减少，则下游服务就可能过载。 而分布式限流则避免了这种问题，通过像redis集群或发票服务器这种取号的方式来限制某个资源的流量。 redis限流 基于redis的单线程及原子操作特性来实现限流功能，这种方式可以实现简单的分布式限流。但是redis本身也容易成为瓶颈，且redis不管是主从结构还是其cluster模式，都存在主节点故障问题。 方案1：固定窗口计数 将要限制的资源名+时间窗口为精度的时间戳 作为redis 的key，设置略大于时间戳的超时时间，然后用redis的incrby的原子特性来增加计数。 如果限流的时间窗口以秒为单位，则 - redis key : 资源名 + unix timestamp - count : incrby key count - expire 2 - 检查count是否达到阈值 1234567891011local key = KEYS[1]local limit = tonumber(ARGV[1])local acquireCount = tonumber(ARGV[2])local current = tonumber(redis.call(&apos;get&apos;, key) or &quot;0&quot;)if current + acquireCount &gt; limit then return 0else redis.call(&quot;incrby&quot;, key, acquireCount) redis.call(&quot;expire&quot;, key, &quot;2&quot;) return 1end 这种方案存在的问题： - 要和redis进行交互：时延较差 - 热点资源redis容易成为瓶颈 - redis进行主从切换会导致限流失效 - 服务的时钟会有误差：由于lua中有写操作就不能使用带随机性质的读操作所以不能通过redis lua获取 - 属于固定窗口算法，在窗口之间容易产生突发流量问题 方案2：令牌桶 1234567891011121314151617181920212223242526272829303132333435local function acquire(key, acquireTokens, currentTimeMillSecond) local rateLimiterInfo = redis.pcall(&quot;HMGET&quot;, key, &quot;lastTimeMilliSecond&quot;, &quot;availableTokens&quot;, &quot;maxLimit&quot;, &quot;rate&quot;) local lastTimeMilliSecond = rateLimiterInfo[1] local availableTokens = tonumber(rateLimiterInfo[2]) local maxLimit = tonumber(rateLimiterInfo[3]) local rate = rateLimiterInfo[4] local currentTokens = availableTokens; local result = -1 if (type(lastTimeMilliSecond) ~= &apos;boolean&apos; and lastTimeMilliSecond ~= false and lastTimeMilliSecond ~= nil) then local diffTime = currentTimeMillSecond - lastTimeMilliSecond if diffTime &gt; 0 then local fillTokens = math.floor((diffTime / 1000) * rate) local allTokens = fillTokens + availableTokens; currentTokens = math.min(allTokens, maxLimit); end end if (currentTokens - acquireTokens &gt;= 0) then result = 1 redis.pcall(&quot;HMSET&quot;, key, &quot;lastTimeMilliSecond&quot;, currentTimeMillSecond, &quot;availableTokens&quot;, currentTokens - acquireTokens) end return resultendlocal key = KEYS[1]local acquireTokens = ARGV[1]local currentTimeMillSecond = ARGV[2]local ret = acquire(key, acquireTokens, currentTimeMillSecond)return ret 这种方案存在的问题： - 要和redis进行交互：时延较差 - 热点资源redis容易成为瓶颈 - redis进行主从切换会导致限流失效 - 服务的时钟会有误差：由于lua中有写操作就不能使用带随机性质的读操作所以不能通过redis lua获取 发票服务器 上述redis方案，是将redis作为一种发票服务器，但是由于redis这种方案本身存在可用性问题(主从切换等)，控制规则也比较简单，所以对于可用性要求比较高且规则复杂的需求，都选择自己开发服务器程序来作为发票服务器。 如阿里开源的sentinel 发票服务器一般由一些服务进程组成一个或多个发票集群。 而服务通过RPC向发票服务器领票，成功则可以执行，否则则进入限流机制。为了减少RPC通信带来的延迟，一般可以批量获取。 发票规则 发票规则(限流算法)可以存储到一致性存储或者数据库等，发票服务器定期更新或者监听通知来获取规则的变化。 也可以通过其他服务来动态调整算法和阈值，然后通知发票服务器，也可以发票服务器自己根据负载情况来计算。 发票服务器特点： - 发票服务器可用性高：通过集群模式，且可以持久化到数据库。 - 发票服务器负载均衡：服务从发票服务集群领票要注意发票服务器负载均衡，避免造成有的发票服务器发票领完有的却有大量剩余发票 - 发票服务器高性能：因为发票服务器的计算和存储都基于内存，所以性能不容易成为瓶颈 - 发票服务器一致性：类似于ID生成器，对于极高要求的场景，可以定期将发票服务器发票的信息等进行持久化存储，故障时再从中进行恢复 微服务架构下限流的难点 在微服务架构下，调用链路很长，服务的调用关系复杂，上游服务一个小的改动，将影响所有下游服务，还会产生叠加效应。 流控规则 微服务架构下，固定流控规则是不合适的，固定规则往往根据压测结果进行计算得来，而微服务架构下，链路上一个节点的变化都会导致固定规则的失效。 如： 服务某个链路为 A-&gt;B-&gt;C-&gt;D 而B做了一些业务逻辑的变更，那么链路就有可能产生变化，导致之前的链路压测结果不准确，如果还按照之前的阈值，则可能导致流控失效。 微服务架构下，应该采用动态规则，让服务自适应或者规则服务器来通知服务改变限流规则 根据调用链路的限流规则 下游服务进行限流时，也要考虑给予上游服务的流量配额，否则很容易因为由于某个上游服务的故障，导致整个下游链路不可用。 如： 链路1：A-&gt;B-&gt;C-&gt;D 链路2：D-&gt;B-&gt;C A如果故障，导致调用B流量暴涨，那么调用C的流量也会暴涨。这个时候链路2的D服务则会收到B或C的流控影响。 所以B和C应该根据给予链路1和链路2进行不同的配额，链路1达到阈值则对链路1的调用进行限流，不影响链路2的调用。 考虑调用链路优先级 一般微服务场景会根据业务来定义其可用性权重。权重高的业务往往要优先保证其可用性。 那么就存在复杂调用链路上，针对不同业务的请求来进行限流，服务压力过大时，优先保证重要的业务可运行。 如： 链路1：A-&gt;B-&gt;C-&gt;D 链路2：D-&gt;B-&gt;C 链路1的重要性高于链路2，那么如果C的负载升高时，C就要降低整体的限流阈值，那么就要降低链路2的限流阈值，牺牲链路2的可用性来保证链路1。 所以就需要将链路进行tag标识，服务C根据tag来区分链路。 总结 限流算法往往相对简单且开源方案很多。而难点在于如何选择适合自己的限流算法。 不管是单节点线路还是分布式限流，都有各自的优缺点，需要根据自身业务特性、规模以及架构复杂性来选择适合自己的方案。 微服务架构下，限流变得相当复杂，要考虑清楚各种可能导致限流失效的场景。]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>SOA</tag>
        <tag>microservice</tag>
        <tag>rate limit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NUMA笔记]]></title>
    <url>%2F2018%2F04%2F12%2Fnuma%2F</url>
    <content type="text"><![CDATA[NUMA 概念、历史、问题 NUMA 概念 NUMA的几个概念（Node，socket，core，thread） cputhreads.jpg socket就是主板上的CPU插槽; core就是socket里独立的一组程序执行的硬件单元，比如寄存器，计算单元等; thread：就是超线程hyperthread的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。 NUMA体系结构中多了Node的概念，这个概念其实是用来解决core的分组的问题，具体参见下图来理解（图中的OS CPU可以理解thread，那么core就没有在图中画出），从图中可以看出每个Socket里有两个node，共有4个socket，每个socket 2个node，每个node中有8个thread，总共4（Socket）× 2（Node）× 8 （4core × 2 Thread） = 64个thread。 另外每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量，因为Node内有自己内部总线，所以增加CPU数量可以通过增加Node的数目来实现，如果单纯的增加CPU的数量，会对总线造成很大的压力，所以UMA结构不可能支持很多的核。 《NUMA Best Practices for Dell PowerEdge 12th Generation Servers》 根据上面提到的，由于每个node内部有自己的CPU总线和内存，所以如果一个虚拟机的vCPU跨不同的Node的话，就会导致一个node中的CPU去访问另外一个node中的内存的情况，这就导致内存访问延迟的增加。在有些特殊场景下，比如NFV(Network Function Virtualization)环境中，对性能有比较高的要求，就非常需要同一个虚拟机的vCPU尽量被分配到同一个Node中的pCPU上，所以在OpenStack的Kilo版本中增加了基于NUMA感知的虚拟机调度的特性。 查看机器的NUMA拓扑结构 123456789101112131415161718192021222324[root@local ~]$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 48 // 48个逻辑CPU（threads）On-line CPU(s) list: 0-47Thread(s) per core: 2 // 每个core有2个threadsCore(s) per socket: 12 // 每个socket有12个coresSocket(s): 2 // 共总有2个socketsNUMA node(s): 2 // 2个NUMA nodesVendor ID: GenuineIntelCPU family: 6Model: 63Model name: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHzStepping: 2CPU MHz: 2500.089BogoMIPS: 4999.27Virtualization: VT-xL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 30720KNUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47 可以看出当前机器有2个sockets，每个sockets包含1个numa node，每个numa node中有12个cores，每个cores包含2个thread，所以总的threads数量=2x1x12x2=48. NUMA 历史 在若干年前，对于x86架构的计算机，那时的内存控制器还没有整合进CPU，所有内存的访问都需要通过北桥芯片来完成。此时的内存访问如下图所示，被称为UMA（uniform memory access, 一致性内存访问）。 这样的访问对于软件层面来说非常容易实现：总线模型保证了所有的内存访问是一致的，不必考虑由不同内存地址之前的差异。 cpubus123 之后的x86平台经历了一场从“拼频率”到“拼核心数”的转变，越来越多的核心被尽可能地塞进了同一块芯片上，各个核心对于内存带宽的争抢访问成为了瓶颈；此时软件、OS方面对于SMP多核心CPU的支持也愈发成熟；再加上各种商业上的考量，x86平台也搞了NUMA（Non-uniform memory access, 非一致性内存访问）。 NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die(核心)上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）。所以NUMA（Non-Uniform Memory Access）就此得名 在这种架构之下，每个Socket都会有一个独立的内存控制器IMC（integrated memory controllers, 集成内存控制器），分属于不同的socket之内的IMC之间通过QPI link通讯。 cpubus-imc123 然后就是进一步的架构演进，由于每个socket上都会有多个core进行内存访问，这就会在每个core的内部出现一个类似最早SMP架构相似的内存访问总线，这个总线被称为IMC bus。 cpubus-imc124 于是，很明显的，在这种架构之下，两个socket各自管理1/2的内存插槽，如果要访问不属于本socket的内存则必须通过QPI link。也就是说内存的访问出现了本地/远程（local/remote）的概念，内存的延时是会有显著的区别的。 以Xeon 2699 v4系列CPU的标准来看，两个Socket之之间通过各自的一条9.6GT/s的QPI link互访。而每个Socket事实上有2个内存控制器。双通道的缘故，每个控制器又有两个内存通道（channel），每个通道最多支持3根内存条（DIMM）。理论上最大单socket支持76.8GB/s的内存带宽，而两个QPI link，每个QPI link有9.6GT/s的速率（~57.6GB/s）事实上QPI link已经出现瓶颈了。 image 核心数还是源源不断的增加，Skylake桌面版本的i7 EE已经有了18个core，Skylake Xeon 28个Core(2017)。为了塞进更多的core，原本核心之间类似环网的设计变成了复杂的路由。 由于这种架构上的变化，导致内存的访问变得更加复杂。两个IMC也有了local/remote的区别，在保证兼容性的前提和性能导向的纠结中，系统允许用户进行更为灵活的内存访问架构划分。于是就有了“NUMA之上的NUMA”这种妖异的设定（SNC）。 性能提升 内存访问 在一个NUMA node上的进程访问本NUMA node的内存 比 访问其他NUMA node的内存快很多。 引用《nderstanding the NUMA Memory System Performance of Multithreaded Workloads》 进程调度 Linux内核调度以sheduling domain为层级进行调度，调度器尽量平衡一个domain下的节点的负载。 引用《Understanding linux kernel》 a scheduling domain is a set of CPUs whose workloads should be kept balanced by the kernel. 在一个启用了NUMA支持的Linux中，Kernel尽量不会将任务内存从一个NUMA node搬迁到另一个NUMA node。 为了尽可能的优化性能，在正常的调度之中，CPU的core也会尽可能的使用可以local访问的本地core。 一旦当某个NUMA node的负载超出了另一个node一个阈值（默认25%），则认为需要在此node上减少负载，不同的NUMA结构和不同的负载状况，系统会对进程进行迁移。在这种情况下将会产生内存的remote访问。 NUMA node之间有不同的拓扑结构，各个 node 之间的访问会有一个距离（node distances）的概念， 如numactl -H命令的结果有这样的描述： 123456789101112[root@local ~]$ numactl -Havailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73363 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117527 MBnode distances:node 0 1 0: 10 21 1: 21 10 可以看出：0 node 到0 node之间距离为10，是最近的距离。 image 上图记录了某个Benchmark工具，在开启/关闭NUMA功能时QPI带宽消耗的情况。很明显的是，在开启了NUMA支持以后，QPI的带宽消耗有了两个数量级以上的下降，性能也有了显著的提升！ 通常情况下，用户可以通过numactl来进行NUMA访问策略的手工配置，cgroup中cpuset.mems也可以达到指定NUMA node的作用。 Numa内存分配策略有四种: 缺省default:总是在本地节点分配(当前进程运行的节点上)。 绑定bind:强制分配到指定节点上。 交叉interleavel:在所有节点或者指定节点上交叉分配内存。 优先preferred:在指定节点上分配，失败则在其他节点上分配 以numactl命令为例，它有如下策略： –interleave=nodes //允许进程在多个node之间交替访问 –membind=nodes //将内存固定在某个node上，CPU则选择对应的core。 –cpunodebind=nodes //与membind相反，将CPU固定在某（几）个core上，内存则限制在对应的NUMA node之上。 –physcpubind=cpus //与cpunodebind类似，不同的是物理core。 –localalloc //本地配置 –preferred=node //按照推荐配置 对于某些大内存访问的应用，比如Mongodb，将NUMA的访问策略制定为interleave=all则意味着整个进程的内存是均匀分布在所有的node之上，进程可以以最快的方式访问本地内存。 北桥有一个功能就是PCI/PCIe控制器，南桥（PCH）整合了PCIe控制器。 在PCIe channel上也是有NUMA亲和性的。 比如：查看网卡em1的NUMA 1234567[root@local ~]$ numactl --prefer netdev:em1 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 PCI address 为00:1f.2的SATA控制器，用到了pci: 00:1f.2 SATA controller: Intel Corporation C610/X99 series chipset 6-Port SATA Controller [AHCI mode] (rev 05) 1234567[root@local ~]$ numactl --prefer pci:00:1f.2 --showpolicy: preferredpreferred node: 0physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前系统numa策略： 1234567[root@local ~]$ numactl --showpolicy: defaultpreferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 cpubind: 0 1 nodebind: 0 1 membind: 0 1 查看当前numa的节点情况： 123456789101112[root@local ~]$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46node 0 size: 196514 MBnode 0 free: 73338 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47node 1 size: 196608 MBnode 1 free: 117521 MBnode distances:node 0 1 0: 10 21 1: 21 10 NUMA带来的问题 MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture PostgreSQL – PostgreSQL, NUMA and zone reclaim mode on linux Oracle – Non-Uniform Memory Access (NUMA) architecture with Oracle database by examples Java – Optimizing Linux Memory Management for Low-latency / High-throughput Databases 这些问题都是：“因为CPU亲和策略导致的内存分配不平均”及“NUMA Zone Claim内存回收”有关，而和数据库种类并没有直接联系。 数据库与NUMA MySQL在NUMA架构上遇到的典型问题 The MySQL “swap insanity” problem and the effects of the NUMA architecture A brief update on NUMA and MySQL 大致分析如下： CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核CPU通过一条总线共享内存成为瓶颈 于是NUMA出现了，CPU平均划分为若干个Chip（不多于4个），每个Chip有自己的内存控制器及内存插槽 CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存（下文称Remote Access）的速度相较慢三倍左右 于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中 由于内存页没有动态调整策略，使得大部分内存页都集中在CPU 0上 又因为Reclaim默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出 当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现了 解决方案： numactl --interleave=all 在MySQL进程启动前，使用sysctl -q -w - vm.drop_caches=3清空文件缓存所占用的空间 Innodb在启动时，就完成整个Innodb_buffer_pool_size的内存分配 不过这种三合一的解决方案只是减少了NUMA内存分配不均，导致的MySQL SWAP问题出现的可能性。如果当系统上其他进程，或者MySQL本身需要大量内存时，Innodb Buffer Pool的那些Page同样还是会被Swap到存储上。于是又在这基础上出现了另外几个进阶方案 配置vm.zone_reclaim_mode = 0使得内存不足时去remote memory分配优先于swap out local page echo -15 &gt; /proc//oom_adj调低MySQL进程被OOM_killer强制Kill的可能 memlock 对MySQL使用Huge Page（黑魔法，巧用了Huge Page不会被swap的特性） 为什么Interleave的策略就解决了问题？ 借用两张 Carrefour性能测试 的结果图，可以看到几乎所有情况下Interleave模式下的程序性能都要比默认的亲和模式要高，有时甚至能高达30%。究其根本原因是Linux服务器的大多数workload分布都是随机的：即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分布的。而Interleave模式就恰恰是针对这种特性将内存page随机打散到各个CPU Core上，使得每个CPU的负载和Remote Access的出现频率都均匀分布。相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多。 image 也就是说，像MySQL这种外部请求随机性强，各个线程访问内存在地址上平均分布的这种应用，Interleave的内存分配模式相较默认模式可以带来一定程度的性能提升。此外各种论文 中也都通过实验证实，真正造成程序在NUMA系统上性能瓶颈的并不是Remote Acess带来的响应时间损耗，而是内存的不合理分布导致Remote Access将interconnect这个小水管塞满所造成的结果。而Interleave恰好，把这种不合理分布情况下的Remote Access请求平均分布在了各个小水管中。所以这也是Interleave效果奇佳的一个原因。 那是不是简简单单的配置个Interleave就已经把NUMA的特性和性能发挥到了极致呢？ 答案是否定的，目前Linux的内存分配机制在NUMA架构的CPU上还有一定的改进空间。 例如：Dynamic Memory Loaction, Page Replication。 Dynamic Memory Relocation MySQL的线程分为两种，用户线程（SQL执行线程）和内部线程（内部功能，如：flush，io，master等）。对于用户线程来说随机性相当的强，但对于内部线程来说他们的行为以及所要访问的内存区域其实是相对固定且可以预测的。如果能对于这把这部分内存集中到这些内存线程所在的core上的时候，就能减少大量Remote Access，潜在的提升例如Page Flush，Purge等功能的吞吐量，甚至可以提高MySQL Crash后Recovery的速度（由于recovery是单线程）。 那是否能在Interleave模式下，把那些明显应该聚集在一个CPU上的内存集中在一起呢？很可惜，Dynamic Memory Relocation这种技术目前只停留在理论和实验阶段。我们来看下难点：要做到按照线程的行为动态的调整page在memory的分布，就势必需要做线程和内存的实时监控（profile）。对于Memory Access这种非常异常频繁的底层操作来说增加profile入口的性能损耗是极大的。 Page Replication 一些动态加载的库，把他们放在任何一个线程所在的CPU都会导致其他CPU上线程的执行效率下降。而这些共享数据往往读写比非常高，如果能把这些数据的副本在每个Memory Zone内都放置一份，理论上会带来较大的性能提升，同时也减少在interconnect上出现的瓶颈。由于缺乏硬件级别（如MESI协议的硬件支持）和操作系统原生级别的支持，Page Replication在数据一致性上维护的成本显得比他带来的提升更多。因此这种尝试也仅仅停留在理论阶段。当然，如果能得到底层的大力支持，相信这个方案还是有极大的实际价值的。 关闭NUMA特性的方法 硬件层，在BIOS中设置关闭 OS内核，启动时设置numa=off 进程，numactl 进程启动时。numactl --interleave=all NUMA取舍 指定numa 在运行程序的时候使用numactl -m和-physcpubind就能制定将这个程序运行在哪个cpu和哪个memory中: numactl –physcpubind=2,6 ./program 玩转cpu-topology(站点已经无法访问) 的测试中显示当程序只使用一个node资源和使用多个node资源的比较表（差不多是38s与28s的差距）。所以限定程序在numa node中运行是有实际意义的。 指定numa带来的问题 SWAP的罪与罚 文章就说到了一个numa的陷阱的问题。现象是当你的服务器还有内存的时候，发现它已经在开始使用swap了，甚至已经导致机器出现停滞的现象。如果一个进程限制它只能使用自己的numa节点的内存，那么当自身numa node内存使用光之后，就不会去使用其他numa node的内存了，会开始使用swap，甚至更糟的情况，机器没有设置swap的时候，可能会直接死机！ 所以你可以使用numactl --interleave=all来取消numa node的限制。 根据具体业务决定NUMA的使用: 如果你的程序是会占用大规模内存的，你大多应该选择关闭numa node的限制。因为这个时候你的程序很有几率会碰到numa陷阱。 如果你的程序并不占用大内存，而是要求更快的程序运行时间。你大多应该选择限制只访问本numa node的方法来进行处理。 推荐阅读: NUMA-aware scheduler for Go PostgreSQL, NUMA and zone reclaim mode on linux NUMA and Java Databases MySQL Server and NUMA architectures]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[顺序、时钟与分布式系统]]></title>
    <url>%2F2018%2F03%2F15%2Fordering_clock%2F</url>
    <content type="text"><![CDATA[顺序、时钟与分布式系统 Ordering 现实生活中时间可以记录事情发生的时刻、比较事情发生的先后顺序。 分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序。 如数据写入先后顺序，事件发生的先后顺序等等。 关系 复习下离散数学中关系： 假设A是一个集合 {1,2,3,4} ；R是集合A上的关系，例如{&lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt;,&lt;1,2&gt;,&lt;1,4&gt;,&lt;2,4&gt;,&lt;3,4&gt;} - 自反性：任取一个A中的元素x，如果都有&lt;x,x&gt;在R中，那么R是自反的。 - &lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt; - 对称性：任取一个A中的元素x,y，如果&lt;x,y&gt; 在关系R上,那么&lt;y,x&gt; 也在关系R上，那么R是对称的。 - 反对称性：任取一个A中的元素x,y(x!=y)，如果&lt;x,y&gt; 在关系R上,那么&lt;y,x&gt; 不在关系R上，那么R是反对称的。 - 对于 &lt;1,2&gt;，有 &lt;2,1&gt; 不在R中；对于&lt;2,4&gt; 有&lt;4,2&gt;不在R中；对于&lt;3,4&gt; 有&lt;4,3&gt; 不在 R中，满足。 - 传递性：任取一个A中的元素x,y,z，如果&lt;x,y&gt;,&lt;y,z&gt; 在关系R上，那么 &lt;x,z&gt; 也在关系R上，那么R是对称的。 - &lt;1,1&gt;&lt;1,2&gt;在R中，并且&lt;1,2&gt;在R中；&lt;1,1&gt;&lt;1,4&gt;在R中，并且&lt;1,4&gt;在R中；&lt;2,2&gt;&lt;2,4&gt;在R中，并且&lt;2,4&gt;在R中；&lt;3,3&gt;&lt;3,4&gt;在R中，并且&lt;3,4&gt;在R中；等等其他，满足。 - 完全性（全关系）：包含了自反性；对集合A中所有&lt;x,y&gt;，都有关系x到y或y到x； - R中并没有&lt;1, 3&gt;，所以不满足完全性 偏序The Partial Ordering 集合内只有部分元素之间是可以比较的。 偏序关系的定义（R为A上的偏序关系）： 设R是集合A上的一个二元关系，若R满足： - 反对称性：对任意x,y∈A，若xRy，且yRx，则x=y； - 传递性：对任意x, y,z∈A，若xRy，且yRz，则xRz - 自反性：对任意x∈A，有xRx； 一个partitial ordering关系满足的条件是自反的，反对称的和可传递的，因此在partitial ordering中，可能有两个元素之间是不相关的。 全序The Total Ordering 集合内只有部分元素之间是可以比较的。 比如：比如复数集中并不是所有的数都可以比较大小，那么“大小”就是复数集的一个偏序关系。 全序关系的定义： - 反对称性：对任意x,y∈A，若xRy，且yRx，则x=y； - 传递性：对任意x, y,z∈A，若xRy，且yRz，则xRz - 完全性(total relation全关系)：对任意x,y∈A，由xRy或yRx （包括了自反性） 完全性本身也包括了自反性，所以全序关系是偏序关系。 所以偏序中满足完全性就是全序了。 一个total ordering关系满足的条件是反对称的，可传递的和完全性，因此在total ordering中，两个元素一定是有关系的，要么是a&lt;&gt;b或b&lt;&gt;a。 happens before 在分布式系统中，一个进程包含一系列的事件，对于同一进程内的事件，如果a happens before b，那么a发生在b之前。并且，假定收或发消息都是一个事件。 happens before的定义如下(用-&gt;表示) - 如果a和b在同一进程中，并且a发生在b之前，那么a-&gt;b - 如果a是一个进程发消息的事件，b是另一个进程接收这条消息的事件，则a-&gt;b - 如果a-&gt;b且b-&gt;c，那么a-&gt;c。 - 如果同时不满足a-&gt;b，且b-&gt;a，那么说a和b是并发的concurrent [图来自Time, Clocks, and the Ordering of Events in a Distributed System] 以一个例子来说明happens before关系，如上图，垂直线上代表一个进程，从下往上，时间依次增加，水平的距离代表空间的隔离。原点代表一个事件，而曲线代表一条消息。 从图中很容易地看出，如果一个事件a，能通过进程的线和消息线，到达b，那么a-&gt;b。 在图中，p3和q4是并行的事件，因为，只有到了p4才能确定q4的发生，而q3也只能确定p1发生。 clock时钟 物理时钟 晶振和时钟偏移 计算机有固定频率晶体的震荡次数，晶体的振荡周期决定了单机的时钟精度。 时钟频率也可能因为温度等外部因素导致时钟偏移，普通的石英晶体的漂移大约\(10^{-6}\) 原子钟的漂移约为 \(10^{-13}\) 所以原子钟精度远远高于石英晶体。 分布式下带来的问题 不同机器上的物理时钟难以同步，导致无法区分在分布式系统中多个节点的事件时序。即使设置了 NTP 时间同步节点间也存在毫秒级别的偏差，因而分布式系统需要有另外的方法记录事件顺序关系。 1978年Lamport在《Time, Clocks and the Ordering of Events in a Distributed System》中提出了逻辑时钟的概念，来解决分布式系统中区分事件发生的时序问题。 逻辑时钟Logical clocks 逻辑时钟指的是分布式系统中用于区分事件的发生顺序的时间机制。 从某种意义上讲，现实世界中的物理时间其实是逻辑时钟的特例。 Logical Clock解决的问题是找到一种方法，给分布式系统中所有时间定一个序，这个序能够正确地排列出具有因果关系的事件（注意，是不能保证并发事件的真实顺序的），使得分布式系统在逻辑上不会发生因果倒置的错误。因果一致性 Lamport timestamps 论文 Time, Clocks, and the Ordering of Events in a Distributed System Lamport timestamps Leslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)。 分布式系统中按是否存在节点交互可分为三类事件: - 发生在节点内部 - 发送事件 - 接收事件 时钟的定义如下 - 对于一个进程i，Ci(a)表示进程i中事件a的发生时间 - 对于整个系统来讲，对于任意的事件b，其发生时间为C(b)，当b为进程j的事件时，则C(b) = Cj(b) 为了使得事件按照正确的排序，需要使得如果事件a发生在事件b之前，那么a发生的时间要小于b，如下 12for any events a, bif a-&gt;b then C(a) &lt; C(b) 根据关系-&gt;的定义，我们可以得出 - 如果a和b都是进程i中的事件，且a发生在b之前，那么Ci(a) &lt; Ci(b) - 如果事件a发送消息给事件b，a属于进程i，b属于进程j，那么Ci(a) &lt; Cj(b) img1/1/clock/happens_before.png 为了让系统满足上述条件，在实现中，需要满足以下原则 - 对于每个进程，相邻的事件的时钟要增加1 - (a) 如果事件a是进程i发送消息m的事件，发送时带时间戳Tm = Ci(a)，(b)事件b是进程j接受消息m的事件，那么事件b的取值为max(进程b的当前时钟，Tm+1) 假设有事件a、b，C(a)、C(b)分别表示事件a、b对应的Lamport时间戳，如果a-&gt;b,则C(a) &lt; C(b)，a发生在b之前(happened before)。 所以Lamport timestamps原理如下： - 每个事件对应一个Lamport时间戳，初始值为0 - 如果事件在节点内发生，时间戳加1 - 如果事件属于发送事件，时间戳加1并在消息中带上该时间戳 - 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1 通过该定义，事件集中Lamport时间戳不等的事件可进行比较，我们获得事件的偏序关系(partial order)。 img1/1/clock/logical_time.png 上图更形象的解释了事件之间的关系。 以B4事件为基准： - B4左边深灰色的区域的事件，都发生在B4前，和B4具有因果关系，这些事件属于与B4因果关系中的因(cause) - B4右边的深红色区域的事件，都发生在B4后，和B4具有因果关系，这些事件属于与B4因果关系中的果(effect) - B4上下的白色区域是跟B4无关的事件，可以认为是并发关系(concurrent) - 在浅灰色和浅红色区域中的事件，C2、A3两个事件与B4是并行关系，根据Lamport timestamps的定义，将他们判定为与B4具前后关系。（所以Lamport timestamps并不能严格的表示并行关系） Lamport timestamps与偏序关系 Lamport timestamps只保证因果关系（偏序）的正确性，不保证绝对时序的正确性。 Lamport logical clock 由于Lamport timestamps只能得到偏序关系，如果要得到全序关系，就需要给Ci(a) = Cj(b)的事件定一个先后顺序。 total order的事件关系=&gt;定义如下： 如果事件a发生在进程Pi，事件b发生在进程Pj，那么当满足下列两者条件之一时，a=&gt;b - Ci(a) &lt; Cj(b) - Ci(a) = Cj(b) 且 Pi &lt; Pj 根据以上条件，对于任意的两个事件，都能判断出它们之间的关系，因此是total ordering的。 当Lamport timestamp一致时，通过义Pi &lt; Pj来定义顺序，确保分布式场景下各个进程间发生的事件的全序定义。至于Pj &lt; Pj：可采用不同的方式，Lamport Logical Clock提到的 arbitrary total ordering。 vector clock Lamport timestamp得到的是全序关系，但无法严格表示对于没有因果关系、存在同时发生关系(concurrent)的事件。 Vector clock是在Lamport timestamp基础上改进的一种逻辑时钟方法，它构不但记录本节点的Lamport timestamp，同时也记录了其他节点的Lamport timestamp。 原理如下: - 本地vector clock的clock数组中每一个逻辑时间(clock)对应一个进程的clock - 初始化vector clock中每一个逻辑时间为0； - 每一次处理内完内部事件，将vector clock中自己的逻辑时间戳+1； - 每发送一个消息的时候，将vector clock中自己的逻辑时间+1，且将其和消息一起发送出去 - 每接收到一个消息的时候，需要将本地的vector clock中自己的逻辑时间戳+1，且将自己vector clock中的逻辑时间和消息中携带的进行比较，取最大的更新本地vector clock中的逻辑时间。 图来源于wikipedia vector clock判定并发关系： - 事件i、事件j对应的vector clock中，每一个进程Pk的逻辑时间戳都满足Vi[Pk]&lt;Vj[Pk]时，我们称事件i happen before事件j； - vector clock中，存在P1、P2，使得Vi[P1]&lt;Vj[P1]，Vi[P2]&gt;Vj[P2]，我们称事件i和事件j是并发关系(没有因果关系)； 和之前lamport timestamp的一样，以B4事件为基准(vector clock为[A:2,B:4,C:1])，根据vector clock的判定，可以判断出 - 灰色区域的事件happens before B4事件，B4事件happens before红色区域的事件 - 白色区域与B4事件没有因果关系。 特性： - vector clock不需要在节点之间同步时钟，不需要在所有节点上维护一段数据的版本数； - 缺点是时钟值的大小随着节点增多和时间不断增长 version vector 分布式系统多个副本被同时更新时，会导致副本之间数据的不一致。version vector用于来发现这些不一致的冲突。 version vector只能发现冲突，无法解决冲突；当然也可以通过再添加一个维度信息timestamp，发生冲突时进行比较，但是又回到了物理时钟不同步的问题。 下图展示了数据由不同副本处理后导致的不同版本冲突。 D5时发现了数据的冲突，这时会将不同版本数据都存储下来，一般由客户端来解决冲突。 image version vector与vector clock的差异 - vector clocks 使用 receive和send 方法来更新clock,而version vector使用sync方法来更新。 - vector clocks是给事件定序的，确定事件的因果关系；而version vector是确定同一个数据不同版本的因果关系。 分布式与时钟 分布式系统中，每个节点的物理时钟是不同步的，都有一定的差异。 这样就带来了一些分布式系统实现的难题，如基于MVCC实现的事务，基于MVCC实现事务会要求版本之间能判断先后顺序，只有确定先后才知道应该用哪一个版本的数据，确定先后顺序就涉及到时间，而不同机器之间的本地时钟是无法保证一致的，所以这就需要确保时钟的同步。 而通常解决方案有两种： - 中心化的时钟方案，如Timestamp oracle(TSO) - 无中心化的时钟方案，如google True Time，Hybrid Logic Time Timestamp oracle 如果我们整个系统不复杂，而且没有跨全球的需求，这时用一台中心授时服务就可以了。 如TiDB使用的就是TSO方案，tipb作为一个TSO集群，来提供授时服务。 使用TSO的好处在于因为只有一个中心授时，所以我们一定能确定所有时间的时间，但TSO需要关注几个问题： - 网络延时：因为所有的事件都需要从TSO获取时间，所以TSO只适合小集群部署，不能是那种全球级别的数据库 - 性能：每个事件都需要从TSO获取时间，所以TSO需要非常高的性能 - 容错：TSO是一个单点，需要考虑节点的failover True Time 由于节点间NTP是有偏差的，且可能出现时间回退的情况，所以NTP无法准确的判定事件的全序关系。在Google Spanner里面，通过引入True Time来解决了分布式时间问题。 True Time实现 Spanner通过使用GPS + 原子钟atomic clock来对集群的机器时间进行校对，保证了集群机器的时间戳差距不会超过一个上限值(ε)。 用两种技术来处理，是因为导致这两种技术的失败的原因是不同的。 - GPS会有一个天线，电波干扰会导致其失灵。原子钟很稳定。 - 当GPS失灵的时候，原子钟仍然能保证在相当长的时间内，不会出现偏差。 API TT.now() : 返回一个当前时间，其位于范围区间[earliest,latest] TT.after(t) : 当前时间是否在t之后 TT.before(t) : 当前时间是否在t之前 虽然spanner引入了TrueTime可以得到全球范围的时序一致性，但由于TrueTime返回的时间仍然有一定的偏差，如果要给两个事件定序，就需要等待2个偏差的时间间隔，来确保其先后顺序。 - 事件a：[Tai, Taj], Taj-Tai=ε - 事件b：[Tbi, Tbj], Tbj-Tbi=ε - 所以要确定b&gt;a, 那么就要确保Tbi &gt; Taj, 就需要在事件b进行等待，以确保：事件b时间 - 事件a时间 &gt; 2ε Hybrid logical clock HLC Logical Physical Clocks and Consistent Snapshots in Globally Distributed Databases TrueTime 需要硬件的支持，所以有一定的成本，而HLC无需硬件支持也能解决分布式下时间问题。 HLC同时使用了物理时钟和逻辑时钟（physical clock + logical clock），能够保证单点的时间发生器是单调递增的，同时能够尽量控制不同节点之间的时钟偏差在规定的偏差范围内。 判断两个事件的先后顺序：先判断物理时间，再判断逻辑时间。 HLC的算法 l.j维护的是节点j当前已知的最大的物理时间(wall time)，c.j则是当前的逻辑时间。 1234567891011121314151617181920212223242526272829303132333435// 在节点j上面：初始化: l.j = 0，c.j = 0。initially l.j :=0; c.j := 0// 本地事件或者发送消息时，// 如果本地时钟pt大于当前的混合逻辑时钟的l，// 则将l更新成本地时钟，将c清零。// 否则，l保持不变，将c加1。Send or local event&#123; l&apos;.j := l.j; l.j := max(l&apos;.j, pt.j); // 本地物理时间pt if (l.j = l&apos;.j) then c.j := c.j+1 else c.j := 0; Timestamp with l.j, c.j&#125;// 收到消息时// l在 当前的逻辑时钟的l、机器的本地时钟pt、收到消息里面带的l，三者中取最大的。// 如果l部分是更新为本地时钟了，则将c清零。否则，c取较大的那个l对应到的c加1。Receive event of message m&#123; l&apos;.j := l.j; l.j := max(l&apos;.j, l.m, pt.j); if (l.j = l&apos;.j = l.m) then c.j := max(c.j, c.m) + 1 elseif (l.j=l&apos;.j) then c.j := c.j + 1 elseif (l.j=l.m) then c.j := c.m + 1 else c.j := 0 Timestamp with l.j, c.j&#125; 特性 HLC算法保证了HLC时间有如下特性： - 事件e发生在事件f之前，那么事件e的HLC时间一定小于事件f的HLC时间：(l.e, c.e) &lt; (l.f, c.f) - 本地WallTime大于等于本地物理时间(l.e ≥ pt.e)：HLC时间总是不断递增，不会随着物理时间发生回退。 - 对事件e，l.e是事件e能感知的到的最大物理时间值：如果l.e &gt; pt.e，那么一定存在着一个发生在e之前的事件g，有pt.g=l.e。简单来说是如果出现l.e &gt; pt.e肯定是因为有一个HLC时间更大的的节点把当前节点的HLC时间往后推了。 - WallTime和物理时钟的偏差是有界的(ε ≥ |pt.e - l.e| )：因为节点之间通过NTP服务校时，那么节点之间的物理时钟偏差一定小于某个值ε。那么对于任一事件b和e，如果b hb e，那么事件b的物理时间pt.b一定满足pt.e + ε ≥ pt.b。结合特性3存在一个事件g满足，l.e = pt.g。那么 pt.e + ε ≥ l.e=pt.g &gt; pt.e。 开源实现 CockroachDB采用基于NTP时钟同步的HLC去中心化方案。 时钟同步 所有节点间的RPC消息都会把时间戳带入到消息中，接收到消息的节点会通过消息中的时间戳更新自己的时间, 从而达到节点间时间同步的效果。 代码分析 参考：https://github.com/cockroachdb/cockroach/blob/v1.1.3/pkg/util/hlc/hlc.go HLC定义 1234567891011// Timestamp represents a state of the hybrid logical clock.type Timestamp struct &#123; // Holds a wall time, typically a unix epoch time // expressed in nanoseconds. WallTime int64 `protobuf:&quot;varint,1,opt,name=wall_time,json=wallTime&quot; json:&quot;wall_time&quot;` // The logical component captures causality for events whose wall // times are equal. It is effectively bounded by (maximum clock // skew)/(minimal ns between events) and nearly impossible to // overflow. Logical int32 `protobuf:&quot;varint,2,opt,name=logical&quot; json:&quot;logical&quot;`&#125; WallTime：本地已知物理时钟 Logical：逻辑时钟 Timestamp：HLC，单调递增 获取物理时钟 12345678910111213141516171819202122232425262728293031323334// PhysicalNow returns the local wall time. It corresponds to the physicalClock// provided at instantiation. For a timestamp value, use Now() instead.func (c *Clock) PhysicalNow() int64 &#123; c.mu.Lock() defer c.mu.Unlock() return c.getPhysicalClockLocked()&#125;// getPhysicalClockLocked returns the current physical clock and checks for// time jumps.func (c *Clock) getPhysicalClockLocked() int64 &#123; // physicalClock 就是 UnixNano newTime := c.physicalClock() if c.mu.lastPhysicalTime != 0 &#123; interval := c.mu.lastPhysicalTime - newTime // 检查时钟是否回退 if interval &gt; int64(c.maxOffset/10) &#123; c.mu.monotonicityErrorsCount++ log.Warningf(context.TODO(), &quot;backward time jump detected (%f seconds)&quot;, float64(-interval)/1e9) &#125; &#125; c.mu.lastPhysicalTime = newTime return newTime&#125;// UnixNano returns the local machine&apos;s physical nanosecond// unix epoch timestamp as a convenience to create a HLC via// c := hlc.NewClock(hlc.UnixNano, ...).func UnixNano() int64 &#123; return timeutil.Now().UnixNano()&#125; 获取当前HLC时钟 123456789101112131415161718// Now returns a timestamp associated with an event from// the local machine that may be sent to other members// of the distributed network. This is the counterpart// of Update, which is passed a timestamp received from// another member of the distributed network.func (c *Clock) Now() Timestamp &#123; c.mu.Lock() defer c.mu.Unlock() if physicalClock := c.getPhysicalClockLocked(); c.mu.timestamp.WallTime &gt;= physicalClock &#123; // The wall time is ahead, so the logical clock ticks. c.mu.timestamp.Logical++ &#125; else &#123; // Use the physical clock, and reset the logical one. c.mu.timestamp.WallTime = physicalClock c.mu.timestamp.Logical = 0 &#125; return c.mu.timestamp&#125; 如果当前物理时钟小于WallTime，则将逻辑时钟+1 如果当前物理时钟大于WallTime，则更新WallTime为当前物理时钟，且将逻辑时钟设置为0 节点时钟同步 节点之间通过在RPC请求中携带HLC时间来进行时钟同步。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// sendSingleRange gathers and rearranges the replicas, and makes an RPC call.func (ds *DistSender) sendSingleRange( ctx context.Context, ba roachpb.BatchRequest, desc *roachpb.RangeDescriptor,) (*roachpb.BatchResponse, *roachpb.Error) &#123; ...... br, err := ds.sendRPC(ctx, desc.RangeID, replicas, ba) if err != nil &#123; log.ErrEvent(ctx, err.Error()) return nil, roachpb.NewError(err) &#125; // If the reply contains a timestamp, update the local HLC with it. if br.Error != nil &amp;&amp; br.Error.Now != (hlc.Timestamp&#123;&#125;) &#123; ds.clock.Update(br.Error.Now) &#125; else if br.Now != (hlc.Timestamp&#123;&#125;) &#123; ds.clock.Update(br.Now) &#125; ......&#125;// Update takes a hybrid timestamp, usually originating from// an event received from another member of a distributed// system. The clock is updated and the hybrid timestamp// associated to the receipt of the event returned.// An error may only occur if offset checking is active and// the remote timestamp was rejected due to clock offset,// in which case the timestamp of the clock will not have been// altered.// To timestamp events of local origin, use Now instead.func (c *Clock) Update(rt Timestamp) Timestamp &#123; c.mu.Lock() defer c.mu.Unlock() // 如果本地物理时间pt physicalClock := c.getPhysicalClockLocked() // 大于本地WallTime且大于rt.WallTime： // 更新本地WallTime=pt，且logical=0 if physicalClock &gt; c.mu.timestamp.WallTime &amp;&amp; physicalClock &gt; rt.WallTime &#123; // Our physical clock is ahead of both wall times. It is used // as the new wall time and the logical clock is reset. c.mu.timestamp.WallTime = physicalClock c.mu.timestamp.Logical = 0 return c.mu.timestamp &#125; // In the remaining cases, our physical clock plays no role // as it is behind the local or remote wall times. Instead, // the logical clock comes into play. // 如果rt.WallTime &gt; 本地WallTime： // 检查rt.WallTime与pt是否大于时钟偏差； // 本地WallTime=rt.WallTime，logical++ if rt.WallTime &gt; c.mu.timestamp.WallTime &#123; offset := time.Duration(rt.WallTime-physicalClock) * time.Nanosecond if c.maxOffset &gt; 0 &amp;&amp; offset &gt; c.maxOffset &#123; log.Warningf(context.TODO(), &quot;remote wall time is too far ahead (%s) to be trustworthy - updating anyway&quot;, offset) &#125; // The remote clock is ahead of ours, and we update // our own logical clock with theirs. c.mu.timestamp.WallTime = rt.WallTime c.mu.timestamp.Logical = rt.Logical + 1 &#125; else if c.mu.timestamp.WallTime &gt; rt.WallTime &#123; // 如果本地WallTime&gt;rt.WallTime：logical++ // Our wall time is larger, so it remains but we tick // the logical clock. c.mu.timestamp.Logical++ &#125; else &#123; // Both wall times are equal, and the larger logical // clock is used for the update. if rt.Logical &gt; c.mu.timestamp.Logical &#123; c.mu.timestamp.Logical = rt.Logical &#125; c.mu.timestamp.Logical++ &#125; return c.mu.timestamp&#125; 参考 Lamport timestamp Time, Clocks, and the Ordering of Events in a Distributed System Vector clock Version vector Why Logical Clocks are Easy Version Vectors are not Vector Clocks Vector Clocks Revisited]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What Is the Most Important Thing in Life？]]></title>
    <url>%2F2018%2F01%2F01%2Fwhich-most-important%2F</url>
    <content type="text"><![CDATA[the most important thing in life. the most important thing in life. Keeping healthy. It is health that is real wealth and not pieces of gold and silver. Everybody needs somebody, be that a friend, a partner, or someone you’re related to. Making someone’s day full of sunshine even when yours is not. Money should not be a priority. The beautiful thing about learning is that nobody can take it away from you. Know who you are. Don’t be a victim BUT instead be a hero in your life. Don't give up. A winner is just a loser who tried on more time. Always make time for gratitude.]]></content>
      <categories>
        <category>2018</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>人生</tag>
      </tags>
  </entry>
</search>
